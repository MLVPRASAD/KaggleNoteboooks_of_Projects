{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a45e2f667e2307a446f281fd53dd3fb9cf3896f"
   },
   "source": [
    "## Pytorch to implement simple feed-forward NN model (0.89+)\n",
    "\n",
    "* As below discussion, NN model can get lB 0.89+\n",
    "* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82499#latest-483679\n",
    "* Add Cycling learning rate , K-fold cross validation (0.85 to 0.86)\n",
    "* Add flatten layer as below discussion (0.86 to 0.897)\n",
    "* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n",
    "\n",
    "## LightGBM (LB 0.899)\n",
    "\n",
    "* Fine tune parameters (0.898 to 0.899)\n",
    "* Reference this kernel : https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899\n",
    "\n",
    "\n",
    "## Plan to do\n",
    "* Modify model structure on NN model\n",
    "* Focal loss\n",
    "* Feature engineering\n",
    "* Tune parameters oof LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'sample_submission.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0508722b8fa65e54571703633e6c878477e0443f"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "84c16d8edbeb1444c54e97ffb534633172d06356"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 202), (200000, 201))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "33bea67299d3edc1c8d87e9e0de371dc717d8b6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>...</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>...</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>...</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>...</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>...</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>...</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1   ...     var_196  var_197  var_198  var_199\n",
       "0  train_0       0   8.9255 -6.7863   ...      7.8784   8.5635  12.7803  -1.0914\n",
       "1  train_1       0  11.5006 -4.1473   ...      8.1267   8.7889  18.3560   1.9518\n",
       "2  train_2       0   8.6093 -2.7457   ...     -6.5213   8.2675  14.7222   0.3965\n",
       "3  train_3       0  11.0604 -2.1518   ...     -2.9275  10.2922  17.9697  -8.9996\n",
       "4  train_4       0   9.8369 -1.4834   ...      3.9267   9.5031  17.9974  -8.8104\n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "996ed97f55d61b4634a2b942eb34ccb7b3930aca"
   },
   "outputs": [],
   "source": [
    "train_features = train_df.drop(['target','ID_code'], axis = 1)\n",
    "test_features = test_df.drop(['ID_code'],axis = 1)\n",
    "train_target = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "699372b84aaa697f192e7da0f1a14468dd3dfa1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 200), (200000, 200), (200000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape,test_features.shape,train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "84f3159307f8a5c5f2993a6d874aafe776490e08"
   },
   "outputs": [],
   "source": [
    "#### Scaling feature #####\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "train_features = sc.fit_transform(train_features)\n",
    "test_features = sc.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1bdb1f695eb045d4dc6710234c7df355ecb9679a"
   },
   "source": [
    "## Split K- fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b7832757a0bcae1b027da430bfa0192dd2eaef7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([     0,      1,      2, ..., 199997, 199998, 199999]),\n",
       "  array([     3,      7,      8, ..., 199983, 199988, 199991])),\n",
       " (array([     0,      1,      2, ..., 199997, 199998, 199999]),\n",
       "  array([     4,      9,     10, ..., 199986, 199992, 199995])),\n",
       " (array([     0,      1,      3, ..., 199995, 199996, 199997]),\n",
       "  array([     2,      5,     12, ..., 199993, 199998, 199999]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement K-fold validation to improve results\n",
    "n_splits = 5 # Number of K-fold Splits\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True).split(train_features, train_target))\n",
    "splits[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55c0a7544d135109579772b0140569d9ea8f9ed8"
   },
   "source": [
    "## Cycling learning rate\n",
    "\n",
    "*copy from ==> https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "8ffbc25621cbd735e0ba5f7bbbc4f7d8da37c9d9"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a1cde968ad94fe9a8876e69bd1792fc9c26a028"
   },
   "source": [
    "## Build Simple NN model (Pytorch)\n",
    "\n",
    "* add flatten layer before fc layer (improve to 0.89+)\n",
    "* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n",
    "\n",
    "* Model structure\n",
    "* (batch_size, 200) ==> Flatten ==> (batch_size* 200,1) ==> fc1 ==> (batch_size* 200, hidden_layer) ==>Reshape ==>(batch_size, hidden_layer * 200) ==> fc2 ==> (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "925b6d89e56800f25609f2ac5c3cbf20608ba799"
   },
   "outputs": [],
   "source": [
    "class Simple_NN(nn.Module):\n",
    "    def __init__(self ,input_dim ,hidden_dim, dropout = 0.75):\n",
    "        super(Simple_NN, self).__init__()\n",
    "        \n",
    "        self.inpt_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(int(hidden_dim*input_dim), 1)\n",
    "        #self.fc3 = nn.Linear(int(hidden_dim/2*input_dim), int(hidden_dim/4))\n",
    "        #self.fc4 = nn.Linear(int(hidden_dim/4*input_dim), int(hidden_dim/8))\n",
    "        #self.fc5 = nn.Linear(int(hidden_dim/8*input_dim), 1)\n",
    "        #self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        #self.bn2 = nn.BatchNorm1d(int(hidden_dim/2))\n",
    "        #self.bn3 = nn.BatchNorm1d(int(hidden_dim/4))\n",
    "        #self.bn4 = nn.BatchNorm1d(int(hidden_dim/8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b_size = x.size(0)\n",
    "        x = x.view(-1, 1)\n",
    "        y = self.fc1(x)\n",
    "        y = self.relu(y)\n",
    "        y = y.view(b_size, -1)\n",
    "        \n",
    "        out= self.fc2(y)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6aafb908b6967fe72a35603de367372145ee722d"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "867d8ce51beef40c85d5751f89d832af9b5d8ada"
   },
   "source": [
    "## Start training\n",
    "* Epoch = 40\n",
    "* Batch size = 256\n",
    "* Cycling step = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "70429be47146b43f3129eacc3812d10d6836cd8a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/40 \t loss=0.2396 \t val_loss=0.2116 \t time=3.64s\n",
      "Epoch 2/40 \t loss=0.2158 \t val_loss=0.2203 \t time=3.56s\n",
      "Epoch 3/40 \t loss=0.2153 \t val_loss=0.2463 \t time=3.56s\n",
      "Epoch 4/40 \t loss=0.2115 \t val_loss=0.2064 \t time=3.56s\n",
      "Epoch 5/40 \t loss=0.2063 \t val_loss=0.2094 \t time=3.56s\n",
      "Epoch 6/40 \t loss=0.2039 \t val_loss=0.2043 \t time=3.58s\n",
      "Epoch 7/40 \t loss=0.2025 \t val_loss=0.2045 \t time=3.61s\n",
      "Epoch 8/40 \t loss=0.2035 \t val_loss=0.2048 \t time=3.55s\n",
      "Epoch 9/40 \t loss=0.2051 \t val_loss=0.2061 \t time=3.60s\n",
      "Epoch 10/40 \t loss=0.2066 \t val_loss=0.2050 \t time=3.56s\n",
      "Epoch 11/40 \t loss=0.2043 \t val_loss=0.2049 \t time=3.58s\n",
      "Epoch 12/40 \t loss=0.2024 \t val_loss=0.2038 \t time=3.55s\n",
      "Epoch 13/40 \t loss=0.2010 \t val_loss=0.2033 \t time=3.57s\n",
      "Epoch 14/40 \t loss=0.2015 \t val_loss=0.2085 \t time=3.59s\n",
      "Epoch 15/40 \t loss=0.2027 \t val_loss=0.2086 \t time=3.57s\n",
      "Epoch 16/40 \t loss=0.2035 \t val_loss=0.2046 \t time=3.55s\n",
      "Epoch 17/40 \t loss=0.2038 \t val_loss=0.2057 \t time=3.57s\n",
      "Epoch 18/40 \t loss=0.2019 \t val_loss=0.2038 \t time=3.58s\n",
      "Epoch 19/40 \t loss=0.2007 \t val_loss=0.2031 \t time=3.57s\n",
      "Epoch 20/40 \t loss=0.1996 \t val_loss=0.2054 \t time=3.58s\n",
      "Epoch 21/40 \t loss=0.2008 \t val_loss=0.2091 \t time=3.56s\n",
      "Epoch 22/40 \t loss=0.2014 \t val_loss=0.2053 \t time=3.62s\n",
      "Epoch 23/40 \t loss=0.2026 \t val_loss=0.2074 \t time=3.58s\n",
      "Epoch 24/40 \t loss=0.2014 \t val_loss=0.2034 \t time=3.57s\n",
      "Epoch 25/40 \t loss=0.1999 \t val_loss=0.2031 \t time=3.60s\n",
      "Epoch 26/40 \t loss=0.1991 \t val_loss=0.2049 \t time=3.58s\n",
      "Epoch 27/40 \t loss=0.1996 \t val_loss=0.2037 \t time=3.56s\n",
      "Epoch 28/40 \t loss=0.2004 \t val_loss=0.2047 \t time=3.58s\n",
      "Epoch 29/40 \t loss=0.2010 \t val_loss=0.2037 \t time=3.56s\n",
      "Epoch 30/40 \t loss=0.2004 \t val_loss=0.2034 \t time=3.57s\n",
      "Epoch 31/40 \t loss=0.2000 \t val_loss=0.2076 \t time=3.58s\n",
      "Epoch 32/40 \t loss=0.1992 \t val_loss=0.2029 \t time=3.57s\n",
      "Epoch 33/40 \t loss=0.1988 \t val_loss=0.2043 \t time=3.57s\n",
      "Epoch 34/40 \t loss=0.1994 \t val_loss=0.2042 \t time=3.55s\n",
      "Epoch 35/40 \t loss=0.1997 \t val_loss=0.2030 \t time=3.57s\n",
      "Epoch 36/40 \t loss=0.2003 \t val_loss=0.2029 \t time=3.58s\n",
      "Epoch 37/40 \t loss=0.1994 \t val_loss=0.2033 \t time=3.56s\n",
      "Epoch 38/40 \t loss=0.1986 \t val_loss=0.2031 \t time=3.62s\n",
      "Epoch 39/40 \t loss=0.1982 \t val_loss=0.2035 \t time=3.58s\n",
      "Epoch 40/40 \t loss=0.1984 \t val_loss=0.2030 \t time=3.54s\n",
      "Fold 2\n",
      "Epoch 1/40 \t loss=0.2364 \t val_loss=0.2111 \t time=3.57s\n",
      "Epoch 2/40 \t loss=0.2164 \t val_loss=0.2123 \t time=3.59s\n",
      "Epoch 3/40 \t loss=0.2144 \t val_loss=0.2088 \t time=3.55s\n",
      "Epoch 4/40 \t loss=0.2110 \t val_loss=0.2052 \t time=3.58s\n",
      "Epoch 5/40 \t loss=0.2067 \t val_loss=0.2073 \t time=3.55s\n",
      "Epoch 6/40 \t loss=0.2045 \t val_loss=0.2035 \t time=3.55s\n",
      "Epoch 7/40 \t loss=0.2021 \t val_loss=0.2036 \t time=3.58s\n",
      "Epoch 8/40 \t loss=0.2044 \t val_loss=0.2037 \t time=3.57s\n",
      "Epoch 9/40 \t loss=0.2056 \t val_loss=0.2043 \t time=3.54s\n",
      "Epoch 10/40 \t loss=0.2056 \t val_loss=0.2038 \t time=3.56s\n",
      "Epoch 11/40 \t loss=0.2039 \t val_loss=0.2037 \t time=3.56s\n",
      "Epoch 12/40 \t loss=0.2024 \t val_loss=0.2031 \t time=3.62s\n",
      "Epoch 13/40 \t loss=0.2007 \t val_loss=0.2038 \t time=3.56s\n",
      "Epoch 14/40 \t loss=0.2006 \t val_loss=0.2044 \t time=3.55s\n",
      "Epoch 15/40 \t loss=0.2025 \t val_loss=0.2036 \t time=3.56s\n",
      "Epoch 16/40 \t loss=0.2024 \t val_loss=0.2085 \t time=3.55s\n",
      "Epoch 17/40 \t loss=0.2028 \t val_loss=0.2026 \t time=3.55s\n",
      "Epoch 18/40 \t loss=0.2013 \t val_loss=0.2028 \t time=3.60s\n",
      "Epoch 19/40 \t loss=0.2001 \t val_loss=0.2035 \t time=3.57s\n",
      "Epoch 20/40 \t loss=0.1992 \t val_loss=0.2029 \t time=3.53s\n",
      "Epoch 21/40 \t loss=0.2004 \t val_loss=0.2024 \t time=3.55s\n",
      "Epoch 22/40 \t loss=0.2008 \t val_loss=0.2033 \t time=3.55s\n",
      "Epoch 23/40 \t loss=0.2018 \t val_loss=0.2050 \t time=3.54s\n",
      "Epoch 24/40 \t loss=0.2003 \t val_loss=0.2028 \t time=3.57s\n",
      "Epoch 25/40 \t loss=0.1993 \t val_loss=0.2046 \t time=3.55s\n",
      "Epoch 26/40 \t loss=0.1984 \t val_loss=0.2021 \t time=3.56s\n",
      "Epoch 27/40 \t loss=0.1986 \t val_loss=0.2022 \t time=3.58s\n",
      "Epoch 28/40 \t loss=0.1993 \t val_loss=0.2023 \t time=3.56s\n",
      "Epoch 29/40 \t loss=0.2002 \t val_loss=0.2032 \t time=3.57s\n",
      "Epoch 30/40 \t loss=0.1998 \t val_loss=0.2031 \t time=3.55s\n",
      "Epoch 31/40 \t loss=0.1991 \t val_loss=0.2029 \t time=3.55s\n",
      "Epoch 32/40 \t loss=0.1979 \t val_loss=0.2019 \t time=3.63s\n",
      "Epoch 33/40 \t loss=0.1980 \t val_loss=0.2020 \t time=3.60s\n",
      "Epoch 34/40 \t loss=0.1988 \t val_loss=0.2023 \t time=3.58s\n",
      "Epoch 35/40 \t loss=0.1993 \t val_loss=0.2027 \t time=3.63s\n",
      "Epoch 36/40 \t loss=0.1993 \t val_loss=0.2026 \t time=3.57s\n",
      "Epoch 37/40 \t loss=0.1986 \t val_loss=0.2026 \t time=3.57s\n",
      "Epoch 38/40 \t loss=0.1978 \t val_loss=0.2020 \t time=3.59s\n",
      "Epoch 39/40 \t loss=0.1975 \t val_loss=0.2072 \t time=3.56s\n",
      "Epoch 40/40 \t loss=0.1978 \t val_loss=0.2020 \t time=3.58s\n",
      "Fold 3\n",
      "Epoch 1/40 \t loss=0.2385 \t val_loss=0.2175 \t time=3.57s\n",
      "Epoch 2/40 \t loss=0.2177 \t val_loss=0.2118 \t time=3.61s\n",
      "Epoch 3/40 \t loss=0.2154 \t val_loss=0.2530 \t time=3.57s\n",
      "Epoch 4/40 \t loss=0.2127 \t val_loss=0.2119 \t time=3.57s\n",
      "Epoch 5/40 \t loss=0.2077 \t val_loss=0.2099 \t time=3.57s\n",
      "Epoch 6/40 \t loss=0.2057 \t val_loss=0.2078 \t time=3.58s\n",
      "Epoch 7/40 \t loss=0.2023 \t val_loss=0.2055 \t time=3.57s\n",
      "Epoch 8/40 \t loss=0.2049 \t val_loss=0.2074 \t time=3.58s\n",
      "Epoch 9/40 \t loss=0.2055 \t val_loss=0.2090 \t time=3.59s\n",
      "Epoch 10/40 \t loss=0.2059 \t val_loss=0.2084 \t time=3.57s\n",
      "Epoch 11/40 \t loss=0.2042 \t val_loss=0.2058 \t time=3.63s\n",
      "Epoch 12/40 \t loss=0.2028 \t val_loss=0.2063 \t time=3.58s\n",
      "Epoch 13/40 \t loss=0.2007 \t val_loss=0.2038 \t time=3.57s\n",
      "Epoch 14/40 \t loss=0.2012 \t val_loss=0.2050 \t time=3.60s\n",
      "Epoch 15/40 \t loss=0.2024 \t val_loss=0.2056 \t time=3.59s\n",
      "Epoch 16/40 \t loss=0.2042 \t val_loss=0.2053 \t time=3.59s\n",
      "Epoch 17/40 \t loss=0.2030 \t val_loss=0.2062 \t time=3.66s\n",
      "Epoch 18/40 \t loss=0.2018 \t val_loss=0.2055 \t time=3.57s\n",
      "Epoch 19/40 \t loss=0.2000 \t val_loss=0.2040 \t time=3.57s\n",
      "Epoch 20/40 \t loss=0.1994 \t val_loss=0.2037 \t time=3.57s\n",
      "Epoch 21/40 \t loss=0.2010 \t val_loss=0.2039 \t time=3.58s\n",
      "Epoch 22/40 \t loss=0.2011 \t val_loss=0.2047 \t time=3.58s\n",
      "Epoch 23/40 \t loss=0.2020 \t val_loss=0.2045 \t time=3.57s\n",
      "Epoch 24/40 \t loss=0.2008 \t val_loss=0.2037 \t time=3.55s\n",
      "Epoch 25/40 \t loss=0.1995 \t val_loss=0.2037 \t time=3.58s\n",
      "Epoch 26/40 \t loss=0.1987 \t val_loss=0.2052 \t time=3.55s\n",
      "Epoch 27/40 \t loss=0.1993 \t val_loss=0.2056 \t time=3.56s\n",
      "Epoch 28/40 \t loss=0.1999 \t val_loss=0.2044 \t time=3.60s\n",
      "Epoch 29/40 \t loss=0.2006 \t val_loss=0.2050 \t time=3.57s\n",
      "Epoch 30/40 \t loss=0.2002 \t val_loss=0.2048 \t time=3.58s\n",
      "Epoch 31/40 \t loss=0.1993 \t val_loss=0.2050 \t time=3.58s\n",
      "Epoch 32/40 \t loss=0.1985 \t val_loss=0.2036 \t time=3.57s\n",
      "Epoch 33/40 \t loss=0.1984 \t val_loss=0.2034 \t time=3.58s\n",
      "Epoch 34/40 \t loss=0.1992 \t val_loss=0.2067 \t time=3.56s\n",
      "Epoch 35/40 \t loss=0.1991 \t val_loss=0.2054 \t time=3.56s\n",
      "Epoch 36/40 \t loss=0.1995 \t val_loss=0.2048 \t time=3.54s\n",
      "Epoch 37/40 \t loss=0.1992 \t val_loss=0.2057 \t time=3.57s\n",
      "Epoch 38/40 \t loss=0.1984 \t val_loss=0.2039 \t time=3.55s\n",
      "Epoch 39/40 \t loss=0.1978 \t val_loss=0.2035 \t time=3.57s\n",
      "Epoch 40/40 \t loss=0.1979 \t val_loss=0.2047 \t time=3.55s\n",
      "Fold 4\n",
      "Epoch 1/40 \t loss=0.2377 \t val_loss=0.2356 \t time=3.55s\n",
      "Epoch 2/40 \t loss=0.2149 \t val_loss=0.2091 \t time=3.56s\n",
      "Epoch 3/40 \t loss=0.2130 \t val_loss=0.2104 \t time=3.54s\n",
      "Epoch 4/40 \t loss=0.2136 \t val_loss=0.2192 \t time=3.60s\n",
      "Epoch 5/40 \t loss=0.2074 \t val_loss=0.2062 \t time=3.55s\n",
      "Epoch 6/40 \t loss=0.2044 \t val_loss=0.2085 \t time=3.54s\n",
      "Epoch 7/40 \t loss=0.2025 \t val_loss=0.2064 \t time=3.62s\n",
      "Epoch 8/40 \t loss=0.2048 \t val_loss=0.2228 \t time=3.55s\n",
      "Epoch 9/40 \t loss=0.2050 \t val_loss=0.2039 \t time=3.59s\n",
      "Epoch 10/40 \t loss=0.2074 \t val_loss=0.2285 \t time=3.58s\n",
      "Epoch 11/40 \t loss=0.2063 \t val_loss=0.2083 \t time=3.57s\n",
      "Epoch 12/40 \t loss=0.2014 \t val_loss=0.2044 \t time=3.56s\n",
      "Epoch 13/40 \t loss=0.2006 \t val_loss=0.2029 \t time=3.57s\n",
      "Epoch 14/40 \t loss=0.2021 \t val_loss=0.2444 \t time=3.56s\n",
      "Epoch 15/40 \t loss=0.2051 \t val_loss=0.2056 \t time=3.57s\n",
      "Epoch 16/40 \t loss=0.2029 \t val_loss=0.2044 \t time=3.57s\n",
      "Epoch 17/40 \t loss=0.2021 \t val_loss=0.2059 \t time=3.56s\n",
      "Epoch 18/40 \t loss=0.2010 \t val_loss=0.2050 \t time=3.56s\n",
      "Epoch 19/40 \t loss=0.1999 \t val_loss=0.2039 \t time=3.56s\n",
      "Epoch 20/40 \t loss=0.1997 \t val_loss=0.2025 \t time=3.57s\n",
      "Epoch 21/40 \t loss=0.2008 \t val_loss=0.2032 \t time=3.61s\n",
      "Epoch 22/40 \t loss=0.2017 \t val_loss=0.2123 \t time=3.62s\n",
      "Epoch 23/40 \t loss=0.2017 \t val_loss=0.2033 \t time=3.53s\n",
      "Epoch 24/40 \t loss=0.2008 \t val_loss=0.2046 \t time=3.57s\n",
      "Epoch 25/40 \t loss=0.1998 \t val_loss=0.2045 \t time=3.56s\n",
      "Epoch 26/40 \t loss=0.1990 \t val_loss=0.2031 \t time=3.57s\n",
      "Epoch 27/40 \t loss=0.1996 \t val_loss=0.2037 \t time=3.59s\n",
      "Epoch 28/40 \t loss=0.2003 \t val_loss=0.2047 \t time=3.56s\n",
      "Epoch 29/40 \t loss=0.2017 \t val_loss=0.2076 \t time=3.58s\n",
      "Epoch 30/40 \t loss=0.2016 \t val_loss=0.2074 \t time=3.57s\n",
      "Epoch 31/40 \t loss=0.1997 \t val_loss=0.2093 \t time=3.56s\n",
      "Epoch 32/40 \t loss=0.1987 \t val_loss=0.2031 \t time=3.59s\n",
      "Epoch 33/40 \t loss=0.1985 \t val_loss=0.2034 \t time=3.57s\n",
      "Epoch 34/40 \t loss=0.1993 \t val_loss=0.2080 \t time=3.58s\n",
      "Epoch 35/40 \t loss=0.1993 \t val_loss=0.2087 \t time=3.59s\n",
      "Epoch 36/40 \t loss=0.1997 \t val_loss=0.2035 \t time=3.70s\n",
      "Epoch 37/40 \t loss=0.2001 \t val_loss=0.2100 \t time=3.56s\n",
      "Epoch 38/40 \t loss=0.1998 \t val_loss=0.2045 \t time=3.68s\n",
      "Epoch 39/40 \t loss=0.1982 \t val_loss=0.2035 \t time=3.57s\n",
      "Epoch 40/40 \t loss=0.1991 \t val_loss=0.2032 \t time=3.57s\n",
      "Fold 5\n",
      "Epoch 1/40 \t loss=0.2341 \t val_loss=0.2127 \t time=3.57s\n",
      "Epoch 2/40 \t loss=0.2151 \t val_loss=0.2138 \t time=3.59s\n",
      "Epoch 3/40 \t loss=0.2153 \t val_loss=0.2255 \t time=3.59s\n",
      "Epoch 4/40 \t loss=0.2094 \t val_loss=0.2142 \t time=3.58s\n",
      "Epoch 5/40 \t loss=0.2058 \t val_loss=0.2102 \t time=3.57s\n",
      "Epoch 6/40 \t loss=0.2030 \t val_loss=0.2062 \t time=3.57s\n",
      "Epoch 7/40 \t loss=0.2014 \t val_loss=0.2048 \t time=3.57s\n",
      "Epoch 8/40 \t loss=0.2038 \t val_loss=0.2044 \t time=3.57s\n",
      "Epoch 9/40 \t loss=0.2052 \t val_loss=0.2324 \t time=3.57s\n",
      "Epoch 10/40 \t loss=0.2064 \t val_loss=0.2171 \t time=3.56s\n",
      "Epoch 11/40 \t loss=0.2033 \t val_loss=0.2062 \t time=3.58s\n",
      "Epoch 12/40 \t loss=0.2010 \t val_loss=0.2043 \t time=3.63s\n",
      "Epoch 13/40 \t loss=0.1997 \t val_loss=0.2041 \t time=3.55s\n",
      "Epoch 14/40 \t loss=0.2014 \t val_loss=0.2266 \t time=3.62s\n",
      "Epoch 15/40 \t loss=0.2041 \t val_loss=0.2052 \t time=3.58s\n",
      "Epoch 16/40 \t loss=0.2022 \t val_loss=0.2107 \t time=3.56s\n",
      "Epoch 17/40 \t loss=0.2018 \t val_loss=0.2143 \t time=3.59s\n",
      "Epoch 18/40 \t loss=0.2003 \t val_loss=0.2042 \t time=3.58s\n",
      "Epoch 19/40 \t loss=0.2019 \t val_loss=0.2176 \t time=3.58s\n",
      "Epoch 20/40 \t loss=0.2010 \t val_loss=0.2073 \t time=3.58s\n",
      "Epoch 21/40 \t loss=0.2025 \t val_loss=0.2508 \t time=3.57s\n",
      "Epoch 22/40 \t loss=0.2043 \t val_loss=0.2049 \t time=3.55s\n",
      "Epoch 23/40 \t loss=0.1999 \t val_loss=0.2044 \t time=3.57s\n",
      "Epoch 24/40 \t loss=0.1994 \t val_loss=0.2057 \t time=3.57s\n",
      "Epoch 25/40 \t loss=0.1983 \t val_loss=0.2090 \t time=3.57s\n",
      "Epoch 26/40 \t loss=0.1977 \t val_loss=0.2058 \t time=3.55s\n",
      "Epoch 27/40 \t loss=0.1988 \t val_loss=0.2278 \t time=3.59s\n",
      "Epoch 28/40 \t loss=0.1998 \t val_loss=0.2045 \t time=3.57s\n",
      "Epoch 29/40 \t loss=0.1995 \t val_loss=0.2048 \t time=3.56s\n",
      "Epoch 30/40 \t loss=0.1993 \t val_loss=0.2046 \t time=3.56s\n",
      "Epoch 31/40 \t loss=0.2000 \t val_loss=0.2200 \t time=3.61s\n",
      "Epoch 32/40 \t loss=0.1989 \t val_loss=0.2043 \t time=3.57s\n",
      "Epoch 33/40 \t loss=0.1972 \t val_loss=0.2076 \t time=3.58s\n",
      "Epoch 34/40 \t loss=0.1979 \t val_loss=0.2039 \t time=3.58s\n",
      "Epoch 35/40 \t loss=0.1984 \t val_loss=0.2042 \t time=3.56s\n",
      "Epoch 36/40 \t loss=0.1985 \t val_loss=0.2042 \t time=3.60s\n",
      "Epoch 37/40 \t loss=0.1981 \t val_loss=0.2042 \t time=3.57s\n",
      "Epoch 38/40 \t loss=0.1973 \t val_loss=0.2042 \t time=3.57s\n",
      "Epoch 39/40 \t loss=0.1971 \t val_loss=0.2041 \t time=3.56s\n",
      "Epoch 40/40 \t loss=0.1970 \t val_loss=0.2039 \t time=3.55s\n",
      "All \t loss=0.1980 \t val_loss=0.2034 \t auc=0.8976\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "## Hyperparameter\n",
    "n_epochs = 40\n",
    "batch_size = 256\n",
    "\n",
    "## Build tensor data for torch\n",
    "train_preds = np.zeros((len(train_features)))\n",
    "test_preds = np.zeros((len(test_features)))\n",
    "\n",
    "x_test = np.array(test_features)\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.float).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "avg_losses_f = []\n",
    "avg_val_losses_f = []\n",
    "\n",
    "## Start K-fold validation\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):  \n",
    "    x_train = np.array(train_features)\n",
    "    y_train = np.array(train_target)\n",
    "    \n",
    "    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.float).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.float).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    ##Loss function\n",
    "    #loss_fn = FocalLoss(2)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    #Build model, initial weight and optimizer\n",
    "    model = Simple_NN(200,16)\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001,weight_decay=1e-5) # Using Adam optimizer\n",
    "    \n",
    "    \n",
    "    ######################Cycling learning rate########################\n",
    "\n",
    "    step_size = 2000\n",
    "    base_lr, max_lr = 0.001, 0.005  \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                             lr=max_lr)\n",
    "    \n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "               step_size=step_size, mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        #avg_auc = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            y_pred = model(x_batch)\n",
    "            ###################tuning learning rate###############\n",
    "            if scheduler:\n",
    "                #print('cycle_LR')\n",
    "                scheduler.batch_step()\n",
    "\n",
    "            ######################################################\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()/len(train_loader)\n",
    "            #avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach().cpu()),4) / len(train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(test_features)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        #avg_val_auc = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            \n",
    "            #avg_val_auc += round(roc_auc_score(y_batch.cpu(),sigmoid(y_pred.cpu().numpy())[:, 0]),4) / len(valid_loader)\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    avg_losses_f.append(avg_loss)\n",
    "    avg_val_losses_f.append(avg_val_loss) \n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "auc  =  round(roc_auc_score(train_target,train_preds),4)      \n",
    "print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'.format(np.average(avg_losses_f),np.average(avg_val_losses_f),auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "259888d043476052d0fb3e46636b3f4ebcdd6279"
   },
   "source": [
    "## LightGBM Model\n",
    "* reference this kernel : https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "3de6ce6d8ed069e31f560369a3fd2bfb36f9dd0c"
   },
   "outputs": [],
   "source": [
    "## Use no scaling data to train LGBM\n",
    "train_features = train_df.drop(['target','ID_code'], axis = 1)\n",
    "test_features = test_df.drop(['ID_code'],axis = 1)\n",
    "train_target = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "b4718e20b804da92ac60815f73305831278a28ed"
   },
   "outputs": [],
   "source": [
    "#LGBM Paramater tuning\n",
    "param = {\n",
    "        'num_leaves': 7,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.04,\n",
    "        'max_depth': 17,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'auc',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4761da539c859a12494702edcaaad19939948505"
   },
   "source": [
    "## LGBM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "9ee91342c38a5cf2e1006bd2a06eccf7b0760dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.884761\tvalid_1's auc: 0.872707\n",
      "[2000]\ttraining's auc: 0.896022\tvalid_1's auc: 0.881688\n",
      "[3000]\ttraining's auc: 0.903935\tvalid_1's auc: 0.887246\n",
      "[4000]\ttraining's auc: 0.910122\tvalid_1's auc: 0.89127\n",
      "[5000]\ttraining's auc: 0.914865\tvalid_1's auc: 0.89397\n",
      "[6000]\ttraining's auc: 0.918765\tvalid_1's auc: 0.895984\n",
      "[7000]\ttraining's auc: 0.921969\tvalid_1's auc: 0.897492\n",
      "[8000]\ttraining's auc: 0.924769\tvalid_1's auc: 0.898499\n",
      "[9000]\ttraining's auc: 0.927355\tvalid_1's auc: 0.899281\n",
      "[10000]\ttraining's auc: 0.92985\tvalid_1's auc: 0.899715\n",
      "[11000]\ttraining's auc: 0.932259\tvalid_1's auc: 0.900023\n",
      "[12000]\ttraining's auc: 0.934669\tvalid_1's auc: 0.900271\n",
      "Early stopping, best iteration is:\n",
      "[12245]\ttraining's auc: 0.935254\tvalid_1's auc: 0.900338\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.885223\tvalid_1's auc: 0.870329\n",
      "[2000]\ttraining's auc: 0.89614\tvalid_1's auc: 0.880231\n",
      "[3000]\ttraining's auc: 0.903625\tvalid_1's auc: 0.885959\n",
      "[4000]\ttraining's auc: 0.909699\tvalid_1's auc: 0.890237\n",
      "[5000]\ttraining's auc: 0.914445\tvalid_1's auc: 0.893333\n",
      "[6000]\ttraining's auc: 0.918354\tvalid_1's auc: 0.895652\n",
      "[7000]\ttraining's auc: 0.921662\tvalid_1's auc: 0.89727\n",
      "[8000]\ttraining's auc: 0.924521\tvalid_1's auc: 0.898385\n",
      "[9000]\ttraining's auc: 0.927174\tvalid_1's auc: 0.899149\n",
      "[10000]\ttraining's auc: 0.929663\tvalid_1's auc: 0.899629\n",
      "[11000]\ttraining's auc: 0.932083\tvalid_1's auc: 0.899936\n",
      "Early stopping, best iteration is:\n",
      "[11692]\ttraining's auc: 0.933743\tvalid_1's auc: 0.900095\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.885328\tvalid_1's auc: 0.871308\n",
      "[2000]\ttraining's auc: 0.896453\tvalid_1's auc: 0.880342\n",
      "[3000]\ttraining's auc: 0.90417\tvalid_1's auc: 0.885754\n",
      "[4000]\ttraining's auc: 0.910375\tvalid_1's auc: 0.889747\n",
      "[5000]\ttraining's auc: 0.915235\tvalid_1's auc: 0.892499\n",
      "[6000]\ttraining's auc: 0.919163\tvalid_1's auc: 0.894531\n",
      "[7000]\ttraining's auc: 0.922459\tvalid_1's auc: 0.89579\n",
      "[8000]\ttraining's auc: 0.925278\tvalid_1's auc: 0.896722\n",
      "[9000]\ttraining's auc: 0.927832\tvalid_1's auc: 0.897362\n",
      "[10000]\ttraining's auc: 0.930339\tvalid_1's auc: 0.897827\n",
      "[11000]\ttraining's auc: 0.932765\tvalid_1's auc: 0.89815\n",
      "Early stopping, best iteration is:\n",
      "[11263]\ttraining's auc: 0.933395\tvalid_1's auc: 0.898213\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.885304\tvalid_1's auc: 0.87218\n",
      "[2000]\ttraining's auc: 0.896687\tvalid_1's auc: 0.881551\n",
      "[3000]\ttraining's auc: 0.9045\tvalid_1's auc: 0.887038\n",
      "[4000]\ttraining's auc: 0.910504\tvalid_1's auc: 0.890737\n",
      "[5000]\ttraining's auc: 0.915279\tvalid_1's auc: 0.893366\n",
      "[6000]\ttraining's auc: 0.919197\tvalid_1's auc: 0.895229\n",
      "[7000]\ttraining's auc: 0.922437\tvalid_1's auc: 0.896439\n",
      "[8000]\ttraining's auc: 0.925287\tvalid_1's auc: 0.897315\n",
      "[9000]\ttraining's auc: 0.927871\tvalid_1's auc: 0.897903\n",
      "[10000]\ttraining's auc: 0.930334\tvalid_1's auc: 0.898232\n",
      "[11000]\ttraining's auc: 0.932764\tvalid_1's auc: 0.898506\n",
      "[12000]\ttraining's auc: 0.935136\tvalid_1's auc: 0.898678\n",
      "Early stopping, best iteration is:\n",
      "[12331]\ttraining's auc: 0.935935\tvalid_1's auc: 0.898759\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttraining's auc: 0.885226\tvalid_1's auc: 0.873046\n",
      "[2000]\ttraining's auc: 0.89676\tvalid_1's auc: 0.880907\n",
      "[3000]\ttraining's auc: 0.904637\tvalid_1's auc: 0.885465\n",
      "[4000]\ttraining's auc: 0.910702\tvalid_1's auc: 0.889077\n",
      "[5000]\ttraining's auc: 0.915465\tvalid_1's auc: 0.891798\n",
      "[6000]\ttraining's auc: 0.919319\tvalid_1's auc: 0.893859\n",
      "[7000]\ttraining's auc: 0.922527\tvalid_1's auc: 0.895289\n",
      "[8000]\ttraining's auc: 0.92532\tvalid_1's auc: 0.896206\n",
      "[9000]\ttraining's auc: 0.927904\tvalid_1's auc: 0.896821\n",
      "[10000]\ttraining's auc: 0.930348\tvalid_1's auc: 0.897311\n",
      "Early stopping, best iteration is:\n",
      "[10082]\ttraining's auc: 0.930548\tvalid_1's auc: 0.897358\n",
      "CV score: 0.89894 \n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):  \n",
    "    print(f'Fold {i + 1}')\n",
    "    x_train = np.array(train_features)\n",
    "    y_train = np.array(train_target)\n",
    "    trn_data = lgb.Dataset(x_train[train_idx.astype(int)], label=y_train[train_idx.astype(int)])\n",
    "    val_data = lgb.Dataset(x_train[valid_idx.astype(int)], label=y_train[valid_idx.astype(int)])\n",
    "    \n",
    "    num_round = 15000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n",
    "    oof[valid_idx] = clf.predict(x_train[valid_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = i + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test_features, num_iteration=clf.best_iteration) / 5\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(train_target, oof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e39e52be147a7e710334cc5592d9864f36716ed7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a0e712896ef68f884a8e594fe0bfff4d7c70387e"
   },
   "source": [
    "## Ensemble two model (NN+ LGBM)\n",
    "* NN model accuracy is too low, ensemble looks don't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "b50f903c99d37da6fa408ef67ae5d3bf3ab2e69d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN auc = 0.89760 \n",
      "LightBGM auc = 0.89894 \n",
      "NN+LightBGM auc = 0.90012 \n"
     ]
    }
   ],
   "source": [
    "esemble = 0.6*oof + 0.4* train_preds\n",
    "print('NN auc = {:<8.5f}'.format(auc))\n",
    "print('LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, oof)))\n",
    "print('NN+LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, esemble)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "38d5b23564b6bedc07cad7f23942b9d189f51b5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000,), (200000,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.shape,predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "304aa085cbbe3be1a807d0eb88bf79417e831a1f"
   },
   "outputs": [],
   "source": [
    "esemble_pred = 0.4* test_preds+ 0.6 *predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "55ee1aa541d2ccdeafff7ecb67873f55633182a0"
   },
   "outputs": [],
   "source": [
    "id_code_test = test_df['ID_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "59cd33ab56622c0b276632e242c9cbf77bbefbd1"
   },
   "source": [
    "## Create submit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "6d3cb9fee66d025f11e3dc366b5d7bc48a628ad8"
   },
   "outputs": [],
   "source": [
    "my_submission_nn = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : test_preds})\n",
    "my_submission_lbgm = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : predictions})\n",
    "my_submission_esemble = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : esemble_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "13d5a462941d6aa309c6a707e66ce776fc011e39"
   },
   "outputs": [],
   "source": [
    "my_submission_nn.to_csv('submission_nn.csv', index = False, header = True)\n",
    "my_submission_lbgm.to_csv('submission_lbgm.csv', index = False, header = True)\n",
    "my_submission_esemble.to_csv('submission_esemble.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "cdaf09d52b7b46fb5a33cc69e37889d29742b0ed"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
