{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b0fcb66f3814a4c83dff403c5b309e0159ff936"
   },
   "source": [
    "There have been some issues regarding the correlation between CV and leaderboard scores in this competition. Every top-scoring public kernel has a much lower CV score than leaderboard score. It has also been very frustrating to tune a model to optimal CV score only to discover that the score on the Leaderboard is abysmal.\n",
    "\n",
    "In this kernel I am going to address this issue and propose a framework for robust local validation. The preprocessing and model architecture have stayed mostly the same as in [my previous kernel](https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch).\n",
    "\n",
    "I'll also write about the impact of seeds on the score.\n",
    "\n",
    "Again, we'll start with standard imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d5f1b9360b808d5941d6f37ba2ba20cf3d7f869"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "6a1856073e217798a8c5ddd17107a8a72855b665"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "# imports for preprocessing the questions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# progress bars\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"F-score is ill-defined and being set to 0.0 due to no predicted samples.\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9898133bb6210b97bead2ed9c46e8280defb305e"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "eaa71cd7a40cf4f647b3af9a5b0fdcd903fefeee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimension:  (1306122, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "3  000042bf85aa498cd78e  ...        0\n",
       "4  0000455dfa3e01eae3af  ...        0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data dimension:  (375806, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000163e3ea7c7a74cd7</td>\n",
       "      <td>Why do so many women become so rude and arroga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002bd4fb5d505b9161</td>\n",
       "      <td>When should I apply for RV college of engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00007756b4a147d2b0b3</td>\n",
       "      <td>What is it really like to be a nurse practitio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000086e4b7e1c7146103</td>\n",
       "      <td>Who are entrepreneurs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000c4c3fbe8785a3090</td>\n",
       "      <td>Is education really making good people nowadays?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text\n",
       "0  0000163e3ea7c7a74cd7  Why do so many women become so rude and arroga...\n",
       "1  00002bd4fb5d505b9161  When should I apply for RV college of engineer...\n",
       "2  00007756b4a147d2b0b3  What is it really like to be a nurse practitio...\n",
       "3  000086e4b7e1c7146103                             Who are entrepreneurs?\n",
       "4  0000c4c3fbe8785a3090   Is education really making good people nowadays?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "print('Train data dimension: ', train_df.shape)\n",
    "display(train_df.head())\n",
    "print('Test data dimension: ', test_df.shape)\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "68dcf13011432b865b124ca0b4e8262b9516bc70"
   },
   "outputs": [],
   "source": [
    "enable_local_test = True\n",
    "if enable_local_test:\n",
    "    n_test = len(test_df)\n",
    "    train_df, local_test_df = (train_df.iloc[:-n_test].reset_index(drop=True), \n",
    "                               train_df.iloc[-n_test:].reset_index(drop=True))\n",
    "else:\n",
    "    local_test_df = pd.DataFrame([[None, None, 0], [None, None, 0]], columns=['qid', 'question_text', 'target'])\n",
    "    n_test = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b708f65338817989da79b07c99f3e86e5acd62df"
   },
   "source": [
    "Here, we create a dataframe I call `local_test_df`. We will pretend that this dataframe is the actual test dataframe. The only difference: We know the labels for this one! So we do not have to blindly submit our model and pray for a good score, but can instead tune the score we achieve on this test dataframe. I have set the size of the local test dataframe to 4 times the size of the public test dataframe. That is a reasonable size (~200k rows) but more or less arbitrary.\n",
    "\n",
    "So we are going to test our model on two sets now: First, the regular test set, and second, a local test set which we know the labels for. Overall, the procedure is:\n",
    "\n",
    "- split the data in a train and local test set\n",
    "- perform CV on the train set\n",
    "- when tuning the model:\n",
    "    - evaluate the predictions on the local test set\n",
    "- when submitting:\n",
    "    - predict the samples in the true test set\n",
    "    - make the size of the local test set 0 for best performance (set `enable_local_test` to False)\n",
    "    \n",
    "    \n",
    "We actually perform CV on the train side of a regular train / test split. It would be ideal to wrap the whole thing in another K-Fold cross validation procedure. That is, however, not feasible regarding computing power on my local machine and in the kaggle kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "301805519ef9c9a7d59eb6c6945480d824d6e160"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d35808558a9a2d4288ff7993c76f542e87edd62f"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6c91fec794f4f77aa09a69ca5eee3e7314a99f33"
   },
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4ea13f2e32ffa7c050af2c2dd9423ed2aa2592f0"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93f521adfd488bc3472b3184a3dcff92da0eb3c9"
   },
   "source": [
    "# Processing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9a4874c7936cd1e527a82aa779e24fc3de3e23a1"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 95000\n",
    "maxlen = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "bfcd85be032654aac572b3f107b0b4184d34a8c9"
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c0f3a9ecd216eddc7621915276b65a9fe5e63ad2"
   },
   "outputs": [],
   "source": [
    "for df in [train_df, test_df, local_test_df]:\n",
    "    df[\"question_text\"] = df[\"question_text\"].str.lower()\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    df[\"question_text\"].fillna(\"_##_\", inplace=True)\n",
    "    \n",
    "x_train = train_df[\"question_text\"].values\n",
    "x_test = test_df[\"question_text\"].values\n",
    "x_test_local = local_test_df[\"question_text\"].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test_local))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_local = tokenizer.texts_to_sequences(x_test_local)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "x_test_local = pad_sequences(x_test_local, maxlen=maxlen)\n",
    "\n",
    "y_train = train_df['target'].values\n",
    "y_test = local_test_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8607fe327a4d7f2af0dfad02cb7fae0a14b0f2b4"
   },
   "source": [
    "# Creating the embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e5df8649c4db0a6e5bbf05088614c8cde4985081"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index, max_features):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    \n",
    "    embeddings_index = []\n",
    "    for o in tqdm(open(EMBEDDING_FILE)):\n",
    "        try:\n",
    "            embeddings_index.append(get_coefs(*o.split(\" \")))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    embeddings_index = dict(embeddings_index)\n",
    "            \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_fasttext(word_index, max_features):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    \n",
    "    embeddings_index = []\n",
    "    for o in tqdm(open(EMBEDDING_FILE)):\n",
    "        if len(o) <= 100:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            coefs = get_coefs(*o.split(\" \"))\n",
    "            assert len(coefs[1]) == 300\n",
    "            embeddings_index.append(coefs)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    embeddings_index = dict(embeddings_index)\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index, max_features):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = []\n",
    "    for o in tqdm(open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore')):\n",
    "        if len(o) <= 100:\n",
    "            continue\n",
    "        try:\n",
    "            coefs = get_coefs(*o.split(\" \"))\n",
    "            assert len(coefs[1]) == 300\n",
    "            embeddings_index.append(coefs)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    embeddings_index = dict(embeddings_index)\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "9f47acd3b1d4bec89a16fda89f3d042e44ea7de3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [02:36, 14044.91it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "1703756it [02:03, 13756.51it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:82: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(95000, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything()\n",
    "\n",
    "glove_embeddings = load_glove(tokenizer.word_index, max_features)\n",
    "paragram_embeddings = load_para(tokenizer.word_index, max_features)\n",
    "\n",
    "embedding_matrix = np.mean([glove_embeddings, paragram_embeddings], axis=0)\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd3804f9bf44762f7715e23099d42f606a4c281b"
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eb9092a72441fede922a687fe5b1c464aed8a973"
   },
   "source": [
    "The only thing I changed about the model is the size of the LSTM and GRU. They had 40 hidden units previously and 60 now. I also changed the number of K-Fold splits to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "a3e084b4fe81baf1bc392d36e2d43b2f6e032331"
   },
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=10).split(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "c78fe9f2abf0db1b3aaf39b97d9303bf03c24d18"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "313b59ffab2f076409a4bd4e680e3b49e9c923a3"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        hidden_size = 60\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = SpatialDropout(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size * 2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(480, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5445e83f96ca47abded343f42ed702ad5ecaca7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49ab78365628601cece06c57a426e2512061966d"
   },
   "source": [
    "Regarding the training procedure, we use Cyclic LR with 5 epochs. I also made a separate function (`train_model`) to train the model because we are going to use it multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "51453ae35fce480b307c64c4e8a82828e785001f"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "1ff4129c342ea6f6de6fd35377a9d426798409d3"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, factor=0.6, min_lr=1e-4, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, torch.optim.Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "        \n",
    "        self.last_loss = np.inf\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "        \n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def step(self, loss):\n",
    "        if loss > self.last_loss:\n",
    "            self.base_lrs = [max(lr * self.factor, self.min_lr) for lr in self.base_lrs]\n",
    "            self.max_lrs = [max(lr * self.factor, self.min_lr) for lr in self.max_lrs]\n",
    "            \n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "9e7f8e30974db91d3681bcba28296e7ef9855e60"
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_val, y_val, validate=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    step_size = 300\n",
    "    scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.003,\n",
    "                         step_size=step_size, mode='exp_range',\n",
    "                         gamma=0.99994)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valid = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "  \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
    "    best_score = -np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_pred = model(x_batch)\n",
    "            scheduler.batch_step()\n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds = np.zeros((x_val_fold.size(0)))\n",
    "        \n",
    "        if validate:\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val.cpu().numpy(), valid_preds)\n",
    "\n",
    "            val_f1, val_threshold = search_result['f1'], search_result['threshold']\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} best_t={:.2f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_f1, val_threshold, elapsed_time))\n",
    "        else:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "   \n",
    "    valid_preds = np.zeros((x_val_fold.size(0)))\n",
    "    \n",
    "    avg_val_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    print('Validation loss: ', avg_val_loss)\n",
    "\n",
    "    test_preds = np.zeros((len(test_loader.dataset)))\n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    \n",
    "    test_preds_local = np.zeros((len(test_local_loader.dataset)))\n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_local_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_local[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    return valid_preds, test_preds, test_preds_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "a65460e7b5b1ede06660ced9ea4a7eeae97ec496"
   },
   "outputs": [],
   "source": [
    "seed = 6017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "b7ad1398071649e90a0f08c85535c03f8e23aa0c"
   },
   "outputs": [],
   "source": [
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x_test_local_cuda = torch.tensor(x_test_local, dtype=torch.long).cuda()\n",
    "test_local = torch.utils.data.TensorDataset(x_test_local_cuda)\n",
    "test_local_loader = torch.utils.data.DataLoader(test_local, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "b7ad1398071649e90a0f08c85535c03f8e23aa0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/5 \t loss=0.1230 \t val_loss=0.1061 \t val_f1=0.6576 best_t=0.33 \t time=48.61s\n",
      "Epoch 2/5 \t loss=0.1035 \t val_loss=0.1033 \t val_f1=0.6699 best_t=0.32 \t time=49.57s\n",
      "Epoch 3/5 \t loss=0.0963 \t val_loss=0.1023 \t val_f1=0.6738 best_t=0.28 \t time=49.83s\n",
      "Epoch 4/5 \t loss=0.0905 \t val_loss=0.1002 \t val_f1=0.6742 best_t=0.37 \t time=49.40s\n",
      "Epoch 5/5 \t loss=0.0851 \t val_loss=0.1016 \t val_f1=0.6746 best_t=0.37 \t time=49.04s\n",
      "Validation loss:  0.10161935366935773\n",
      "Fold 2\n",
      "Epoch 1/5 \t loss=0.1251 \t val_loss=0.1076 \t val_f1=0.6575 best_t=0.44 \t time=50.20s\n",
      "Epoch 2/5 \t loss=0.1044 \t val_loss=0.1014 \t val_f1=0.6708 best_t=0.35 \t time=49.02s\n",
      "Epoch 3/5 \t loss=0.0975 \t val_loss=0.0993 \t val_f1=0.6777 best_t=0.38 \t time=49.51s\n",
      "Epoch 4/5 \t loss=0.0914 \t val_loss=0.0985 \t val_f1=0.6816 best_t=0.34 \t time=49.61s\n",
      "Epoch 5/5 \t loss=0.0860 \t val_loss=0.1011 \t val_f1=0.6774 best_t=0.40 \t time=49.38s\n",
      "Validation loss:  0.10107187095742962\n",
      "Fold 3\n",
      "Epoch 1/5 \t loss=0.1225 \t val_loss=0.1053 \t val_f1=0.6569 best_t=0.35 \t time=49.43s\n",
      "Epoch 2/5 \t loss=0.1030 \t val_loss=0.1026 \t val_f1=0.6693 best_t=0.32 \t time=50.07s\n",
      "Epoch 3/5 \t loss=0.0964 \t val_loss=0.1006 \t val_f1=0.6790 best_t=0.38 \t time=49.53s\n",
      "Epoch 4/5 \t loss=0.0902 \t val_loss=0.0991 \t val_f1=0.6814 best_t=0.40 \t time=49.61s\n",
      "Epoch 5/5 \t loss=0.0848 \t val_loss=0.1022 \t val_f1=0.6797 best_t=0.41 \t time=49.60s\n",
      "Validation loss:  0.10221600582475193\n",
      "Fold 4\n",
      "Epoch 1/5 \t loss=0.1239 \t val_loss=0.1050 \t val_f1=0.6634 best_t=0.37 \t time=50.02s\n",
      "Epoch 2/5 \t loss=0.1042 \t val_loss=0.1018 \t val_f1=0.6727 best_t=0.34 \t time=48.85s\n",
      "Epoch 3/5 \t loss=0.0974 \t val_loss=0.1007 \t val_f1=0.6761 best_t=0.33 \t time=50.19s\n",
      "Epoch 4/5 \t loss=0.0912 \t val_loss=0.0993 \t val_f1=0.6807 best_t=0.41 \t time=49.10s\n",
      "Epoch 5/5 \t loss=0.0860 \t val_loss=0.0996 \t val_f1=0.6778 best_t=0.37 \t time=49.43s\n",
      "Validation loss:  0.09963094978542118\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros(len(train_df))\n",
    "test_preds = np.zeros((len(test_df), len(splits)))\n",
    "test_preds_local = np.zeros((n_test, len(splits)))\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    x_train_fold = torch.tensor(x_train[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(x_train[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    seed_everything(seed + i)\n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "\n",
    "    valid_preds_fold, test_preds_fold, test_preds_local_fold = train_model(model,\n",
    "                                                                           x_train_fold, \n",
    "                                                                           y_train_fold, \n",
    "                                                                           x_val_fold, \n",
    "                                                                           y_val_fold, validate=True)\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds[:, i] = test_preds_fold\n",
    "    test_preds_local[:, i] = test_preds_local_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f647692ec05df5f637f5cb4b4429037ea6d0001"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "2346d3937a2f82bc4512036315e7d32e5509d150"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.4, 'f1': 0.6770549663916017}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = threshold_search(y_train, train_preds)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "429d800ca5ab80e2044349ddc3e2c933b8b2030e"
   },
   "source": [
    "Here we see our very low CV f1 score. But another metric that I have not seen in public kernels yet is the correlation between the test predictions of each fold (in this case the predictions of the local test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "67ed4c346322597ffc27ac55864af321cbbb7c72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936395</td>\n",
       "      <td>0.940116</td>\n",
       "      <td>0.939954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.936395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936422</td>\n",
       "      <td>0.940424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.940116</td>\n",
       "      <td>0.936422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.939954</td>\n",
       "      <td>0.940424</td>\n",
       "      <td>0.940745</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  1.000000  0.936395  0.940116  0.939954\n",
       "1  0.936395  1.000000  0.936422  0.940424\n",
       "2  0.940116  0.936422  1.000000  0.940745\n",
       "3  0.939954  0.940424  0.940745  1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_preds_local).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff8342d54769338955799d74b5b3a10739e2555e"
   },
   "source": [
    "That is astonishingly low! I am used to seeing correlations of > 99% here. Remember that the model architecture of each of these predictions is exactly the same! The only difference is some of the training data and the seed used to initialize the parameters.\n",
    "\n",
    "Because we have very low correlations it makes sense that, when stacking the predictions of each fold, the score gets much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "3829e5fc69a3ec57a1a302ac51664ce0bbd56062"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923298493951116"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, test_preds_local.mean(axis=1) > search_result['threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ff3a64f7e4dd8a466aea5c214cd7b6cb019b276"
   },
   "source": [
    "And now we see a score that is about the same as what could be expected on the leaderboard. So the reason why models that have a high CV score often score badly on the leaderboard is that they seem to have a higher correlation between folds than models with a lower CV score.\n",
    "So the challenge we face with neural networks in this competition is __finding the perfect tradeoff between CV score and correlation between the predictions of each fold__. \n",
    "\n",
    "So when tuning a model, it makes little sense to only track the change in CV score. We have to tune models on a local test set in order to get a valid estimate of how well it will perform on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "e87e9557c51794f9744e01abc27c6ff380422845"
   },
   "outputs": [],
   "source": [
    "submission = test_df[['qid']].copy()\n",
    "submission['prediction'] = test_preds.mean(axis=1) > search_result['threshold']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3311612bd120713b89c05f56bb56d5def0723b3f"
   },
   "source": [
    "# A note on seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c9e3d86ae4058066bb019a2e278a334b86c97eb"
   },
   "source": [
    "You might have noticed the line declaring the random seed to a cryptic value of 6017 above. That is because I hyperparameter-tuned the random seed. That might sound horrifying but, in my opinion, it makes sense in this competition.\n",
    "\n",
    "The problem when tuning the seed without a local test set is that you are bound to overfit to the public test set which will be exchanged in stage 2. However, if we tune the seed on a cross-validation of local test sets, we do not have this risk. And the seed does make a huge difference. Not only on the public leaderboard but also on the local test set that is close to the size of the test set used in stage 2.\n",
    "\n",
    "I said in my first kernel that evaluating the model multiple times will not be necessary anymore because PyTorch behaves deterministically. But I have to correct that statement: PyTorch does behave deterministically, but that only means that we can run the model with one fixed seed and get the same result. That solves the problem of reproducability. But it does not change impact of the seed on the score. If you change some parameter of the model and have an unlucky seed, you might believe that the change was bad. But it could just have been the seed.\n",
    "\n",
    "The code below wraps the regular K-Fold CV in a K-Fold CV for the local test set. Seeds are selected randomly. Every seed takes about 1 hour to evaluate on my machine (GTX 1080 TI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccc1223006544e86c8de69698384266e6a922c08"
   },
   "source": [
    "```python\n",
    "if enable_local_test:\n",
    "    x_train_full = np.concatenate([x_train, x_test_local])\n",
    "    y_train_full = np.concatenate((y_train, y_test))\n",
    "else:\n",
    "    x_train_full = x_train\n",
    "    y_train_full = y_train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "885679bc214fc5c3e3d7970202c1a225f90d5bd8"
   },
   "source": [
    "```python\n",
    "seed_splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=11).split(x_train, y_train))\n",
    "seeds = list(np.random.randint(0, 100000, 8))\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for seed_i, seed in enumerate(seeds):\n",
    "    seed_test_scores = []\n",
    "    seed_train_scores = []\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(seed_splits):\n",
    "        seed_x_train = x_train_full[train_index]\n",
    "        seed_y_train = y_train_full[train_index]\n",
    "        \n",
    "        seed_x_test = x_train_full[val_index]\n",
    "        seed_y_test = y_train_full[val_index]\n",
    "        \n",
    "        splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=10).split(seed_x_train, seed_y_train))\n",
    "        \n",
    "        train_preds = np.zeros(len(seed_x_train))\n",
    "        test_preds_local = np.zeros((len(seed_x_test), len(splits)))\n",
    "        \n",
    "        x_test_local_cuda = torch.tensor(seed_x_test, dtype=torch.long).cuda()\n",
    "        test_local = torch.utils.data.TensorDataset(x_test_local_cuda)\n",
    "        test_local_loader = torch.utils.data.DataLoader(test_local, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print(f'Seed Fold {i + 1}\\n')\n",
    "        \n",
    "        for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "            x_train_fold = torch.tensor(seed_x_train[train_idx], dtype=torch.long).cuda()\n",
    "            y_train_fold = torch.tensor(seed_y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "            x_val_fold = torch.tensor(seed_x_train[valid_idx], dtype=torch.long).cuda()\n",
    "            y_val_fold = torch.tensor(seed_y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "\n",
    "            train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "            valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            print(f'Fold {i + 1}')\n",
    "\n",
    "            seed_everything(seed + i)\n",
    "            model = NeuralNet()\n",
    "            model.cuda()\n",
    "\n",
    "            valid_preds_fold, test_preds_fold, test_preds_local_fold = train_model(model,\n",
    "                                                                                   x_train_fold, \n",
    "                                                                                   y_train_fold, \n",
    "                                                                                   x_val_fold, \n",
    "                                                                                   y_val_fold, validate=True)\n",
    "\n",
    "            train_preds[valid_idx] = valid_preds_fold\n",
    "            test_preds_local[:, i] = test_preds_local_fold\n",
    "\n",
    "        train_search_result = threshold_search(seed_y_train, train_preds)\n",
    "        seed_train_scores.append(train_search_result['f1'])\n",
    "\n",
    "        test_score = f1_score(seed_y_test, test_preds_local.mean(axis=1) > train_search_result['threshold'])\n",
    "        seed_test_scores.append(test_score)\n",
    "    \n",
    "    train_score = np.mean(seed_train_scores)\n",
    "    test_score = np.mean(seed_test_scores)\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "    print('\\ni={} \\t seed={} \\t score={}'.format(seed_i, seed, test_score))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "7ea303bb8ba8dcfe49c4b7f3a8267cf8dcff974b"
   },
   "outputs": [],
   "source": [
    "# loading the results from my local machine here\n",
    "# you have to trust me on this ;)\n",
    "test_scores = [0.6894145809793863, 0.6904706309470233, 0.6905915253597362, 0.6908101789878276, 0.6910334464526553, 0.6916507797390641, 0.6903868185698696, 0.6908830283890897]\n",
    "train_scores = [0.669555770620476, 0.6708382008438574, 0.6700974173065081, 0.6701065866112219, 0.6704778141088164, 0.6708436318389969, 0.6705310002773053, 0.6710429366071224]\n",
    "seeds = [42853, 73399, 21152, 58237, 25688, 6017, 29547, 65803]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81a8fdad23c38e6c05a84808cbff03aaad06e818"
   },
   "source": [
    "Because seed tuning would exceed the runtime of kernels, I copied the results into this kernel. Now we can evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "05c6005c5497625e1e663c86d17f0f8e539ae7b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>local_test_score</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.669556</td>\n",
       "      <td>0.689415</td>\n",
       "      <td>42853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.670838</td>\n",
       "      <td>0.690471</td>\n",
       "      <td>73399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.670097</td>\n",
       "      <td>0.690592</td>\n",
       "      <td>21152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.670107</td>\n",
       "      <td>0.690810</td>\n",
       "      <td>58237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670478</td>\n",
       "      <td>0.691033</td>\n",
       "      <td>25688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cv_score  local_test_score   seed\n",
       "0  0.669556          0.689415  42853\n",
       "1  0.670838          0.690471  73399\n",
       "2  0.670097          0.690592  21152\n",
       "3  0.670107          0.690810  58237\n",
       "4  0.670478          0.691033  25688"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.DataFrame()\n",
    "eval_df['cv_score'] = train_scores\n",
    "eval_df['local_test_score'] = test_scores\n",
    "eval_df['seed'] = seeds\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "f5782e9c2e667e36140a53f630abd764eff13519"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "      <th>local_test_score</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.670844</td>\n",
       "      <td>0.691651</td>\n",
       "      <td>6017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cv_score  local_test_score  seed\n",
       "5  0.670844          0.691651  6017"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.loc[[eval_df['local_test_score'].idxmax()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "bb5e3a3712b02f07e483b2261d7a3e432f18152e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 14))\n",
    "sns.violinplot(x='level_0', y=0, data=eval_df[['cv_score', 'local_test_score']].unstack().reset_index())\n",
    "plt.title('Distribution of scores for different random seeds')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "91bb49b92fb594901728451f0df57692a04044cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8.000000\n",
       "mean     0.690655\n",
       "std      0.000639\n",
       "min      0.689415\n",
       "25%      0.690450\n",
       "50%      0.690701\n",
       "75%      0.690921\n",
       "max      0.691651\n",
       "Name: local_test_score, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['local_test_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f93a3e93eca048e542a4021c05fb8c860dfea4a0"
   },
   "source": [
    "The seed really does have a huge influence on the score. And the influence shown here is evalulated on the whole training set, it is surely even stronger on the ~ 50k rows in the public test set. And keep in mind that the statistic where calculated on a small sample of only 8 seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdc3e1523aa925905c404f5574281e0daaa119fd"
   },
   "source": [
    "# Possible Shortcomings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79b2103d29165a36489f042bbc94ced92d6c6a0e"
   },
   "source": [
    "- The validation technique shown in this kernel only evaluates the model on a subset of the data. It would be ideal to wrap the procedure in another K-Fold cross-validation, but that is computationally hardly feasible (except for seed tuning where it is absolutely necessary).\n",
    "- The model behaves differently when the data in the local test set is added to the training data (e. g. batches are shuffled differently). Thus, it is impossible to tune the seed on a model that is exactly the same as the one used for submitting.\n",
    "- When tuning the model using the shown technique, you will tune it so that it behaves ideally with ~1M training samples. The best architecture for the model also changes when more training data is added (e. g. less need for regularization), so it might again not behave ideally when submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04e73b2d6d7cfb1222742d481e45efbec00867df"
   },
   "source": [
    "# Takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3a5d4cb877d28429e7678b646b93db1e7d7b05e"
   },
   "source": [
    "- The much higher scores on the leaderboard compared to CV scores are caused by a low correlation between folds of K-Fold CV.\n",
    "- When tuning a model, you have to find the best tradeoff between CV score and correlation between folds.\n",
    "- The seed is a valid hyperparameter to tune when not tuning it to the public LB.\n",
    "- Because the seed has a huge influence on the score, the LB score of top public kernels is not a good indicator on how good the model architecture is.\n",
    "- Although this is a kernels-only competition, local compute does matter a lot because you will likely not be able to achieve a good score on the leaderboard without tuning the seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aca20e18311bb580203bdb778e28c43d733bfb48"
   },
   "source": [
    "All of the points above are my current beliefs. I might be wrong about some of them. I'm looking forward to discussion in the comments. Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3857e00ce0c83c1ab52cbbb6fbf7a600be0b5a03"
   },
   "source": [
    "__Note: Version 1 is the one scoring 0.696 on the Leaderboard. The only difference is that `enable_local_test` is set to `False`. Runtime is also only 70 Minutes.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
