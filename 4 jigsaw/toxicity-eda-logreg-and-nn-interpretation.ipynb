{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I'll do EDA and visualization of the data, maybe even modelling, though I plan to do serious modelling in my other kernels.\n",
    "\n",
    "We have quite an interesting data. We are challenged to build a model that recognizes toxicity and minimizes unintended bias with respect to mentions of identities.\n",
    "For examplewe need to make sure that a comment like \"I am a gay woman\" is considered to be not toxic.\n",
    "\n",
    "**Two important points**:\n",
    "1. A subset of comments is labeled with identities. Only identities with more than 500 examples in the test set will be included in the evaluation calculation. This means that not all the test data will be included in evaluation. If we can correctly extract identities, then we will know which test samples are evaluated.\n",
    "2. Target column was created as a fraction of human raters who believed that the comment is toxic. For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic). I think that we could try both regression and classification approaches here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "* [1 Data overview](#overview)\n",
    "* [1.1 Additional toxic subtypes](#add_toxic)\n",
    "* [2 Text overview](#text_overview)\n",
    "* [2.1 Text length](#text_l)\n",
    "* [2.2 Word count](#word_c)\n",
    "* [3 Basic model](#basic_model)\n",
    "* [3.1 Validation function](#validation_function)\n",
    "* [4 ELI5 for model interpretation](#eli5)\n",
    "* [5 Interpreting deep learning models with LIME](#lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jigsaw-public-files', 'jigsaw-unintended-bias-in-toxicity-classification', 'fasttext-crawl-300d-2m']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)\n",
    "pd.set_option('max_columns', 50)\n",
    "import json\n",
    "import altair as alt\n",
    "from  altair.vega import v3\n",
    "from IPython.display import HTML\n",
    "import gc\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import lime\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "requirejs.config({\n",
       "    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
       "    paths: {\"vega\": \"https://cdn.jsdelivr.net/npm/vega@v3.3.1?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@v2.6.0?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@3?noext\"}\n",
       "});\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing altair. I use code from this great kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
    "\n",
    "vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\n",
    "vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
    "vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
    "vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
    "noext = \"?noext\"\n",
    "\n",
    "paths = {\n",
    "    'vega': vega_url + noext,\n",
    "    'vega-lib': vega_lib_url + noext,\n",
    "    'vega-lite': vega_lite_url + noext,\n",
    "    'vega-embed': vega_embed_url + noext\n",
    "}\n",
    "\n",
    "workaround = \"\"\"\n",
    "requirejs.config({{\n",
    "    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
    "    paths: {}\n",
    "}});\n",
    "\"\"\"\n",
    "\n",
    "#------------------------------------------------ Defs for future rendering\n",
    "def add_autoincrement(render_func):\n",
    "    # Keep track of unique <div/> IDs\n",
    "    cache = {}\n",
    "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
    "        if autoincrement:\n",
    "            if id in cache:\n",
    "                counter = 1 + cache[id]\n",
    "                cache[id] = counter\n",
    "            else:\n",
    "                cache[id] = 0\n",
    "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
    "        else:\n",
    "            if id not in cache:\n",
    "                cache[id] = 0\n",
    "            actual_id = id\n",
    "        return render_func(chart, id=actual_id)\n",
    "    # Cache will stay outside and \n",
    "    return wrapped\n",
    "            \n",
    "@add_autoincrement\n",
    "def render(chart, id=\"vega-chart\"):\n",
    "    chart_str = \"\"\"\n",
    "    <div id=\"{id}\"></div><script>\n",
    "    require([\"vega-embed\"], function(vg_embed) {{\n",
    "        const spec = {chart};     \n",
    "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
    "        console.log(\"anything?\");\n",
    "    }});\n",
    "    console.log(\"really...anything?\");\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(\n",
    "        chart_str.format(\n",
    "            id=id,\n",
    "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
    "        )\n",
    "    )\n",
    "\n",
    "HTML(\"\".join((\n",
    "    \"<script>\",\n",
    "    workaround.format(json.dumps(paths)),\n",
    "    \"</script>\",\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "sub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target  \\\n",
       "0  59848  0.000000   \n",
       "1  59849  0.000000   \n",
       "2  59852  0.000000   \n",
       "3  59855  0.000000   \n",
       "4  59856  0.893617   \n",
       "\n",
       "                                                                                                         comment_text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   bisexual  black  buddhist  christian  female  heterosexual  hindu  \\\n",
       "0       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "1       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "2       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "3       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "4       0.0    0.0       0.0        0.0     0.0           0.0    0.0   \n",
       "\n",
       "   homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  \\\n",
       "0                        NaN                                  NaN     NaN   \n",
       "1                        NaN                                  NaN     NaN   \n",
       "2                        NaN                                  NaN     NaN   \n",
       "3                        NaN                                  NaN     NaN   \n",
       "4                        0.0                                 0.25     0.0   \n",
       "\n",
       "   latino  male  muslim  other_disability  other_gender  \\\n",
       "0     NaN   NaN     NaN               NaN           NaN   \n",
       "1     NaN   NaN     NaN               NaN           NaN   \n",
       "2     NaN   NaN     NaN               NaN           NaN   \n",
       "3     NaN   NaN     NaN               NaN           NaN   \n",
       "4     0.0   0.0     0.0               0.0           0.0   \n",
       "\n",
       "   other_race_or_ethnicity  other_religion  other_sexual_orientation  \\\n",
       "0                      NaN             NaN                       NaN   \n",
       "1                      NaN             NaN                       NaN   \n",
       "2                      NaN             NaN                       NaN   \n",
       "3                      NaN             NaN                       NaN   \n",
       "4                      0.0             0.0                       0.0   \n",
       "\n",
       "   physical_disability  psychiatric_or_mental_illness  transgender  white  \\\n",
       "0                  NaN                            NaN          NaN    NaN   \n",
       "1                  NaN                            NaN          NaN    NaN   \n",
       "2                  NaN                            NaN          NaN    NaN   \n",
       "3                  NaN                            NaN          NaN    NaN   \n",
       "4                  0.0                            0.0          0.0    0.0   \n",
       "\n",
       "                    created_date  publication_id  parent_id  article_id  \\\n",
       "0  2015-09-29 10:50:41.987077+00               2        NaN        2006   \n",
       "1  2015-09-29 10:50:42.870083+00               2        NaN        2006   \n",
       "2  2015-09-29 10:50:45.222647+00               2        NaN        2006   \n",
       "3  2015-09-29 10:50:47.601894+00               2        NaN        2006   \n",
       "4  2015-09-29 10:50:48.488476+00               2        NaN        2006   \n",
       "\n",
       "     rating  funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "0  rejected      0    0    0      0         0              0.0   \n",
       "1  rejected      0    0    0      0         0              0.0   \n",
       "2  rejected      0    0    0      0         0              0.0   \n",
       "3  rejected      0    0    0      0         0              0.0   \n",
       "4  rejected      0    0    0      1         0              0.0   \n",
       "\n",
       "   identity_annotator_count  toxicity_annotator_count  \n",
       "0                         0                         4  \n",
       "1                         0                         4  \n",
       "2                         0                         4  \n",
       "3                         0                         4  \n",
       "4                         4                        47  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1804874, 45), (97320, 2), 0.2992508064274847, 0.0799690172277954)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape, (train['target'] > 0).sum() / train.shape[0], (train['target'] >= 0.5).sum() / train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Well said.                                                                                                                                                                                                                                                                                                                                                           184\n",
       "No.                                                                                                                                                                                                                                                                                                                                                                  160\n",
       "Exactly.                                                                                                                                                                                                                                                                                                                                                             132\n",
       "Yes.                                                                                                                                                                                                                                                                                                                                                                 127\n",
       "Thank you.                                                                                                                                                                                                                                                                                                                                                           120\n",
       "Why?                                                                                                                                                                                                                                                                                                                                                                 118\n",
       "Sᴛᴀʀᴛ ᴡᴏʀᴋɪɴɢ ғʀᴏᴍ ʜᴏᴍᴇ! Gʀᴇᴀᴛ ᴊᴏʙ ғᴏʀ sᴛᴜᴅᴇɴᴛs, sᴛᴀʏ-ᴀᴛ-ʜᴏᴍᴇ ᴍᴏᴍs ᴏʀ ᴀɴʏᴏɴᴇ ɴᴇᴇᴅɪɴɢ ᴀɴ ᴇxᴛʀᴀ ɪɴᴄᴏᴍᴇ... Yᴏᴜ ᴏɴʟʏ ɴᴇᴇᴅ ᴀ ᴄᴏᴍᴘᴜᴛᴇʀ ᴀɴᴅ ᴀ ʀᴇʟɪᴀʙʟᴇ ɪɴᴛᴇʀɴᴇᴛ ᴄᴏɴɴᴇᴄᴛɪᴏɴ... Mᴀᴋᴇ $90 ʜᴏᴜʀʟʏ ᴀɴᴅ ᴜᴘ ᴛᴏ $12000 ᴀ ᴍᴏɴᴛʜ ʙʏ ғᴏʟʟᴏᴡɪɴɢ ʟɪɴᴋ ᴀᴛ ᴛʜᴇ ʙᴏᴛᴛᴏᴍ ᴀɴᴅ sɪɢɴɪɴɢ ᴜᴘ... Yᴏᴜ ᴄᴀɴ ʜᴀᴠᴇ ʏᴏᴜʀ ғɪʀsᴛ ᴄʜᴇᴄᴋ ʙʏ ᴛʜᴇ ᴇɴᴅ ᴏғ ᴛʜɪs ᴡᴇᴇᴋ... \\n\\n+++++++++http://www.cashapp24.com/    117\n",
       "Exactly!                                                                                                                                                                                                                                                                                                                                                             103\n",
       "Well said!                                                                                                                                                                                                                                                                                                                                                           100\n",
       "LOL                                                                                                                                                                                                                                                                                                                                                                   95\n",
       "Agreed.                                                                                                                                                                                                                                                                                                                                                               93\n",
       "Start working at home with Google! It's by-far the best job I've had. Last Wednesday I got a brand new BMW since getting a check for $6474 this - 4 weeks past. I began this 8-months ago and immediately was bringing home at least $77 per hour. I work through this link, go to tech tab for work detail.\\n+_+_+_+_+_+_+_+_+ http://www.22moneybay.com             87\n",
       "What?                                                                                                                                                                                                                                                                                                                                                                 84\n",
       "Amen.                                                                                                                                                                                                                                                                                                                                                                 84\n",
       "scary area                                                                                                                                                                                                                                                                                                                                                            76\n",
       "Good.                                                                                                                                                                                                                                                                                                                                                                 76\n",
       "Nope.                                                                                                                                                                                                                                                                                                                                                                 76\n",
       "Huh?                                                                                                                                                                                                                                                                                                                                                                  73\n",
       "Amen!                                                                                                                                                                                                                                                                                                                                                                 73\n",
       "test                                                                                                                                                                                                                                                                                                                                                                  73\n",
       "Name: comment_text, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.16666667])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train['comment_text'] == 'Well said.', 'target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of unique comments: 0.9866744160534198\n"
     ]
    }
   ],
   "source": [
    "print('Rate of unique comments:', train['comment_text'].nunique() / train['comment_text'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1170, 1522)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comments = set(train['comment_text'].values)\n",
    "test_comments = set(test['comment_text'].values)\n",
    "len(train_comments.intersection(test_comments)), len(test.loc[test['comment_text'].isin(list(train_comments.intersection(test_comments)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a lot of data in train - 1.8 mln rows! Test data has less than 100k rows. There are also additional columns in train, we'll look at them later.\n",
    "29% samples have value of target higher than 0 and only 7.99% have target higher than 0.5.\n",
    "- One more point: ~1.4% of all comments are duplicates and they can have different target values.\n",
    "- 1170 unique comments from train data are in test data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"vega-chart\"></div><script>\n",
       "    require([\"vega-embed\"], function(vg_embed) {\n",
       "        const spec = {\"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"config\": {\"view\": {\"height\": 300, \"width\": 400}}, \"data\": {\"name\": \"data-1857363ddd5a4a9198676541ea0237c0\"}, \"datasets\": {\"data-1857363ddd5a4a9198676541ea0237c0\": [{\"bins\": \"(-0.001, 0.05]\", \"target\": 1265492}, {\"bins\": \"(0.05, 0.1]\", \"target\": 15328}, {\"bins\": \"(0.1, 0.15]\", \"target\": 6476}, {\"bins\": \"(0.15, 0.2]\", \"target\": 247384}, {\"bins\": \"(0.2, 0.25]\", \"target\": 1900}, {\"bins\": \"(0.25, 0.3]\", \"target\": 61303}, {\"bins\": \"(0.3, 0.35]\", \"target\": 2531}, {\"bins\": \"(0.35, 0.4]\", \"target\": 52997}, {\"bins\": \"(0.4, 0.45]\", \"target\": 3448}, {\"bins\": \"(0.45, 0.5]\", \"target\": 41577}, {\"bins\": \"(0.5, 0.55]\", \"target\": 4717}, {\"bins\": \"(0.55, 0.6]\", \"target\": 29486}, {\"bins\": \"(0.6, 0.65]\", \"target\": 6628}, {\"bins\": \"(0.65, 0.7]\", \"target\": 20156}, {\"bins\": \"(0.7, 0.75]\", \"target\": 8041}, {\"bins\": \"(0.75, 0.8]\", \"target\": 13608}, {\"bins\": \"(0.8, 0.85]\", \"target\": 9852}, {\"bins\": \"(0.85, 0.9]\", \"target\": 6433}, {\"bins\": \"(0.9, 0.95]\", \"target\": 2456}, {\"bins\": \"(0.95, 1.0]\", \"target\": 5061}]}, \"encoding\": {\"tooltip\": [{\"field\": \"target\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"target\", \"type\": \"quantitative\"}}, \"mark\": \"bar\", \"selection\": {\"selector001\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of target bins\", \"width\": 400};     \n",
       "        vg_embed(\"#vega-chart\", spec, {defaultStyle: true}).catch(console.warn);\n",
       "        console.log(\"anything?\");\n",
       "    });\n",
       "    console.log(\"really...anything?\");\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df = pd.cut(train['target'], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n",
    "hist_df['bins'] = hist_df['bins'].astype(str)\n",
    "render(alt.Chart(hist_df).mark_bar().encode(\n",
    "    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins')),\n",
    "    y=alt.Y('target:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['target', 'bins']\n",
    ").properties(title=\"Counts of target bins\", width=400).interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of comments aren't toxic. We can also see some spikes in the distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    1264764\n",
       "0.166667     138501\n",
       "0.200000     107492\n",
       "0.300000      59098\n",
       "0.400000      50013\n",
       "0.500000      37896\n",
       "0.600000      24175\n",
       "0.100000      14591\n",
       "0.700000      13223\n",
       "0.800000       7029\n",
       "1.000000       4406\n",
       "0.833333       3900\n",
       "0.142857       3390\n",
       "0.111111       1852\n",
       "0.900000       1709\n",
       "0.750000        958\n",
       "0.625000        518\n",
       "0.666667        497\n",
       "0.687500        444\n",
       "0.714286        420\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember how target was created? This is a fraction of voters who considered the comment to be toxic. Then is is completely normal that 0%, 1/6, 1/5 of voters could think the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"vega-chart-1\"></div><script>\n",
       "    require([\"vega-embed\"], function(vg_embed) {\n",
       "        const spec = {\"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"config\": {\"view\": {\"height\": 300, \"width\": 400}}, \"datasets\": {\"data-5d4bcf3d4cc20c9bb140b16c8666654d\": [{\"created_date\": \"2015-09-01T00:00:00\", \"target\": 324}, {\"created_date\": \"2015-10-01T00:00:00\", \"target\": 260}, {\"created_date\": \"2015-11-01T00:00:00\", \"target\": 31}, {\"created_date\": \"2015-12-01T00:00:00\", \"target\": 36}, {\"created_date\": \"2016-01-01T00:00:00\", \"target\": 808}, {\"created_date\": \"2016-02-01T00:00:00\", \"target\": 3861}, {\"created_date\": \"2016-03-01T00:00:00\", \"target\": 14243}, {\"created_date\": \"2016-04-01T00:00:00\", \"target\": 28092}, {\"created_date\": \"2016-05-01T00:00:00\", \"target\": 27483}, {\"created_date\": \"2016-06-01T00:00:00\", \"target\": 30439}, {\"created_date\": \"2016-07-01T00:00:00\", \"target\": 32742}, {\"created_date\": \"2016-08-01T00:00:00\", \"target\": 36415}, {\"created_date\": \"2016-09-01T00:00:00\", \"target\": 35939}, {\"created_date\": \"2016-10-01T00:00:00\", \"target\": 53303}, {\"created_date\": \"2016-11-01T00:00:00\", \"target\": 62613}, {\"created_date\": \"2016-12-01T00:00:00\", \"target\": 108016}, {\"created_date\": \"2017-01-01T00:00:00\", \"target\": 127259}, {\"created_date\": \"2017-02-01T00:00:00\", \"target\": 105828}, {\"created_date\": \"2017-03-01T00:00:00\", \"target\": 115919}, {\"created_date\": \"2017-04-01T00:00:00\", \"target\": 97886}, {\"created_date\": \"2017-05-01T00:00:00\", \"target\": 115173}, {\"created_date\": \"2017-06-01T00:00:00\", \"target\": 142533}, {\"created_date\": \"2017-07-01T00:00:00\", \"target\": 152375}, {\"created_date\": \"2017-08-01T00:00:00\", \"target\": 146384}, {\"created_date\": \"2017-09-01T00:00:00\", \"target\": 145590}, {\"created_date\": \"2017-10-01T00:00:00\", \"target\": 162428}, {\"created_date\": \"2017-11-01T00:00:00\", \"target\": 58894}], \"data-84e2314005c0416d47bc98f71c3ef22a\": [{\"created_date\": \"2015-09-01T00:00:00\", \"target\": 0.06425480116094566}, {\"created_date\": \"2015-10-01T00:00:00\", \"target\": 0.06726784307278609}, {\"created_date\": \"2015-11-01T00:00:00\", \"target\": 0.0}, {\"created_date\": \"2015-12-01T00:00:00\", \"target\": 0.031481481481481485}, {\"created_date\": \"2016-01-01T00:00:00\", \"target\": 0.07606714673766468}, {\"created_date\": \"2016-02-01T00:00:00\", \"target\": 0.06787184026739472}, {\"created_date\": \"2016-03-01T00:00:00\", \"target\": 0.08851217950514355}, {\"created_date\": \"2016-04-01T00:00:00\", \"target\": 0.08952184864681158}, {\"created_date\": \"2016-05-01T00:00:00\", \"target\": 0.08999319803652858}, {\"created_date\": \"2016-06-01T00:00:00\", \"target\": 0.09819733997442676}, {\"created_date\": \"2016-07-01T00:00:00\", \"target\": 0.10376581106782388}, {\"created_date\": \"2016-08-01T00:00:00\", \"target\": 0.10685252759429766}, {\"created_date\": \"2016-09-01T00:00:00\", \"target\": 0.10606122862827311}, {\"created_date\": \"2016-10-01T00:00:00\", \"target\": 0.10368525943113696}, {\"created_date\": \"2016-11-01T00:00:00\", \"target\": 0.10635887166643054}, {\"created_date\": \"2016-12-01T00:00:00\", \"target\": 0.10431511685434779}, {\"created_date\": \"2017-01-01T00:00:00\", \"target\": 0.10578812091081131}, {\"created_date\": \"2017-02-01T00:00:00\", \"target\": 0.10125124148277205}, {\"created_date\": \"2017-03-01T00:00:00\", \"target\": 0.09552681375066854}, {\"created_date\": \"2017-04-01T00:00:00\", \"target\": 0.09684582981359548}, {\"created_date\": \"2017-05-01T00:00:00\", \"target\": 0.09907348785005703}, {\"created_date\": \"2017-06-01T00:00:00\", \"target\": 0.10047669017330972}, {\"created_date\": \"2017-07-01T00:00:00\", \"target\": 0.10082189517892162}, {\"created_date\": \"2017-08-01T00:00:00\", \"target\": 0.11260745641204453}, {\"created_date\": \"2017-09-01T00:00:00\", \"target\": 0.10610711182019587}, {\"created_date\": \"2017-10-01T00:00:00\", \"target\": 0.10940431187998147}, {\"created_date\": \"2017-11-01T00:00:00\", \"target\": 0.10612955675008996}]}, \"layer\": [{\"data\": {\"name\": \"data-84e2314005c0416d47bc98f71c3ef22a\"}, \"encoding\": {\"tooltip\": [{\"field\": \"created_date\", \"timeUnit\": \"yearmonth\", \"type\": \"temporal\"}, {\"field\": \"target\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"title\": \"Date\"}, \"field\": \"created_date\", \"type\": \"temporal\"}, \"y\": {\"axis\": {\"title\": \"Rate\"}, \"field\": \"target\", \"type\": \"quantitative\"}}, \"mark\": \"line\", \"selection\": {\"selector002\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts and toxicity rate of comments\", \"width\": 800}, {\"data\": {\"name\": \"data-5d4bcf3d4cc20c9bb140b16c8666654d\"}, \"encoding\": {\"tooltip\": [{\"field\": \"created_date\", \"timeUnit\": \"yearmonth\", \"type\": \"temporal\"}, {\"field\": \"target\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"title\": \"Date\"}, \"field\": \"created_date\", \"type\": \"temporal\"}, \"y\": {\"axis\": {\"title\": \"Counts\"}, \"field\": \"target\", \"type\": \"quantitative\"}}, \"mark\": {\"color\": \"green\", \"type\": \"line\"}, \"selection\": {\"selector003\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}}], \"resolve\": {\"scale\": {\"y\": \"independent\"}}};     \n",
       "        vg_embed(\"#vega-chart-1\", spec, {defaultStyle: true}).catch(console.warn);\n",
       "        console.log(\"anything?\");\n",
       "    });\n",
       "    console.log(\"really...anything?\");\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['created_date'] = pd.to_datetime(train['created_date']).values.astype('datetime64[M]')\n",
    "counts = train.groupby(['created_date'])['target'].mean().sort_index().reset_index()\n",
    "means = train.groupby(['created_date'])['target'].count().sort_index().reset_index()\n",
    "c = alt.Chart(counts).mark_line().encode(\n",
    "    x=alt.X(\"created_date:T\", axis=alt.Axis(title='Date')),\n",
    "    y=alt.Y('target:Q', axis=alt.Axis(title='Rate')),\n",
    "    tooltip=[alt.Tooltip('created_date:T', timeUnit='yearmonth'), alt.Tooltip('target:Q')]\n",
    ").properties(title=\"Counts and toxicity rate of comments\", width=800).interactive()\n",
    "r = alt.Chart(means).mark_line(color='green').encode(\n",
    "    x=alt.X(\"created_date:T\", axis=alt.Axis(title='Date')),\n",
    "    y=alt.Y('target:Q', axis=alt.Axis(title='Counts')),\n",
    "    tooltip=[alt.Tooltip('created_date:T', timeUnit='yearmonth'), alt.Tooltip('target:Q')],\n",
    ").properties().interactive()\n",
    "render(alt.layer(\n",
    "    c,\n",
    "    r\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how despite the increase of number of comments the toxicity rate is quite stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"add_toxic\"></a>\n",
    "### Additional toxic subtypes\n",
    "\n",
    "Here I plot histogram of scores for additional toxicity subtypes **for scores higher that 0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"vega-chart-2\"></div><script>\n",
       "    require([\"vega-embed\"], function(vg_embed) {\n",
       "        const spec = {\"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"config\": {\"view\": {\"height\": 300, \"width\": 400}}, \"datasets\": {\"data-295de92c408fdb3b4af10a68e8e090cf\": [{\"bins\": \"(-0.000542, 0.0504]\", \"identity_attack\": 27483}, {\"bins\": \"(0.0504, 0.1]\", \"identity_attack\": 57279}, {\"bins\": \"(0.1, 0.15]\", \"identity_attack\": 6076}, {\"bins\": \"(0.15, 0.2]\", \"identity_attack\": 81065}, {\"bins\": \"(0.2, 0.25]\", \"identity_attack\": 1768}, {\"bins\": \"(0.25, 0.3]\", \"identity_attack\": 17650}, {\"bins\": \"(0.3, 0.35]\", \"identity_attack\": 1126}, {\"bins\": \"(0.35, 0.4]\", \"identity_attack\": 11089}, {\"bins\": \"(0.4, 0.45]\", \"identity_attack\": 884}, {\"bins\": \"(0.45, 0.5]\", \"identity_attack\": 6499}, {\"bins\": \"(0.5, 0.55]\", \"identity_attack\": 648}, {\"bins\": \"(0.55, 0.6]\", \"identity_attack\": 3406}, {\"bins\": \"(0.6, 0.65]\", \"identity_attack\": 436}, {\"bins\": \"(0.65, 0.7]\", \"identity_attack\": 1551}, {\"bins\": \"(0.7, 0.75]\", \"identity_attack\": 225}, {\"bins\": \"(0.75, 0.8]\", \"identity_attack\": 684}, {\"bins\": \"(0.8, 0.85]\", \"identity_attack\": 256}, {\"bins\": \"(0.85, 0.9]\", \"identity_attack\": 185}, {\"bins\": \"(0.9, 0.95]\", \"identity_attack\": 50}, {\"bins\": \"(0.95, 1.0]\", \"identity_attack\": 192}], \"data-393d214e9f05b89d5c10cf635735449a\": [{\"bins\": \"(-0.000542, 0.0504]\", \"insult\": 1664}, {\"bins\": \"(0.0504, 0.1]\", \"insult\": 43966}, {\"bins\": \"(0.1, 0.15]\", \"insult\": 5909}, {\"bins\": \"(0.15, 0.2]\", \"insult\": 191390}, {\"bins\": \"(0.2, 0.25]\", \"insult\": 3045}, {\"bins\": \"(0.25, 0.3]\", \"insult\": 50793}, {\"bins\": \"(0.3, 0.35]\", \"insult\": 3190}, {\"bins\": \"(0.35, 0.4]\", \"insult\": 40511}, {\"bins\": \"(0.4, 0.45]\", \"insult\": 3821}, {\"bins\": \"(0.45, 0.5]\", \"insult\": 30363}, {\"bins\": \"(0.5, 0.55]\", \"insult\": 4589}, {\"bins\": \"(0.55, 0.6]\", \"insult\": 21233}, {\"bins\": \"(0.6, 0.65]\", \"insult\": 6089}, {\"bins\": \"(0.65, 0.7]\", \"insult\": 14284}, {\"bins\": \"(0.7, 0.75]\", \"insult\": 6697}, {\"bins\": \"(0.75, 0.8]\", \"insult\": 9401}, {\"bins\": \"(0.8, 0.85]\", \"insult\": 8168}, {\"bins\": \"(0.85, 0.9]\", \"insult\": 3976}, {\"bins\": \"(0.9, 0.95]\", \"insult\": 1434}, {\"bins\": \"(0.95, 1.0]\", \"insult\": 4016}], \"data-3e758d66973bbe8d750db5ac7a3af649\": [{\"bins\": \"(-0.000545, 0.0504]\", \"severe_toxicity\": 42247}, {\"bins\": \"(0.0504, 0.1]\", \"severe_toxicity\": 45850}, {\"bins\": \"(0.1, 0.15]\", \"severe_toxicity\": 2586}, {\"bins\": \"(0.15, 0.2]\", \"severe_toxicity\": 13158}, {\"bins\": \"(0.2, 0.25]\", \"severe_toxicity\": 209}, {\"bins\": \"(0.25, 0.3]\", \"severe_toxicity\": 453}, {\"bins\": \"(0.3, 0.35]\", \"severe_toxicity\": 35}, {\"bins\": \"(0.35, 0.4]\", \"severe_toxicity\": 75}, {\"bins\": \"(0.4, 0.45]\", \"severe_toxicity\": 5}, {\"bins\": \"(0.45, 0.5]\", \"severe_toxicity\": 10}, {\"bins\": \"(0.5, 0.55]\", \"severe_toxicity\": 3}, {\"bins\": \"(0.55, 0.6]\", \"severe_toxicity\": 3}, {\"bins\": \"(0.6, 0.65]\", \"severe_toxicity\": 1}, {\"bins\": \"(0.65, 0.7]\", \"severe_toxicity\": 0}, {\"bins\": \"(0.7, 0.75]\", \"severe_toxicity\": 0}, {\"bins\": \"(0.75, 0.8]\", \"severe_toxicity\": 0}, {\"bins\": \"(0.8, 0.85]\", \"severe_toxicity\": 0}, {\"bins\": \"(0.85, 0.9]\", \"severe_toxicity\": 0}, {\"bins\": \"(0.9, 0.95]\", \"severe_toxicity\": 0}, {\"bins\": \"(0.95, 1.0]\", \"severe_toxicity\": 1}], \"data-5fd19fa210c0ee81a6e75c4efad8c28c\": [{\"bins\": \"(-0.00071, 0.0503]\", \"sexual_explicit\": 15197}, {\"bins\": \"(0.0503, 0.1]\", \"sexual_explicit\": 17634}, {\"bins\": \"(0.1, 0.15]\", \"sexual_explicit\": 1834}, {\"bins\": \"(0.15, 0.2]\", \"sexual_explicit\": 19929}, {\"bins\": \"(0.2, 0.25]\", \"sexual_explicit\": 816}, {\"bins\": \"(0.25, 0.3]\", \"sexual_explicit\": 4546}, {\"bins\": \"(0.3, 0.35]\", \"sexual_explicit\": 575}, {\"bins\": \"(0.35, 0.4]\", \"sexual_explicit\": 2917}, {\"bins\": \"(0.4, 0.45]\", \"sexual_explicit\": 439}, {\"bins\": \"(0.45, 0.5]\", \"sexual_explicit\": 1920}, {\"bins\": \"(0.5, 0.55]\", \"sexual_explicit\": 349}, {\"bins\": \"(0.55, 0.6]\", \"sexual_explicit\": 1117}, {\"bins\": \"(0.6, 0.65]\", \"sexual_explicit\": 256}, {\"bins\": \"(0.65, 0.7]\", \"sexual_explicit\": 639}, {\"bins\": \"(0.7, 0.75]\", \"sexual_explicit\": 172}, {\"bins\": \"(0.75, 0.8]\", \"sexual_explicit\": 275}, {\"bins\": \"(0.8, 0.85]\", \"sexual_explicit\": 103}, {\"bins\": \"(0.85, 0.9]\", \"sexual_explicit\": 97}, {\"bins\": \"(0.9, 0.95]\", \"sexual_explicit\": 41}, {\"bins\": \"(0.95, 1.0]\", \"sexual_explicit\": 70}], \"data-9c752742aa8e5a40debad544aacd8d74\": [{\"bins\": \"(-0.000544, 0.0504]\", \"obscene\": 22002}, {\"bins\": \"(0.0504, 0.1]\", \"obscene\": 50820}, {\"bins\": \"(0.1, 0.15]\", \"obscene\": 10149}, {\"bins\": \"(0.15, 0.2]\", \"obscene\": 37798}, {\"bins\": \"(0.2, 0.25]\", \"obscene\": 3700}, {\"bins\": \"(0.25, 0.3]\", \"obscene\": 6819}, {\"bins\": \"(0.3, 0.35]\", \"obscene\": 1181}, {\"bins\": \"(0.35, 0.4]\", \"obscene\": 3769}, {\"bins\": \"(0.4, 0.45]\", \"obscene\": 736}, {\"bins\": \"(0.45, 0.5]\", \"obscene\": 2553}, {\"bins\": \"(0.5, 0.55]\", \"obscene\": 586}, {\"bins\": \"(0.55, 0.6]\", \"obscene\": 1970}, {\"bins\": \"(0.6, 0.65]\", \"obscene\": 713}, {\"bins\": \"(0.65, 0.7]\", \"obscene\": 1560}, {\"bins\": \"(0.7, 0.75]\", \"obscene\": 729}, {\"bins\": \"(0.75, 0.8]\", \"obscene\": 975}, {\"bins\": \"(0.8, 0.85]\", \"obscene\": 479}, {\"bins\": \"(0.85, 0.9]\", \"obscene\": 323}, {\"bins\": \"(0.9, 0.95]\", \"obscene\": 107}, {\"bins\": \"(0.95, 1.0]\", \"obscene\": 206}], \"data-c2614004b79f384ca2c3efe7a00cb930\": [{\"bins\": \"(-0.000544, 0.0504]\", \"threat\": 20289}, {\"bins\": \"(0.0504, 0.1]\", \"threat\": 32113}, {\"bins\": \"(0.1, 0.15]\", \"threat\": 2629}, {\"bins\": \"(0.15, 0.2]\", \"threat\": 36796}, {\"bins\": \"(0.2, 0.25]\", \"threat\": 841}, {\"bins\": \"(0.25, 0.3]\", \"threat\": 5741}, {\"bins\": \"(0.3, 0.35]\", \"threat\": 420}, {\"bins\": \"(0.35, 0.4]\", \"threat\": 3314}, {\"bins\": \"(0.4, 0.45]\", \"threat\": 280}, {\"bins\": \"(0.45, 0.5]\", \"threat\": 1713}, {\"bins\": \"(0.5, 0.55]\", \"threat\": 204}, {\"bins\": \"(0.55, 0.6]\", \"threat\": 994}, {\"bins\": \"(0.6, 0.65]\", \"threat\": 147}, {\"bins\": \"(0.65, 0.7]\", \"threat\": 527}, {\"bins\": \"(0.7, 0.75]\", \"threat\": 112}, {\"bins\": \"(0.75, 0.8]\", \"threat\": 266}, {\"bins\": \"(0.8, 0.85]\", \"threat\": 202}, {\"bins\": \"(0.85, 0.9]\", \"threat\": 121}, {\"bins\": \"(0.9, 0.95]\", \"threat\": 60}, {\"bins\": \"(0.95, 1.0]\", \"threat\": 160}]}, \"vconcat\": [{\"hconcat\": [{\"data\": {\"name\": \"data-3e758d66973bbe8d750db5ac7a3af649\"}, \"encoding\": {\"tooltip\": [{\"field\": \"severe_toxicity\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"severe_toxicity\", \"type\": \"quantitative\"}}, \"height\": 200, \"mark\": \"bar\", \"selection\": {\"selector004\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of severe_toxicity bins\", \"width\": 300}, {\"data\": {\"name\": \"data-9c752742aa8e5a40debad544aacd8d74\"}, \"encoding\": {\"tooltip\": [{\"field\": \"obscene\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"obscene\", \"type\": \"quantitative\"}}, \"height\": 200, \"mark\": \"bar\", \"selection\": {\"selector005\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of obscene bins\", \"width\": 300}]}, {\"hconcat\": [{\"data\": {\"name\": \"data-c2614004b79f384ca2c3efe7a00cb930\"}, \"encoding\": {\"tooltip\": [{\"field\": \"threat\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"threat\", \"type\": \"quantitative\"}}, \"height\": 200, \"mark\": \"bar\", \"selection\": {\"selector006\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of threat bins\", \"width\": 300}, {\"data\": {\"name\": \"data-393d214e9f05b89d5c10cf635735449a\"}, \"encoding\": {\"tooltip\": [{\"field\": \"insult\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"insult\", \"type\": \"quantitative\"}}, \"height\": 200, \"mark\": \"bar\", \"selection\": {\"selector007\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of insult bins\", \"width\": 300}]}, {\"hconcat\": [{\"data\": {\"name\": \"data-295de92c408fdb3b4af10a68e8e090cf\"}, \"encoding\": {\"tooltip\": [{\"field\": \"identity_attack\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"identity_attack\", \"type\": \"quantitative\"}}, \"height\": 200, \"mark\": \"bar\", \"selection\": {\"selector008\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of identity_attack bins\", \"width\": 300}, {\"data\": {\"name\": \"data-5fd19fa210c0ee81a6e75c4efad8c28c\"}, \"encoding\": {\"tooltip\": [{\"field\": \"sexual_explicit\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"sexual_explicit\", \"type\": \"quantitative\"}}, \"height\": 200, \"mark\": \"bar\", \"selection\": {\"selector009\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of sexual_explicit bins\", \"width\": 300}]}]};     \n",
       "        vg_embed(\"#vega-chart-2\", spec, {defaultStyle: true}).catch(console.warn);\n",
       "        console.log(\"anything?\");\n",
       "    });\n",
       "    console.log(\"really...anything?\");\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_dict = {}\n",
    "for col in ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']:\n",
    "    df_ = train.loc[train[col] > 0]\n",
    "    hist_df = pd.cut(df_[col], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n",
    "    hist_df['bins'] = hist_df['bins'].astype(str)\n",
    "    plot_dict[col] = alt.Chart(hist_df).mark_bar().encode(\n",
    "        x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins')),\n",
    "        y=alt.Y(f'{col}:Q', axis=alt.Axis(title='Count')),\n",
    "        tooltip=[col, 'bins']\n",
    "    ).properties(title=f\"Counts of {col} bins\", width=300, height=200).interactive()\n",
    "    \n",
    "render((plot_dict['severe_toxicity'] | plot_dict['obscene']) & (plot_dict['threat'] | plot_dict['insult']) & (plot_dict['identity_attack'] | plot_dict['sexual_explicit']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"text_overview\"></a>\n",
    "## Text exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"text_l\"></a>\n",
    "### Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"vega-chart-3\"></div><script>\n",
       "    require([\"vega-embed\"], function(vg_embed) {\n",
       "        const spec = {\"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"config\": {\"view\": {\"height\": 300, \"width\": 400}}, \"data\": {\"name\": \"data-a58cc2365d41e45c65df8ff467b5f248\"}, \"datasets\": {\"data-a58cc2365d41e45c65df8ff467b5f248\": [{\"bins\": \"(-0.905, 191.5]\", \"comment_text\": 868541}, {\"bins\": \"(191.5, 382.0]\", \"comment_text\": 438545}, {\"bins\": \"(382.0, 572.5]\", \"comment_text\": 210698}, {\"bins\": \"(572.5, 763.0]\", \"comment_text\": 114504}, {\"bins\": \"(763.0, 953.5]\", \"comment_text\": 82887}, {\"bins\": \"(953.5, 1144.0]\", \"comment_text\": 89692}, {\"bins\": \"(1144.0, 1334.5]\", \"comment_text\": 0}, {\"bins\": \"(1334.5, 1525.0]\", \"comment_text\": 0}, {\"bins\": \"(1525.0, 1715.5]\", \"comment_text\": 4}, {\"bins\": \"(1715.5, 1906.0]\", \"comment_text\": 3}]}, \"encoding\": {\"tooltip\": [{\"field\": \"comment_text\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"sort\": [\"(-0.905, 191.5]\", \"(191.5, 382.0]\", \"(382.0, 572.5]\", \"(572.5, 763.0]\", \"(763.0, 953.5]\", \"(953.5, 1144.0]\", \"(1144.0, 1334.5]\", \"(1334.5, 1525.0]\", \"(1525.0, 1715.5]\", \"(1715.5, 1906.0]\"], \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"comment_text\", \"type\": \"quantitative\"}}, \"mark\": \"bar\", \"selection\": {\"selector010\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of target bins of text length\", \"width\": 400};     \n",
       "        vg_embed(\"#vega-chart-3\", spec, {defaultStyle: true}).catch(console.warn);\n",
       "        console.log(\"anything?\");\n",
       "    });\n",
       "    console.log(\"really...anything?\");\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df = pd.cut(train['comment_text'].apply(lambda x: len(x)), 10).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n",
    "hist_df['bins'] = hist_df['bins'].astype(str)\n",
    "render(alt.Chart(hist_df).mark_bar().encode(\n",
    "    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins'), sort=list(hist_df['bins'].values)),\n",
    "    y=alt.Y('comment_text:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['comment_text', 'bins']\n",
    ").properties(title=\"Counts of target bins of text length\", width=400).interactive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = train['comment_text'].apply(lambda x: len(x)).value_counts(normalize=True).sort_index().cumsum().reset_index().rename(columns={'index': 'Text length'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"vega-chart-4\"></div><script>\n",
       "    require([\"vega-embed\"], function(vg_embed) {\n",
       "        const spec = {\"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"config\": {\"view\": {\"height\": 300, \"width\": 400}}, \"data\": {\"name\": \"data-d8b3b2518e525ea16ff14329d9ad3092\"}, \"datasets\": {\"data-d8b3b2518e525ea16ff14329d9ad3092\": [{\"Text length\": 1, \"comment_text\": 8.754073691570713e-05}, {\"Text length\": 2, \"comment_text\": 0.00022106806347700725}, {\"Text length\": 3, \"comment_text\": 0.0007512989826436638}, {\"Text length\": 4, \"comment_text\": 0.0017929229408811917}, {\"Text length\": 5, \"comment_text\": 0.0027929927518486053}, {\"Text length\": 6, \"comment_text\": 0.0036467919644252176}, {\"Text length\": 7, \"comment_text\": 0.004649077996580371}, {\"Text length\": 8, \"comment_text\": 0.0058973645805746}, {\"Text length\": 9, \"comment_text\": 0.007162272823476874}, {\"Text length\": 10, \"comment_text\": 0.008943560603122434}, {\"Text length\": 11, \"comment_text\": 0.010461672116723938}, {\"Text length\": 12, \"comment_text\": 0.011933797040679848}, {\"Text length\": 13, \"comment_text\": 0.013538895235900125}, {\"Text length\": 14, \"comment_text\": 0.015193858407844537}, {\"Text length\": 15, \"comment_text\": 0.016961294805066727}, {\"Text length\": 16, \"comment_text\": 0.018835109819300408}, {\"Text length\": 17, \"comment_text\": 0.020821398058811863}, {\"Text length\": 18, \"comment_text\": 0.022822091736043625}, {\"Text length\": 19, \"comment_text\": 0.02473746089754742}, {\"Text length\": 20, \"comment_text\": 0.02688719544965466}, {\"Text length\": 21, \"comment_text\": 0.029014767789884505}, {\"Text length\": 22, \"comment_text\": 0.031258691742470666}, {\"Text length\": 23, \"comment_text\": 0.03354694011881162}, {\"Text length\": 24, \"comment_text\": 0.03587397236593801}, {\"Text length\": 25, \"comment_text\": 0.03820820733192456}, {\"Text length\": 26, \"comment_text\": 0.04067763179036321}, {\"Text length\": 27, \"comment_text\": 0.04318916445136891}, {\"Text length\": 28, \"comment_text\": 0.04577826485394549}, {\"Text length\": 29, \"comment_text\": 0.048488149311253864}, {\"Text length\": 30, \"comment_text\": 0.051176979667278716}, {\"Text length\": 31, \"comment_text\": 0.053810958548907016}, {\"Text length\": 32, \"comment_text\": 0.056514194342652174}, {\"Text length\": 33, \"comment_text\": 0.0592551058965889}, {\"Text length\": 34, \"comment_text\": 0.062122896113523715}, {\"Text length\": 35, \"comment_text\": 0.06496686195269033}, {\"Text length\": 36, \"comment_text\": 0.06789615230758492}, {\"Text length\": 37, \"comment_text\": 0.07080438856119597}, {\"Text length\": 38, \"comment_text\": 0.07376470601271891}, {\"Text length\": 39, \"comment_text\": 0.07670784775003686}, {\"Text length\": 40, \"comment_text\": 0.07976346271263259}, {\"Text length\": 41, \"comment_text\": 0.08277696947266126}, {\"Text length\": 42, \"comment_text\": 0.08584144932000795}, {\"Text length\": 43, \"comment_text\": 0.08903668621743126}, {\"Text length\": 44, \"comment_text\": 0.0922435582760902}, {\"Text length\": 45, \"comment_text\": 0.09531136245521848}, {\"Text length\": 46, \"comment_text\": 0.0984572884312146}, {\"Text length\": 47, \"comment_text\": 0.10167856592759385}, {\"Text length\": 48, \"comment_text\": 0.10491591102758421}, {\"Text length\": 49, \"comment_text\": 0.10808676949194239}, {\"Text length\": 50, \"comment_text\": 0.11133962814024692}, {\"Text length\": 51, \"comment_text\": 0.11456589213429859}, {\"Text length\": 52, \"comment_text\": 0.1177467235940016}, {\"Text length\": 53, \"comment_text\": 0.12098240652810115}, {\"Text length\": 54, \"comment_text\": 0.12420867052215281}, {\"Text length\": 55, \"comment_text\": 0.12738673170537115}, {\"Text length\": 56, \"comment_text\": 0.1306905634409937}, {\"Text length\": 57, \"comment_text\": 0.13397057079884803}, {\"Text length\": 58, \"comment_text\": 0.13724171327195142}, {\"Text length\": 59, \"comment_text\": 0.14055163961584025}, {\"Text length\": 60, \"comment_text\": 0.1437568495086084}, {\"Text length\": 61, \"comment_text\": 0.14696593778845507}, {\"Text length\": 62, \"comment_text\": 0.15032739127495884}, {\"Text length\": 63, \"comment_text\": 0.15358135803385725}, {\"Text length\": 64, \"comment_text\": 0.15680540580672117}, {\"Text length\": 65, \"comment_text\": 0.1601203186482824}, {\"Text length\": 66, \"comment_text\": 0.16334492047644328}, {\"Text length\": 67, \"comment_text\": 0.1666310224425639}, {\"Text length\": 68, \"comment_text\": 0.16987058376374198}, {\"Text length\": 69, \"comment_text\": 0.17310349642135686}, {\"Text length\": 70, \"comment_text\": 0.17634638207431658}, {\"Text length\": 71, \"comment_text\": 0.1796119839944506}, {\"Text length\": 72, \"comment_text\": 0.18289864001586817}, {\"Text length\": 73, \"comment_text\": 0.18618585009258268}, {\"Text length\": 74, \"comment_text\": 0.18942707357965158}, {\"Text length\": 75, \"comment_text\": 0.19259516176752509}, {\"Text length\": 76, \"comment_text\": 0.19584580419464187}, {\"Text length\": 77, \"comment_text\": 0.19910586556180657}, {\"Text length\": 78, \"comment_text\": 0.2022312914918161}, {\"Text length\": 79, \"comment_text\": 0.20541932567037927}, {\"Text length\": 80, \"comment_text\": 0.20856913003345393}, {\"Text length\": 81, \"comment_text\": 0.2117416506637029}, {\"Text length\": 82, \"comment_text\": 0.21485156304539824}, {\"Text length\": 83, \"comment_text\": 0.2179825295283771}, {\"Text length\": 84, \"comment_text\": 0.22109909057363566}, {\"Text length\": 85, \"comment_text\": 0.22424612466022564}, {\"Text length\": 86, \"comment_text\": 0.2272546449225819}, {\"Text length\": 87, \"comment_text\": 0.23033630048413356}, {\"Text length\": 88, \"comment_text\": 0.23348167240483272}, {\"Text length\": 89, \"comment_text\": 0.23648188183773497}, {\"Text length\": 90, \"comment_text\": 0.23960232127007208}, {\"Text length\": 91, \"comment_text\": 0.24254934139446857}, {\"Text length\": 92, \"comment_text\": 0.2455063345142099}, {\"Text length\": 93, \"comment_text\": 0.24850820611300295}, {\"Text length\": 94, \"comment_text\": 0.25149788849526344}, {\"Text length\": 95, \"comment_text\": 0.25447261138450666}, {\"Text length\": 96, \"comment_text\": 0.25749055058691084}, {\"Text length\": 97, \"comment_text\": 0.2604242733841809}, {\"Text length\": 98, \"comment_text\": 0.263340820467246}, {\"Text length\": 99, \"comment_text\": 0.26629781358698734}, {\"Text length\": 100, \"comment_text\": 0.26923430666074205}, {\"Text length\": 101, \"comment_text\": 0.2721309077531175}, {\"Text length\": 102, \"comment_text\": 0.2749576978780791}, {\"Text length\": 103, \"comment_text\": 0.27782437998442006}, {\"Text length\": 104, \"comment_text\": 0.28062180517864416}, {\"Text length\": 105, \"comment_text\": 0.2834513655800905}, {\"Text length\": 106, \"comment_text\": 0.2861906149681364}, {\"Text length\": 107, \"comment_text\": 0.28900632398715936}, {\"Text length\": 108, \"comment_text\": 0.2917871275224754}, {\"Text length\": 109, \"comment_text\": 0.29453080935289677}, {\"Text length\": 110, \"comment_text\": 0.2971803017828393}, {\"Text length\": 111, \"comment_text\": 0.2999289701109331}, {\"Text length\": 112, \"comment_text\": 0.3026676654436821}, {\"Text length\": 113, \"comment_text\": 0.3054490230342951}, {\"Text length\": 114, \"comment_text\": 0.30810738034898855}, {\"Text length\": 115, \"comment_text\": 0.31080008909209195}, {\"Text length\": 116, \"comment_text\": 0.31344958152203445}, {\"Text length\": 117, \"comment_text\": 0.3160774657953965}, {\"Text length\": 118, \"comment_text\": 0.318735269054793}, {\"Text length\": 119, \"comment_text\": 0.3214091399178006}, {\"Text length\": 120, \"comment_text\": 0.3240259430852239}, {\"Text length\": 121, \"comment_text\": 0.32663111109141163}, {\"Text length\": 122, \"comment_text\": 0.3292772791895725}, {\"Text length\": 123, \"comment_text\": 0.331807649730674}, {\"Text length\": 124, \"comment_text\": 0.3343889933590935}, {\"Text length\": 125, \"comment_text\": 0.33693155311672757}, {\"Text length\": 126, \"comment_text\": 0.3394325587270915}, {\"Text length\": 127, \"comment_text\": 0.34198786175655504}, {\"Text length\": 128, \"comment_text\": 0.34447390787390175}, {\"Text length\": 129, \"comment_text\": 0.34701037302326954}, {\"Text length\": 130, \"comment_text\": 0.34954240573026185}, {\"Text length\": 131, \"comment_text\": 0.3520229112946392}, {\"Text length\": 132, \"comment_text\": 0.3544862411448116}, {\"Text length\": 133, \"comment_text\": 0.3569046925159322}, {\"Text length\": 134, \"comment_text\": 0.35930873844933253}, {\"Text length\": 135, \"comment_text\": 0.3617692980230202}, {\"Text length\": 136, \"comment_text\": 0.36424315492383436}, {\"Text length\": 137, \"comment_text\": 0.36665163329961015}, {\"Text length\": 138, \"comment_text\": 0.3691227199239396}, {\"Text length\": 139, \"comment_text\": 0.3715251036914491}, {\"Text length\": 140, \"comment_text\": 0.373898676583518}, {\"Text length\": 141, \"comment_text\": 0.3762428845448494}, {\"Text length\": 142, \"comment_text\": 0.37855384918836466}, {\"Text length\": 143, \"comment_text\": 0.3809313004675121}, {\"Text length\": 144, \"comment_text\": 0.38328548142418833}, {\"Text length\": 145, \"comment_text\": 0.38567345975397754}, {\"Text length\": 146, \"comment_text\": 0.38802487043416906}, {\"Text length\": 147, \"comment_text\": 0.39036132162134335}, {\"Text length\": 148, \"comment_text\": 0.39263959700233947}, {\"Text length\": 149, \"comment_text\": 0.394897372337349}, {\"Text length\": 150, \"comment_text\": 0.3971385260134505}, {\"Text length\": 151, \"comment_text\": 0.39945392309934125}, {\"Text length\": 152, \"comment_text\": 0.4017116984343508}, {\"Text length\": 153, \"comment_text\": 0.40395562238693694}, {\"Text length\": 154, \"comment_text\": 0.40620453283719554}, {\"Text length\": 155, \"comment_text\": 0.40843128107557675}, {\"Text length\": 156, \"comment_text\": 0.41058821834654413}, {\"Text length\": 157, \"comment_text\": 0.4128554126216016}, {\"Text length\": 158, \"comment_text\": 0.41506609325637167}, {\"Text length\": 159, \"comment_text\": 0.4172429765180287}, {\"Text length\": 160, \"comment_text\": 0.41936888669236777}, {\"Text length\": 161, \"comment_text\": 0.42151086447031794}, {\"Text length\": 162, \"comment_text\": 0.4236384368105478}, {\"Text length\": 163, \"comment_text\": 0.4257444009941972}, {\"Text length\": 164, \"comment_text\": 0.42784814895665885}, {\"Text length\": 165, \"comment_text\": 0.4299408158131818}, {\"Text length\": 166, \"comment_text\": 0.4320268340061415}, {\"Text length\": 167, \"comment_text\": 0.43417213611587324}, {\"Text length\": 168, \"comment_text\": 0.436326303110356}, {\"Text length\": 169, \"comment_text\": 0.4383774158196088}, {\"Text length\": 170, \"comment_text\": 0.44044792046425435}, {\"Text length\": 171, \"comment_text\": 0.44247354662984817}, {\"Text length\": 172, \"comment_text\": 0.4444853214130186}, {\"Text length\": 173, \"comment_text\": 0.4465214746292542}, {\"Text length\": 174, \"comment_text\": 0.44853158724653386}, {\"Text length\": 175, \"comment_text\": 0.45048075378115066}, {\"Text length\": 176, \"comment_text\": 0.4524775690713039}, {\"Text length\": 177, \"comment_text\": 0.4544572086472521}, {\"Text length\": 178, \"comment_text\": 0.4564024967947904}, {\"Text length\": 179, \"comment_text\": 0.45835664982707963}, {\"Text length\": 180, \"comment_text\": 0.46028642442630374}, {\"Text length\": 181, \"comment_text\": 0.46219403681365046}, {\"Text length\": 182, \"comment_text\": 0.4640911221503554}, {\"Text length\": 183, \"comment_text\": 0.466025329191955}, {\"Text length\": 184, \"comment_text\": 0.46794291457464654}, {\"Text length\": 185, \"comment_text\": 0.4698904189433726}, {\"Text length\": 186, \"comment_text\": 0.4718140989343304}, {\"Text length\": 187, \"comment_text\": 0.4737128464369261}, {\"Text length\": 188, \"comment_text\": 0.47561602638189737}, {\"Text length\": 189, \"comment_text\": 0.4775313955434012}, {\"Text length\": 190, \"comment_text\": 0.47939800783877473}, {\"Text length\": 191, \"comment_text\": 0.4812197416550965}, {\"Text length\": 192, \"comment_text\": 0.48307582689982825}, {\"Text length\": 193, \"comment_text\": 0.4849285878127784}, {\"Text length\": 194, \"comment_text\": 0.48679520010815197}, {\"Text length\": 195, \"comment_text\": 0.48861859609036457}, {\"Text length\": 196, \"comment_text\": 0.4904004379253071}, {\"Text length\": 197, \"comment_text\": 0.49218671220262505}, {\"Text length\": 198, \"comment_text\": 0.4939774189223185}, {\"Text length\": 199, \"comment_text\": 0.49578308513502917}, {\"Text length\": 200, \"comment_text\": 0.49754553503457893}, {\"Text length\": 201, \"comment_text\": 0.4993523093578835}, {\"Text length\": 202, \"comment_text\": 0.501116421423324}, {\"Text length\": 203, \"comment_text\": 0.502877209156983}, {\"Text length\": 204, \"comment_text\": 0.5046507401624715}, {\"Text length\": 205, \"comment_text\": 0.506449203656322}, {\"Text length\": 206, \"comment_text\": 0.5081396263672702}, {\"Text length\": 207, \"comment_text\": 0.509893765437366}, {\"Text length\": 208, \"comment_text\": 0.5115958233095499}, {\"Text length\": 209, \"comment_text\": 0.513279597356935}, {\"Text length\": 210, \"comment_text\": 0.5150193309893104}, {\"Text length\": 211, \"comment_text\": 0.5167191726403065}, {\"Text length\": 212, \"comment_text\": 0.5184533657197127}, {\"Text length\": 213, \"comment_text\": 0.5201271667717529}, {\"Text length\": 214, \"comment_text\": 0.5218037381002777}, {\"Text length\": 215, \"comment_text\": 0.523426012009703}, {\"Text length\": 216, \"comment_text\": 0.5250831914028351}, {\"Text length\": 217, \"comment_text\": 0.5267603167866569}, {\"Text length\": 218, \"comment_text\": 0.5283604284842047}, {\"Text length\": 219, \"comment_text\": 0.5299948916101626}, {\"Text length\": 220, \"comment_text\": 0.5316099628007277}, {\"Text length\": 221, \"comment_text\": 0.5332582773091089}, {\"Text length\": 222, \"comment_text\": 0.5348467538454211}, {\"Text length\": 223, \"comment_text\": 0.536471798031331}, {\"Text length\": 224, \"comment_text\": 0.5380824367795207}, {\"Text length\": 225, \"comment_text\": 0.5396487511039556}, {\"Text length\": 226, \"comment_text\": 0.541278227732241}, {\"Text length\": 227, \"comment_text\": 0.5428495285543482}, {\"Text length\": 228, \"comment_text\": 0.5445050457815895}, {\"Text length\": 229, \"comment_text\": 0.5460674817189458}, {\"Text length\": 230, \"comment_text\": 0.547605539223237}, {\"Text length\": 231, \"comment_text\": 0.5491690832711871}, {\"Text length\": 232, \"comment_text\": 0.5507847085170491}, {\"Text length\": 233, \"comment_text\": 0.5523526850073748}, {\"Text length\": 234, \"comment_text\": 0.5538447559220203}, {\"Text length\": 235, \"comment_text\": 0.5553628674356218}, {\"Text length\": 236, \"comment_text\": 0.5568732221750662}, {\"Text length\": 237, \"comment_text\": 0.5583242929977386}, {\"Text length\": 238, \"comment_text\": 0.5597964179216944}, {\"Text length\": 239, \"comment_text\": 0.5613200699882653}, {\"Text length\": 240, \"comment_text\": 0.5627883165251427}, {\"Text length\": 241, \"comment_text\": 0.5642754009421159}, {\"Text length\": 242, \"comment_text\": 0.5657364447601332}, {\"Text length\": 243, \"comment_text\": 0.5672628671031887}, {\"Text length\": 244, \"comment_text\": 0.5687566001837251}, {\"Text length\": 245, \"comment_text\": 0.5702170899464454}, {\"Text length\": 246, \"comment_text\": 0.5716759175432748}, {\"Text length\": 247, \"comment_text\": 0.5731347451401043}, {\"Text length\": 248, \"comment_text\": 0.5745974511240123}, {\"Text length\": 249, \"comment_text\": 0.5760252516242134}, {\"Text length\": 250, \"comment_text\": 0.5774602548432747}, {\"Text length\": 251, \"comment_text\": 0.578894149951742}, {\"Text length\": 252, \"comment_text\": 0.5802970179635811}, {\"Text length\": 253, \"comment_text\": 0.5817148454684374}, {\"Text length\": 254, \"comment_text\": 0.5831005377660715}, {\"Text length\": 255, \"comment_text\": 0.5845095003861769}, {\"Text length\": 256, \"comment_text\": 0.5858852196884661}, {\"Text length\": 257, \"comment_text\": 0.5873174526310426}, {\"Text length\": 258, \"comment_text\": 0.5886460772330924}, {\"Text length\": 259, \"comment_text\": 0.5900195803141939}, {\"Text length\": 260, \"comment_text\": 0.5913831103999505}, {\"Text length\": 261, \"comment_text\": 0.5927632621446153}, {\"Text length\": 262, \"comment_text\": 0.5941162651797302}, {\"Text length\": 263, \"comment_text\": 0.5954714844360328}, {\"Text length\": 264, \"comment_text\": 0.5967929063192224}, {\"Text length\": 265, \"comment_text\": 0.5981182065894907}, {\"Text length\": 266, \"comment_text\": 0.5994047229889734}, {\"Text length\": 267, \"comment_text\": 0.6007926315077953}, {\"Text length\": 268, \"comment_text\": 0.6021373237134562}, {\"Text length\": 269, \"comment_text\": 0.6034565293754581}, {\"Text length\": 270, \"comment_text\": 0.6047890323645865}, {\"Text length\": 271, \"comment_text\": 0.6061159948007454}, {\"Text length\": 272, \"comment_text\": 0.6074435112922013}, {\"Text length\": 273, \"comment_text\": 0.6087261493046054}, {\"Text length\": 274, \"comment_text\": 0.6099993683769618}, {\"Text length\": 275, \"comment_text\": 0.6113002902141649}, {\"Text length\": 276, \"comment_text\": 0.6126034282725557}, {\"Text length\": 277, \"comment_text\": 0.6139276204322299}, {\"Text length\": 278, \"comment_text\": 0.6151897583986475}, {\"Text length\": 279, \"comment_text\": 0.6164485720332835}, {\"Text length\": 280, \"comment_text\": 0.6177478317045958}, {\"Text length\": 281, \"comment_text\": 0.6190138480580919}, {\"Text length\": 282, \"comment_text\": 0.6202577021997107}, {\"Text length\": 283, \"comment_text\": 0.6215242726085037}, {\"Text length\": 284, \"comment_text\": 0.6228351674410516}, {\"Text length\": 285, \"comment_text\": 0.6240734810297011}, {\"Text length\": 286, \"comment_text\": 0.6253195513925075}, {\"Text length\": 287, \"comment_text\": 0.6266066218472872}, {\"Text length\": 288, \"comment_text\": 0.6278305299982163}, {\"Text length\": 289, \"comment_text\": 0.6290910057987431}, {\"Text length\": 290, \"comment_text\": 0.6302905355166071}, {\"Text length\": 291, \"comment_text\": 0.6315022544510036}, {\"Text length\": 292, \"comment_text\": 0.6326962436158983}, {\"Text length\": 293, \"comment_text\": 0.6339051922738101}, {\"Text length\": 294, \"comment_text\": 0.6351091544340496}, {\"Text length\": 295, \"comment_text\": 0.6362848597741454}, {\"Text length\": 296, \"comment_text\": 0.6374500380635993}, {\"Text length\": 297, \"comment_text\": 0.638643473173197}, {\"Text length\": 298, \"comment_text\": 0.6398180704026989}, {\"Text length\": 299, \"comment_text\": 0.6409915595216069}, {\"Text length\": 300, \"comment_text\": 0.6421451026498253}, {\"Text length\": 301, \"comment_text\": 0.6433235782664057}, {\"Text length\": 302, \"comment_text\": 0.6444909727770475}, {\"Text length\": 303, \"comment_text\": 0.6455819076567122}, {\"Text length\": 304, \"comment_text\": 0.6467698022133406}, {\"Text length\": 305, \"comment_text\": 0.6479250075074497}, {\"Text length\": 306, \"comment_text\": 0.6490868614651222}, {\"Text length\": 307, \"comment_text\": 0.650186107174241}, {\"Text length\": 308, \"comment_text\": 0.651291447491626}, {\"Text length\": 309, \"comment_text\": 0.6524123013573253}, {\"Text length\": 310, \"comment_text\": 0.6535065605687717}, {\"Text length\": 311, \"comment_text\": 0.6546529009781299}, {\"Text length\": 312, \"comment_text\": 0.6557676602355628}, {\"Text length\": 313, \"comment_text\": 0.6568574870046336}, {\"Text length\": 314, \"comment_text\": 0.6579467597184075}, {\"Text length\": 315, \"comment_text\": 0.6590853433536087}, {\"Text length\": 316, \"comment_text\": 0.660221710767622}, {\"Text length\": 317, \"comment_text\": 0.6613098753708021}, {\"Text length\": 318, \"comment_text\": 0.6624207562411565}, {\"Text length\": 319, \"comment_text\": 0.6635144613973059}, {\"Text length\": 320, \"comment_text\": 0.6646076124981584}, {\"Text length\": 321, \"comment_text\": 0.6656669662258978}, {\"Text length\": 322, \"comment_text\": 0.6667152388476986}, {\"Text length\": 323, \"comment_text\": 0.6677701601330626}, {\"Text length\": 324, \"comment_text\": 0.6688345003584745}, {\"Text length\": 325, \"comment_text\": 0.6698800027037906}, {\"Text length\": 326, \"comment_text\": 0.6709604105328135}, {\"Text length\": 327, \"comment_text\": 0.6720319534770856}, {\"Text length\": 328, \"comment_text\": 0.6730835504306679}, {\"Text length\": 329, \"comment_text\": 0.6741030121770281}, {\"Text length\": 330, \"comment_text\": 0.6751080684856678}, {\"Text length\": 331, \"comment_text\": 0.6761286383426218}, {\"Text length\": 332, \"comment_text\": 0.6771037756652272}, {\"Text length\": 333, \"comment_text\": 0.6781293320198536}, {\"Text length\": 334, \"comment_text\": 0.679180374918139}, {\"Text length\": 335, \"comment_text\": 0.680192633945639}, {\"Text length\": 336, \"comment_text\": 0.681221514632047}, {\"Text length\": 337, \"comment_text\": 0.6823207603411658}, {\"Text length\": 338, \"comment_text\": 0.683339668032229}, {\"Text length\": 339, \"comment_text\": 0.6843812919904665}, {\"Text length\": 340, \"comment_text\": 0.6853536590365872}, {\"Text length\": 341, \"comment_text\": 0.6863886343312615}, {\"Text length\": 342, \"comment_text\": 0.6873909203634166}, {\"Text length\": 343, \"comment_text\": 0.688408719943886}, {\"Text length\": 344, \"comment_text\": 0.6894165465290105}, {\"Text length\": 345, \"comment_text\": 0.6904454272154186}, {\"Text length\": 346, \"comment_text\": 0.6914399564734166}, {\"Text length\": 347, \"comment_text\": 0.6924383641184932}, {\"Text length\": 348, \"comment_text\": 0.6934035284457538}, {\"Text length\": 349, \"comment_text\": 0.6944185577497385}, {\"Text length\": 350, \"comment_text\": 0.6953687625839817}, {\"Text length\": 351, \"comment_text\": 0.6963832378326694}, {\"Text length\": 352, \"comment_text\": 0.6973400913304759}, {\"Text length\": 353, \"comment_text\": 0.6983434854732249}, {\"Text length\": 354, \"comment_text\": 0.6992820551462325}, {\"Text length\": 355, \"comment_text\": 0.7002344762016636}, {\"Text length\": 356, \"comment_text\": 0.7011891134782823}, {\"Text length\": 357, \"comment_text\": 0.7021132777135696}, {\"Text length\": 358, \"comment_text\": 0.7030335635617784}, {\"Text length\": 359, \"comment_text\": 0.7039189439262803}, {\"Text length\": 360, \"comment_text\": 0.7048170675626116}, {\"Text length\": 361, \"comment_text\": 0.7057678264521519}, {\"Text length\": 362, \"comment_text\": 0.7066947609639239}, {\"Text length\": 363, \"comment_text\": 0.7075989792085216}, {\"Text length\": 364, \"comment_text\": 0.7085120623378702}, {\"Text length\": 365, \"comment_text\": 0.7093880237623242}, {\"Text length\": 366, \"comment_text\": 0.7102839311774679}, {\"Text length\": 367, \"comment_text\": 0.7111471493300924}, {\"Text length\": 368, \"comment_text\": 0.7120613405700349}, {\"Text length\": 369, \"comment_text\": 0.7129378560497859}, {\"Text length\": 370, \"comment_text\": 0.7137938714835502}, {\"Text length\": 371, \"comment_text\": 0.7146853464563184}, {\"Text length\": 372, \"comment_text\": 0.7155834700926498}, {\"Text length\": 373, \"comment_text\": 0.7163940529920655}, {\"Text length\": 374, \"comment_text\": 0.7172528387023145}, {\"Text length\": 375, \"comment_text\": 0.7180811513712321}, {\"Text length\": 376, \"comment_text\": 0.7189792750075634}, {\"Text length\": 377, \"comment_text\": 0.7198181147271228}, {\"Text length\": 378, \"comment_text\": 0.7207062653681093}, {\"Text length\": 379, \"comment_text\": 0.72156837541014}, {\"Text length\": 380, \"comment_text\": 0.7224360260051399}, {\"Text length\": 381, \"comment_text\": 0.7232964738812797}, {\"Text length\": 382, \"comment_text\": 0.7241979218493927}, {\"Text length\": 383, \"comment_text\": 0.7250506129513754}, {\"Text length\": 384, \"comment_text\": 0.7259177094910785}, {\"Text length\": 385, \"comment_text\": 0.7267836979201877}, {\"Text length\": 386, \"comment_text\": 0.7276241998056379}, {\"Text length\": 387, \"comment_text\": 0.728448634087477}, {\"Text length\": 388, \"comment_text\": 0.7292885819176302}, {\"Text length\": 389, \"comment_text\": 0.7301052594253122}, {\"Text length\": 390, \"comment_text\": 0.7309413288683869}, {\"Text length\": 391, \"comment_text\": 0.7317735199243831}, {\"Text length\": 392, \"comment_text\": 0.7325780082155325}, {\"Text length\": 393, \"comment_text\": 0.7334284830963276}, {\"Text length\": 394, \"comment_text\": 0.7342396200510402}, {\"Text length\": 395, \"comment_text\": 0.7350773516600057}, {\"Text length\": 396, \"comment_text\": 0.7359195157113466}, {\"Text length\": 397, \"comment_text\": 0.7367882744169405}, {\"Text length\": 398, \"comment_text\": 0.7375988573163562}, {\"Text length\": 399, \"comment_text\": 0.7384144267134444}, {\"Text length\": 400, \"comment_text\": 0.7391878879079654}, {\"Text length\": 401, \"comment_text\": 0.7399957005308965}, {\"Text length\": 402, \"comment_text\": 0.7408012969326397}, {\"Text length\": 403, \"comment_text\": 0.7416129879426493}, {\"Text length\": 404, \"comment_text\": 0.7423958680772182}, {\"Text length\": 405, \"comment_text\": 0.7431942617601014}, {\"Text length\": 406, \"comment_text\": 0.7439743716181857}, {\"Text length\": 407, \"comment_text\": 0.7447090489419214}, {\"Text length\": 408, \"comment_text\": 0.7454880506894118}, {\"Text length\": 409, \"comment_text\": 0.7462670524369023}, {\"Text length\": 410, \"comment_text\": 0.7470499325714712}, {\"Text length\": 411, \"comment_text\": 0.7478040018305996}, {\"Text length\": 412, \"comment_text\": 0.7485375710437413}, {\"Text length\": 413, \"comment_text\": 0.7492827754181187}, {\"Text length\": 414, \"comment_text\": 0.7500617771656092}, {\"Text length\": 415, \"comment_text\": 0.7507720760562797}, {\"Text length\": 416, \"comment_text\": 0.7515289155918927}, {\"Text length\": 417, \"comment_text\": 0.7522624848050344}, {\"Text length\": 418, \"comment_text\": 0.7530187702853505}, {\"Text length\": 419, \"comment_text\": 0.7537794882080421}, {\"Text length\": 420, \"comment_text\": 0.7544958817069789}, {\"Text length\": 421, \"comment_text\": 0.755243302302544}, {\"Text length\": 422, \"comment_text\": 0.7559475065849482}, {\"Text length\": 423, \"comment_text\": 0.7566855082404654}, {\"Text length\": 424, \"comment_text\": 0.7574063341817777}, {\"Text length\": 425, \"comment_text\": 0.7581033357453217}, {\"Text length\": 426, \"comment_text\": 0.7588103103042106}, {\"Text length\": 427, \"comment_text\": 0.7595250416372566}, {\"Text length\": 428, \"comment_text\": 0.7602342324173332}, {\"Text length\": 429, \"comment_text\": 0.7609600448563179}, {\"Text length\": 430, \"comment_text\": 0.7616636950834251}, {\"Text length\": 431, \"comment_text\": 0.7623728858635017}, {\"Text length\": 432, \"comment_text\": 0.76309260369422}, {\"Text length\": 433, \"comment_text\": 0.7637613484376204}, {\"Text length\": 434, \"comment_text\": 0.7644622283882431}, {\"Text length\": 435, \"comment_text\": 0.7651547975094116}, {\"Text length\": 436, \"comment_text\": 0.7658257584739997}, {\"Text length\": 437, \"comment_text\": 0.7665127870421989}, {\"Text length\": 438, \"comment_text\": 0.7671948291127257}, {\"Text length\": 439, \"comment_text\": 0.7678796414597372}, {\"Text length\": 440, \"comment_text\": 0.7685589132537793}, {\"Text length\": 441, \"comment_text\": 0.7692365228819307}, {\"Text length\": 442, \"comment_text\": 0.7699119162888942}, {\"Text length\": 443, \"comment_text\": 0.7705978367464996}, {\"Text length\": 444, \"comment_text\": 0.771266027434603}, {\"Text length\": 445, \"comment_text\": 0.771946961394536}, {\"Text length\": 446, \"comment_text\": 0.772610165584967}, {\"Text length\": 447, \"comment_text\": 0.7733016265955417}, {\"Text length\": 448, \"comment_text\": 0.7739775740578022}, {\"Text length\": 449, \"comment_text\": 0.7746745756213462}, {\"Text length\": 450, \"comment_text\": 0.7753599420236547}, {\"Text length\": 451, \"comment_text\": 0.7760552814213079}, {\"Text length\": 452, \"comment_text\": 0.7767395397130225}, {\"Text length\": 453, \"comment_text\": 0.7774304466683002}, {\"Text length\": 454, \"comment_text\": 0.7781047319646699}, {\"Text length\": 455, \"comment_text\": 0.7787374631137695}, {\"Text length\": 456, \"comment_text\": 0.7793679780416813}, {\"Text length\": 457, \"comment_text\": 0.7800140065179074}, {\"Text length\": 458, \"comment_text\": 0.7806683458235875}, {\"Text length\": 459, \"comment_text\": 0.7812938742538269}, {\"Text length\": 460, \"comment_text\": 0.7819216189052541}, {\"Text length\": 461, \"comment_text\": 0.7825615527732139}, {\"Text length\": 462, \"comment_text\": 0.7832280812954265}, {\"Text length\": 463, \"comment_text\": 0.7838414205091334}, {\"Text length\": 464, \"comment_text\": 0.784465286773482}, {\"Text length\": 465, \"comment_text\": 0.785069207047141}, {\"Text length\": 466, \"comment_text\": 0.7856914111455988}, {\"Text length\": 467, \"comment_text\": 0.7862997638616332}, {\"Text length\": 468, \"comment_text\": 0.7868959273611351}, {\"Text length\": 469, \"comment_text\": 0.7875186855148899}, {\"Text length\": 470, \"comment_text\": 0.7881109706273133}, {\"Text length\": 471, \"comment_text\": 0.7887243098410202}, {\"Text length\": 472, \"comment_text\": 0.7893165949534435}, {\"Text length\": 473, \"comment_text\": 0.7899116503423516}, {\"Text length\": 474, \"comment_text\": 0.7905017192335871}, {\"Text length\": 475, \"comment_text\": 0.7911106260049185}, {\"Text length\": 476, \"comment_text\": 0.7917317219927825}, {\"Text length\": 477, \"comment_text\": 0.7923267773816906}, {\"Text length\": 478, \"comment_text\": 0.7929390084848036}, {\"Text length\": 479, \"comment_text\": 0.7935567801408859}, {\"Text length\": 480, \"comment_text\": 0.7941634706910295}, {\"Text length\": 481, \"comment_text\": 0.7947513233610775}, {\"Text length\": 482, \"comment_text\": 0.795325324648702}, {\"Text length\": 483, \"comment_text\": 0.7959104070422652}, {\"Text length\": 484, \"comment_text\": 0.7965110029841426}, {\"Text length\": 485, \"comment_text\": 0.7970844502164701}, {\"Text length\": 486, \"comment_text\": 0.7976844921030506}, {\"Text length\": 487, \"comment_text\": 0.7982357771235008}, {\"Text length\": 488, \"comment_text\": 0.7988275081806272}, {\"Text length\": 489, \"comment_text\": 0.7994192392377536}, {\"Text length\": 490, \"comment_text\": 0.7999738485899854}, {\"Text length\": 491, \"comment_text\": 0.8005755526424567}, {\"Text length\": 492, \"comment_text\": 0.8011495539300812}, {\"Text length\": 493, \"comment_text\": 0.8017362984895353}, {\"Text length\": 494, \"comment_text\": 0.8022958943394395}, {\"Text length\": 495, \"comment_text\": 0.8028499496363743}, {\"Text length\": 496, \"comment_text\": 0.8034394644723131}, {\"Text length\": 497, \"comment_text\": 0.8040112495387498}, {\"Text length\": 498, \"comment_text\": 0.8045647507803877}, {\"Text length\": 499, \"comment_text\": 0.8051276709620735}, {\"Text length\": 500, \"comment_text\": 0.8056828343696022}, {\"Text length\": 501, \"comment_text\": 0.8062352275006462}, {\"Text length\": 502, \"comment_text\": 0.8068086747329738}, {\"Text length\": 503, \"comment_text\": 0.8073621759746117}, {\"Text length\": 504, \"comment_text\": 0.8079012717785293}, {\"Text length\": 505, \"comment_text\": 0.8084608676284335}, {\"Text length\": 506, \"comment_text\": 0.8090121526488837}, {\"Text length\": 507, \"comment_text\": 0.8095412754574565}, {\"Text length\": 508, \"comment_text\": 0.8100920064226097}, {\"Text length\": 509, \"comment_text\": 0.8106322103371212}, {\"Text length\": 510, \"comment_text\": 0.8111496979844584}, {\"Text length\": 511, \"comment_text\": 0.8116921181201576}, {\"Text length\": 512, \"comment_text\": 0.8122278895922936}, {\"Text length\": 513, \"comment_text\": 0.8127475934608185}, {\"Text length\": 514, \"comment_text\": 0.8132711757164219}, {\"Text length\": 515, \"comment_text\": 0.8137725957601479}, {\"Text length\": 516, \"comment_text\": 0.8143033807346115}, {\"Text length\": 517, \"comment_text\": 0.8148219764925425}, {\"Text length\": 518, \"comment_text\": 0.8153854507295253}, {\"Text length\": 519, \"comment_text\": 0.8158680328931556}, {\"Text length\": 520, \"comment_text\": 0.8163866286510866}, {\"Text length\": 521, \"comment_text\": 0.8169146433490655}, {\"Text length\": 522, \"comment_text\": 0.8174448742682322}, {\"Text length\": 523, \"comment_text\": 0.8179440780907705}, {\"Text length\": 524, \"comment_text\": 0.8184349710838548}, {\"Text length\": 525, \"comment_text\": 0.8189280802981268}, {\"Text length\": 526, \"comment_text\": 0.8194444598348701}, {\"Text length\": 527, \"comment_text\": 0.8199436636574085}, {\"Text length\": 528, \"comment_text\": 0.820457272917667}, {\"Text length\": 529, \"comment_text\": 0.8209647875696593}, {\"Text length\": 530, \"comment_text\": 0.8214318561849754}, {\"Text length\": 531, \"comment_text\": 0.821920532956872}, {\"Text length\": 532, \"comment_text\": 0.8224086556734715}, {\"Text length\": 533, \"comment_text\": 0.822906751385416}, {\"Text length\": 534, \"comment_text\": 0.8233721578348413}, {\"Text length\": 535, \"comment_text\": 0.8238663751597072}, {\"Text length\": 536, \"comment_text\": 0.8243589303186822}, {\"Text length\": 537, \"comment_text\": 0.8248354178740462}, {\"Text length\": 538, \"comment_text\": 0.8253357298071784}, {\"Text length\": 539, \"comment_text\": 0.8258310552426381}, {\"Text length\": 540, \"comment_text\": 0.826316407682753}, {\"Text length\": 541, \"comment_text\": 0.8268095168970251}, {\"Text length\": 542, \"comment_text\": 0.8273142612725327}, {\"Text length\": 543, \"comment_text\": 0.8278090326526956}, {\"Text length\": 544, \"comment_text\": 0.8282977094245921}, {\"Text length\": 545, \"comment_text\": 0.828758129376345}, {\"Text length\": 546, \"comment_text\": 0.8292312925999273}, {\"Text length\": 547, \"comment_text\": 0.8297155369294484}, {\"Text length\": 548, \"comment_text\": 0.8301598892775902}, {\"Text length\": 549, \"comment_text\": 0.8306457957730021}, {\"Text length\": 550, \"comment_text\": 0.8310984589505979}, {\"Text length\": 551, \"comment_text\": 0.8315644194553201}, {\"Text length\": 552, \"comment_text\": 0.8320231772411821}, {\"Text length\": 553, \"comment_text\": 0.8324869215247166}, {\"Text length\": 554, \"comment_text\": 0.8329523279741419}, {\"Text length\": 555, \"comment_text\": 0.8333867073269388}, {\"Text length\": 556, \"comment_text\": 0.8338432488916131}, {\"Text length\": 557, \"comment_text\": 0.8343103175069292}, {\"Text length\": 558, \"comment_text\": 0.8347978861682318}, {\"Text length\": 559, \"comment_text\": 0.8352488871799367}, {\"Text length\": 560, \"comment_text\": 0.8356777259797643}, {\"Text length\": 561, \"comment_text\": 0.8361242945490938}, {\"Text length\": 562, \"comment_text\": 0.8365664306760477}, {\"Text length\": 563, \"comment_text\": 0.837015215466565}, {\"Text length\": 564, \"comment_text\": 0.8374634462017853}, {\"Text length\": 565, \"comment_text\": 0.8378828660615649}, {\"Text length\": 566, \"comment_text\": 0.8383344211285668}, {\"Text length\": 567, \"comment_text\": 0.8387904086379442}, {\"Text length\": 568, \"comment_text\": 0.8392159231059901}, {\"Text length\": 569, \"comment_text\": 0.8396541808458656}, {\"Text length\": 570, \"comment_text\": 0.840059195267925}, {\"Text length\": 571, \"comment_text\": 0.8404869259571588}, {\"Text length\": 572, \"comment_text\": 0.840936264802973}, {\"Text length\": 573, \"comment_text\": 0.8414077658606646}, {\"Text length\": 574, \"comment_text\": 0.8418199830015841}, {\"Text length\": 575, \"comment_text\": 0.8422582407414596}, {\"Text length\": 576, \"comment_text\": 0.8426854173753964}, {\"Text length\": 577, \"comment_text\": 0.8430926480186435}, {\"Text length\": 578, \"comment_text\": 0.8435430949750515}, {\"Text length\": 579, \"comment_text\": 0.8439420147888447}, {\"Text length\": 580, \"comment_text\": 0.844364204925109}, {\"Text length\": 581, \"comment_text\": 0.8447758680107316}, {\"Text length\": 582, \"comment_text\": 0.84520636897645}, {\"Text length\": 583, \"comment_text\": 0.8456451807716223}, {\"Text length\": 584, \"comment_text\": 0.8460490870830878}, {\"Text length\": 585, \"comment_text\": 0.8464651826110858}, {\"Text length\": 586, \"comment_text\": 0.8468690889225513}, {\"Text length\": 587, \"comment_text\": 0.8472840763399555}, {\"Text length\": 588, \"comment_text\": 0.8476890907620149}, {\"Text length\": 589, \"comment_text\": 0.8480907808522927}, {\"Text length\": 590, \"comment_text\": 0.848496903384946}, {\"Text length\": 591, \"comment_text\": 0.8488941610328483}, {\"Text length\": 592, \"comment_text\": 0.8492775672983273}, {\"Text length\": 593, \"comment_text\": 0.8496847979415744}, {\"Text length\": 594, \"comment_text\": 0.8500787312576951}, {\"Text length\": 595, \"comment_text\": 0.8504715564632219}, {\"Text length\": 596, \"comment_text\": 0.850861611392264}, {\"Text length\": 597, \"comment_text\": 0.851245017657743}, {\"Text length\": 598, \"comment_text\": 0.8516733024022736}, {\"Text length\": 599, \"comment_text\": 0.8520528302806739}, {\"Text length\": 600, \"comment_text\": 0.8524644933662965}, {\"Text length\": 601, \"comment_text\": 0.852859534793011}, {\"Text length\": 602, \"comment_text\": 0.8532662113809613}, {\"Text length\": 603, \"comment_text\": 0.8536667933606451}, {\"Text length\": 604, \"comment_text\": 0.8540712537274076}, {\"Text length\": 605, \"comment_text\": 0.8544690654306067}, {\"Text length\": 606, \"comment_text\": 0.8548452689772255}, {\"Text length\": 607, \"comment_text\": 0.8552358779615645}, {\"Text length\": 608, \"comment_text\": 0.8556004463469477}, {\"Text length\": 609, \"comment_text\": 0.8559954877736622}, {\"Text length\": 610, \"comment_text\": 0.8563916373109706}, {\"Text length\": 611, \"comment_text\": 0.8567678408575893}, {\"Text length\": 612, \"comment_text\": 0.8571518011783651}, {\"Text length\": 613, \"comment_text\": 0.8575257885037961}, {\"Text length\": 614, \"comment_text\": 0.8578598838478478}, {\"Text length\": 615, \"comment_text\": 0.8582499387768899}, {\"Text length\": 616, \"comment_text\": 0.8586350072082597}, {\"Text length\": 617, \"comment_text\": 0.8590123188654724}, {\"Text length\": 618, \"comment_text\": 0.8593968332415451}, {\"Text length\": 619, \"comment_text\": 0.8597580772951466}, {\"Text length\": 620, \"comment_text\": 0.8601414835606256}, {\"Text length\": 621, \"comment_text\": 0.8605415114850126}, {\"Text length\": 622, \"comment_text\": 0.8609337826352425}, {\"Text length\": 623, \"comment_text\": 0.8612833915276084}, {\"Text length\": 624, \"comment_text\": 0.8616474058576945}, {\"Text length\": 625, \"comment_text\": 0.8620407851185182}, {\"Text length\": 626, \"comment_text\": 0.8624114481121676}, {\"Text length\": 627, \"comment_text\": 0.8627926381564588}, {\"Text length\": 628, \"comment_text\": 0.8631627470948112}, {\"Text length\": 629, \"comment_text\": 0.8635029370471292}, {\"Text length\": 630, \"comment_text\": 0.8638603027136522}, {\"Text length\": 631, \"comment_text\": 0.8642226548778477}, {\"Text length\": 632, \"comment_text\": 0.864602736811545}, {\"Text length\": 633, \"comment_text\": 0.8649684133075219}, {\"Text length\": 634, \"comment_text\": 0.8653490492965162}, {\"Text length\": 635, \"comment_text\": 0.865717496068978}, {\"Text length\": 636, \"comment_text\": 0.8660986861132691}, {\"Text length\": 637, \"comment_text\": 0.8664588220562768}, {\"Text length\": 638, \"comment_text\": 0.8668028903956734}, {\"Text length\": 639, \"comment_text\": 0.8671536073986331}, {\"Text length\": 640, \"comment_text\": 0.8675209460605009}, {\"Text length\": 641, \"comment_text\": 0.8678927171647441}, {\"Text length\": 642, \"comment_text\": 0.8682467584994855}, {\"Text length\": 643, \"comment_text\": 0.8685941511706636}, {\"Text length\": 644, \"comment_text\": 0.8689565033348591}, {\"Text length\": 645, \"comment_text\": 0.8692961392318801}, {\"Text length\": 646, \"comment_text\": 0.8696540589537}, {\"Text length\": 647, \"comment_text\": 0.8699881542977517}, {\"Text length\": 648, \"comment_text\": 0.870351614572541}, {\"Text length\": 649, \"comment_text\": 0.8706696423129816}, {\"Text length\": 650, \"comment_text\": 0.8710258998689108}, {\"Text length\": 651, \"comment_text\": 0.871355562770587}, {\"Text length\": 652, \"comment_text\": 0.8717018473311713}, {\"Text length\": 653, \"comment_text\": 0.8720475778364586}, {\"Text length\": 654, \"comment_text\": 0.8723849975122919}, {\"Text length\": 655, \"comment_text\": 0.8727141063586712}, {\"Text length\": 656, \"comment_text\": 0.8730526341450984}, {\"Text length\": 657, \"comment_text\": 0.8733861754338531}, {\"Text length\": 658, \"comment_text\": 0.8737285816073589}, {\"Text length\": 659, \"comment_text\": 0.8740582445090351}, {\"Text length\": 660, \"comment_text\": 0.8743984344613531}, {\"Text length\": 661, \"comment_text\": 0.874739732524265}, {\"Text length\": 662, \"comment_text\": 0.8750644089282688}, {\"Text length\": 663, \"comment_text\": 0.8754079232123684}, {\"Text length\": 664, \"comment_text\": 0.8757325996163722}, {\"Text length\": 665, \"comment_text\": 0.8760450868038434}, {\"Text length\": 666, \"comment_text\": 0.8763841686455676}, {\"Text length\": 667, \"comment_text\": 0.8767204802108071}, {\"Text length\": 668, \"comment_text\": 0.8770407241724354}, {\"Text length\": 669, \"comment_text\": 0.8773493329728281}, {\"Text length\": 670, \"comment_text\": 0.8776756715427227}, {\"Text length\": 671, \"comment_text\": 0.8779925911725694}, {\"Text length\": 672, \"comment_text\": 0.8783167135212764}, {\"Text length\": 673, \"comment_text\": 0.8786247682663721}, {\"Text length\": 674, \"comment_text\": 0.8789367013985465}, {\"Text length\": 675, \"comment_text\": 0.8792652561896289}, {\"Text length\": 676, \"comment_text\": 0.8795694325476462}, {\"Text length\": 677, \"comment_text\": 0.8798797035139297}, {\"Text length\": 678, \"comment_text\": 0.8801744609318991}, {\"Text length\": 679, \"comment_text\": 0.8805102184418416}, {\"Text length\": 680, \"comment_text\": 0.8808193812975312}, {\"Text length\": 681, \"comment_text\": 0.8811329765955964}, {\"Text length\": 682, \"comment_text\": 0.881442139451286}, {\"Text length\": 683, \"comment_text\": 0.8817706942423684}, {\"Text length\": 684, \"comment_text\": 0.8820593570520715}, {\"Text length\": 685, \"comment_text\": 0.8823629793547918}, {\"Text length\": 686, \"comment_text\": 0.8826560746068703}, {\"Text length\": 687, \"comment_text\": 0.882973548292014}, {\"Text length\": 688, \"comment_text\": 0.883296562530127}, {\"Text length\": 689, \"comment_text\": 0.8836035091646289}, {\"Text length\": 690, \"comment_text\": 0.8839237531262573}, {\"Text length\": 691, \"comment_text\": 0.8842390105902131}, {\"Text length\": 692, \"comment_text\": 0.8845448491141212}, {\"Text length\": 693, \"comment_text\": 0.8848556741357017}, {\"Text length\": 694, \"comment_text\": 0.8851487693877802}, {\"Text length\": 695, \"comment_text\": 0.8854402024739679}, {\"Text length\": 696, \"comment_text\": 0.8857200003989201}, {\"Text length\": 697, \"comment_text\": 0.8860202983698587}, {\"Text length\": 698, \"comment_text\": 0.886307853068968}, {\"Text length\": 699, \"comment_text\": 0.8865699212244181}, {\"Text length\": 700, \"comment_text\": 0.8868757597483262}, {\"Text length\": 701, \"comment_text\": 0.8871610982262477}, {\"Text length\": 702, \"comment_text\": 0.8874447745382783}, {\"Text length\": 703, \"comment_text\": 0.8877561536151557}, {\"Text length\": 704, \"comment_text\": 0.8880420461483741}, {\"Text length\": 705, \"comment_text\": 0.8883373576216403}, {\"Text length\": 706, \"comment_text\": 0.8886487366985177}, {\"Text length\": 707, \"comment_text\": 0.8889501427800502}, {\"Text length\": 708, \"comment_text\": 0.8892465623639104}, {\"Text length\": 709, \"comment_text\": 0.8895346711183165}, {\"Text length\": 710, \"comment_text\": 0.8898305366468797}, {\"Text length\": 711, \"comment_text\": 0.8901380373366785}, {\"Text length\": 712, \"comment_text\": 0.890397335215644}, {\"Text length\": 713, \"comment_text\": 0.8906688223111421}, {\"Text length\": 714, \"comment_text\": 0.8909724446138624}, {\"Text length\": 715, \"comment_text\": 0.8912433776540635}, {\"Text length\": 716, \"comment_text\": 0.8915220674684217}, {\"Text length\": 717, \"comment_text\": 0.891783581568575}, {\"Text length\": 718, \"comment_text\": 0.892089420092483}, {\"Text length\": 719, \"comment_text\": 0.8923797450680769}, {\"Text length\": 720, \"comment_text\": 0.8926756105966401}, {\"Text length\": 721, \"comment_text\": 0.8929554085215923}, {\"Text length\": 722, \"comment_text\": 0.8932313280594658}, {\"Text length\": 723, \"comment_text\": 0.8934989367678854}, {\"Text length\": 724, \"comment_text\": 0.8937720860292743}, {\"Text length\": 725, \"comment_text\": 0.8940507758436325}, {\"Text length\": 726, \"comment_text\": 0.8943388845980386}, {\"Text length\": 727, \"comment_text\": 0.8946192365782876}, {\"Text length\": 728, \"comment_text\": 0.8949006966691305}, {\"Text length\": 729, \"comment_text\": 0.8951876973129428}, {\"Text length\": 730, \"comment_text\": 0.895466941182598}, {\"Text length\": 731, \"comment_text\": 0.8957339958357206}, {\"Text length\": 732, \"comment_text\": 0.896061442516209}, {\"Text length\": 733, \"comment_text\": 0.8963290512246286}, {\"Text length\": 734, \"comment_text\": 0.8966265789190826}, {\"Text length\": 735, \"comment_text\": 0.8969180120052703}, {\"Text length\": 736, \"comment_text\": 0.8972022423725979}, {\"Text length\": 737, \"comment_text\": 0.8974859186846286}, {\"Text length\": 738, \"comment_text\": 0.897762946333096}, {\"Text length\": 739, \"comment_text\": 0.8980366496497818}, {\"Text length\": 740, \"comment_text\": 0.8983064745793891}, {\"Text length\": 741, \"comment_text\": 0.8985607859606821}, {\"Text length\": 742, \"comment_text\": 0.8988223000608354}, {\"Text length\": 743, \"comment_text\": 0.8991015439304906}, {\"Text length\": 744, \"comment_text\": 0.8993746931918795}, {\"Text length\": 745, \"comment_text\": 0.899633437015548}, {\"Text length\": 746, \"comment_text\": 0.8999010457239676}, {\"Text length\": 747, \"comment_text\": 0.9001597895476361}, {\"Text length\": 748, \"comment_text\": 0.9004445739702606}, {\"Text length\": 749, \"comment_text\": 0.9007038718492261}, {\"Text length\": 750, \"comment_text\": 0.9009842238294751}, {\"Text length\": 751, \"comment_text\": 0.9012529406484885}, {\"Text length\": 752, \"comment_text\": 0.9015283061310652}, {\"Text length\": 753, \"comment_text\": 0.9018053337795325}, {\"Text length\": 754, \"comment_text\": 0.9020662938243889}, {\"Text length\": 755, \"comment_text\": 0.9023111862656341}, {\"Text length\": 756, \"comment_text\": 0.9025538624856916}, {\"Text length\": 757, \"comment_text\": 0.9028026333140153}, {\"Text length\": 758, \"comment_text\": 0.9030757825754042}, {\"Text length\": 759, \"comment_text\": 0.9033262155696188}, {\"Text length\": 760, \"comment_text\": 0.9035871756144751}, {\"Text length\": 761, \"comment_text\": 0.9038448113275498}, {\"Text length\": 762, \"comment_text\": 0.9041174065336417}, {\"Text length\": 763, \"comment_text\": 0.904377812523201}, {\"Text length\": 764, \"comment_text\": 0.9046271374068218}, {\"Text length\": 765, \"comment_text\": 0.904879786622224}, {\"Text length\": 766, \"comment_text\": 0.9051280033952508}, {\"Text length\": 767, \"comment_text\": 0.9053889634401071}, {\"Text length\": 768, \"comment_text\": 0.905653247816745}, {\"Text length\": 769, \"comment_text\": 0.9058964780920995}, {\"Text length\": 770, \"comment_text\": 0.9061430326992355}, {\"Text length\": 771, \"comment_text\": 0.9064117495182489}, {\"Text length\": 772, \"comment_text\": 0.9066477770747431}, {\"Text length\": 773, \"comment_text\": 0.9069220344467258}, {\"Text length\": 774, \"comment_text\": 0.9071780079939097}, {\"Text length\": 775, \"comment_text\": 0.9074345355963905}, {\"Text length\": 776, \"comment_text\": 0.9076716712634787}, {\"Text length\": 777, \"comment_text\": 0.9079315231977412}, {\"Text length\": 778, \"comment_text\": 0.9081930372978945}, {\"Text length\": 779, \"comment_text\": 0.9084484567897815}, {\"Text length\": 780, \"comment_text\": 0.9087049843922623}, {\"Text length\": 781, \"comment_text\": 0.9089515389993983}, {\"Text length\": 782, \"comment_text\": 0.9091986476618312}, {\"Text length\": 783, \"comment_text\": 0.9094679185361416}, {\"Text length\": 784, \"comment_text\": 0.9097117028667929}, {\"Text length\": 785, \"comment_text\": 0.9099532709762566}, {\"Text length\": 786, \"comment_text\": 0.9102197715740822}, {\"Text length\": 787, \"comment_text\": 0.9104740829553752}, {\"Text length\": 788, \"comment_text\": 0.9107178672860266}, {\"Text length\": 789, \"comment_text\": 0.9109500164554423}, {\"Text length\": 790, \"comment_text\": 0.9111782872377794}, {\"Text length\": 791, \"comment_text\": 0.9114137607389767}, {\"Text length\": 792, \"comment_text\": 0.9116564369590342}, {\"Text length\": 793, \"comment_text\": 0.9119013294002795}, {\"Text length\": 794, \"comment_text\": 0.9121273839614289}, {\"Text length\": 795, \"comment_text\": 0.9123861277850974}, {\"Text length\": 796, \"comment_text\": 0.9126271418392641}, {\"Text length\": 797, \"comment_text\": 0.9128548585663043}, {\"Text length\": 798, \"comment_text\": 0.9130892239569077}, {\"Text length\": 799, \"comment_text\": 0.9133374407299345}, {\"Text length\": 800, \"comment_text\": 0.9135729142311318}, {\"Text length\": 801, \"comment_text\": 0.9138155904511893}, {\"Text length\": 802, \"comment_text\": 0.914056604505356}, {\"Text length\": 803, \"comment_text\": 0.9143048212783828}, {\"Text length\": 804, \"comment_text\": 0.914557470493785}, {\"Text length\": 805, \"comment_text\": 0.9147763223360743}, {\"Text length\": 806, \"comment_text\": 0.9150034850078176}, {\"Text length\": 807, \"comment_text\": 0.9152162422418406}, {\"Text length\": 808, \"comment_text\": 0.9154528238536318}, {\"Text length\": 809, \"comment_text\": 0.9156938379077985}, {\"Text length\": 810, \"comment_text\": 0.9159315276301835}, {\"Text length\": 811, \"comment_text\": 0.9161620146337084}, {\"Text length\": 812, \"comment_text\": 0.9163919475819363}, {\"Text length\": 813, \"comment_text\": 0.9166257589172428}, {\"Text length\": 814, \"comment_text\": 0.9168595702525494}, {\"Text length\": 815, \"comment_text\": 0.9171122194679516}, {\"Text length\": 816, \"comment_text\": 0.9173515713562275}, {\"Text length\": 817, \"comment_text\": 0.9175948016315819}, {\"Text length\": 818, \"comment_text\": 0.917823072413919}, {\"Text length\": 819, \"comment_text\": 0.9180574378045224}, {\"Text length\": 820, \"comment_text\": 0.9183023302457677}, {\"Text length\": 821, \"comment_text\": 0.9185156415350876}, {\"Text length\": 822, \"comment_text\": 0.9187710610269746}, {\"Text length\": 823, \"comment_text\": 0.9190120750811412}, {\"Text length\": 824, \"comment_text\": 0.9192375755869937}, {\"Text length\": 825, \"comment_text\": 0.9194575355398769}, {\"Text length\": 826, \"comment_text\": 0.9196730630503845}, {\"Text length\": 827, \"comment_text\": 0.9199063203303941}, {\"Text length\": 828, \"comment_text\": 0.9201273883938711}, {\"Text length\": 829, \"comment_text\": 0.9203661862268501}, {\"Text length\": 830, \"comment_text\": 0.9205883624009209}, {\"Text length\": 831, \"comment_text\": 0.920817741293852}, {\"Text length\": 832, \"comment_text\": 0.9210548769609401}, {\"Text length\": 833, \"comment_text\": 0.9213069721210455}, {\"Text length\": 834, \"comment_text\": 0.9215125266362083}, {\"Text length\": 835, \"comment_text\": 0.9217474460821087}, {\"Text length\": 836, \"comment_text\": 0.9219707303667735}, {\"Text length\": 837, \"comment_text\": 0.9222139606421279}, {\"Text length\": 838, \"comment_text\": 0.9224289340973386}, {\"Text length\": 839, \"comment_text\": 0.9226394751101739}, {\"Text length\": 840, \"comment_text\": 0.9228710702242926}, {\"Text length\": 841, \"comment_text\": 0.9230877058453941}, {\"Text length\": 842, \"comment_text\": 0.9233060036323865}, {\"Text length\": 843, \"comment_text\": 0.9235309500829421}, {\"Text length\": 844, \"comment_text\": 0.9237486938146374}, {\"Text length\": 845, \"comment_text\": 0.9239802889287562}, {\"Text length\": 846, \"comment_text\": 0.924220194872329}, {\"Text length\": 847, \"comment_text\": 0.9244534521523385}, {\"Text length\": 848, \"comment_text\": 0.9246723039946279}, {\"Text length\": 849, \"comment_text\": 0.9249083315511221}, {\"Text length\": 850, \"comment_text\": 0.9251321698910837}, {\"Text length\": 851, \"comment_text\": 0.9253465892909974}, {\"Text length\": 852, \"comment_text\": 0.9255770762945223}, {\"Text length\": 853, \"comment_text\": 0.925784292975576}, {\"Text length\": 854, \"comment_text\": 0.9260092394261316}, {\"Text length\": 855, \"comment_text\": 0.9262197804389668}, {\"Text length\": 856, \"comment_text\": 0.9264486052766009}, {\"Text length\": 857, \"comment_text\": 0.9266718895612657}, {\"Text length\": 858, \"comment_text\": 0.9268779981317254}, {\"Text length\": 859, \"comment_text\": 0.9270852148127791}, {\"Text length\": 860, \"comment_text\": 0.927306282876256}, {\"Text length\": 861, \"comment_text\": 0.9275157157784975}, {\"Text length\": 862, \"comment_text\": 0.927732905454896}, {\"Text length\": 863, \"comment_text\": 0.9279645005690147}, {\"Text length\": 864, \"comment_text\": 0.9281783659136316}, {\"Text length\": 865, \"comment_text\": 0.9284016501982963}, {\"Text length\": 866, \"comment_text\": 0.9286232723170703}, {\"Text length\": 867, \"comment_text\": 0.9288349214404994}, {\"Text length\": 868, \"comment_text\": 0.9290515570616009}, {\"Text length\": 869, \"comment_text\": 0.9292859224522043}, {\"Text length\": 870, \"comment_text\": 0.929501449962712}, {\"Text length\": 871, \"comment_text\": 0.9297374775192062}, {\"Text length\": 872, \"comment_text\": 0.9299253022648671}, {\"Text length\": 873, \"comment_text\": 0.9301535730472043}, {\"Text length\": 874, \"comment_text\": 0.9303740870553843}, {\"Text length\": 875, \"comment_text\": 0.9305957091741582}, {\"Text length\": 876, \"comment_text\": 0.9308167772376352}, {\"Text length\": 877, \"comment_text\": 0.9310555750706142}, {\"Text length\": 878, \"comment_text\": 0.9312583593092923}, {\"Text length\": 879, \"comment_text\": 0.9315054679717253}, {\"Text length\": 880, \"comment_text\": 0.9317387252517348}, {\"Text length\": 881, \"comment_text\": 0.9319692122552597}, {\"Text length\": 882, \"comment_text\": 0.9321902803187367}, {\"Text length\": 883, \"comment_text\": 0.9324157808245892}, {\"Text length\": 884, \"comment_text\": 0.9326307542797999}, {\"Text length\": 885, \"comment_text\": 0.9328501601773861}, {\"Text length\": 886, \"comment_text\": 0.9330778769044263}, {\"Text length\": 887, \"comment_text\": 0.9333111341844359}, {\"Text length\": 888, \"comment_text\": 0.9335349725243975}, {\"Text length\": 889, \"comment_text\": 0.9337577027537654}, {\"Text length\": 890, \"comment_text\": 0.9339710140430852}, {\"Text length\": 891, \"comment_text\": 0.9341987307701255}, {\"Text length\": 892, \"comment_text\": 0.934415366391227}, {\"Text length\": 893, \"comment_text\": 0.9346403128417826}, {\"Text length\": 894, \"comment_text\": 0.9348658133476351}, {\"Text length\": 895, \"comment_text\": 0.935104611180614}, {\"Text length\": 896, \"comment_text\": 0.9353234630229034}, {\"Text length\": 897, \"comment_text\": 0.9355467473075681}, {\"Text length\": 898, \"comment_text\": 0.9357700315922329}, {\"Text length\": 899, \"comment_text\": 0.935990545600413}, {\"Text length\": 900, \"comment_text\": 0.936236546152252}, {\"Text length\": 901, \"comment_text\": 0.9364709115428554}, {\"Text length\": 902, \"comment_text\": 0.9367146958735068}, {\"Text length\": 903, \"comment_text\": 0.9369457369323286}, {\"Text length\": 904, \"comment_text\": 0.9371596022769455}, {\"Text length\": 905, \"comment_text\": 0.9374050487734876}, {\"Text length\": 906, \"comment_text\": 0.9376311033346371}, {\"Text length\": 907, \"comment_text\": 0.9378460767898478}, {\"Text length\": 908, \"comment_text\": 0.9380815502910451}, {\"Text length\": 909, \"comment_text\": 0.9383264427322904}, {\"Text length\": 910, \"comment_text\": 0.9385746595053172}, {\"Text length\": 911, \"comment_text\": 0.9388195519465624}, {\"Text length\": 912, \"comment_text\": 0.9390716471066678}, {\"Text length\": 913, \"comment_text\": 0.9393115530502406}, {\"Text length\": 914, \"comment_text\": 0.9395509049385165}, {\"Text length\": 915, \"comment_text\": 0.9397974595456524}, {\"Text length\": 916, \"comment_text\": 0.9400373654892252}, {\"Text length\": 917, \"comment_text\": 0.9402883525387368}, {\"Text length\": 918, \"comment_text\": 0.9405382314776544}, {\"Text length\": 919, \"comment_text\": 0.9407753671447425}, {\"Text length\": 920, \"comment_text\": 0.9410335569131142}, {\"Text length\": 921, \"comment_text\": 0.9412446519812464}, {\"Text length\": 922, \"comment_text\": 0.9414961930860548}, {\"Text length\": 923, \"comment_text\": 0.9417588152968019}, {\"Text length\": 924, \"comment_text\": 0.9420059239592349}, {\"Text length\": 925, \"comment_text\": 0.9422668840040912}, {\"Text length\": 926, \"comment_text\": 0.9425139926665241}, {\"Text length\": 927, \"comment_text\": 0.9427577769971754}, {\"Text length\": 928, \"comment_text\": 0.9430115343231716}, {\"Text length\": 929, \"comment_text\": 0.9432769268104034}, {\"Text length\": 930, \"comment_text\": 0.9435478598506045}, {\"Text length\": 931, \"comment_text\": 0.9438115901719455}, {\"Text length\": 932, \"comment_text\": 0.9440575907237846}, {\"Text length\": 933, \"comment_text\": 0.9443368345934398}, {\"Text length\": 934, \"comment_text\": 0.9445994568041869}, {\"Text length\": 935, \"comment_text\": 0.9448648492914187}, {\"Text length\": 936, \"comment_text\": 0.9451468634375585}, {\"Text length\": 937, \"comment_text\": 0.945448823574388}, {\"Text length\": 938, \"comment_text\": 0.9457131079510259}, {\"Text length\": 939, \"comment_text\": 0.9459868112677118}, {\"Text length\": 940, \"comment_text\": 0.9462682713585547}, {\"Text length\": 941, \"comment_text\": 0.9465652449977118}, {\"Text length\": 942, \"comment_text\": 0.9468494753650394}, {\"Text length\": 943, \"comment_text\": 0.9471392462853363}, {\"Text length\": 944, \"comment_text\": 0.9474384361456811}, {\"Text length\": 945, \"comment_text\": 0.9477354097848382}, {\"Text length\": 946, \"comment_text\": 0.9480412483087463}, {\"Text length\": 947, \"comment_text\": 0.9483520733303267}, {\"Text length\": 948, \"comment_text\": 0.9486717632366581}, {\"Text length\": 949, \"comment_text\": 0.9489992099171466}, {\"Text length\": 950, \"comment_text\": 0.9493177917128841}, {\"Text length\": 951, \"comment_text\": 0.9496640762734684}, {\"Text length\": 952, \"comment_text\": 0.9499699147973765}, {\"Text length\": 953, \"comment_text\": 0.9503017939202405}, {\"Text length\": 954, \"comment_text\": 0.9506198216606812}, {\"Text length\": 955, \"comment_text\": 0.950952808894139}, {\"Text length\": 956, \"comment_text\": 0.9512841339617061}, {\"Text length\": 957, \"comment_text\": 0.9516475942364954}, {\"Text length\": 958, \"comment_text\": 0.9520182572301448}, {\"Text length\": 959, \"comment_text\": 0.9523844877814187}, {\"Text length\": 960, \"comment_text\": 0.9527418534479417}, {\"Text length\": 961, \"comment_text\": 0.9531601651971275}, {\"Text length\": 962, \"comment_text\": 0.9535335984672616}, {\"Text length\": 963, \"comment_text\": 0.9539219912304129}, {\"Text length\": 964, \"comment_text\": 0.9543247894312845}, {\"Text length\": 965, \"comment_text\": 0.9547414390145795}, {\"Text length\": 966, \"comment_text\": 0.9551525480449052}, {\"Text length\": 967, \"comment_text\": 0.9555664273517156}, {\"Text length\": 968, \"comment_text\": 0.9559958202068402}, {\"Text length\": 969, \"comment_text\": 0.9564196725089953}, {\"Text length\": 970, \"comment_text\": 0.9568679032442156}, {\"Text length\": 971, \"comment_text\": 0.9573360799701255}, {\"Text length\": 972, \"comment_text\": 0.9577765539311887}, {\"Text length\": 973, \"comment_text\": 0.9582857307490718}, {\"Text length\": 974, \"comment_text\": 0.9587993400093303}, {\"Text length\": 975, \"comment_text\": 0.9593223682096368}, {\"Text length\": 976, \"comment_text\": 0.9599024641055276}, {\"Text length\": 977, \"comment_text\": 0.9604831140567154}, {\"Text length\": 978, \"comment_text\": 0.9610853721644835}, {\"Text length\": 979, \"comment_text\": 0.9617031438205659}, {\"Text length\": 980, \"comment_text\": 0.9623691182874816}, {\"Text length\": 981, \"comment_text\": 0.9630256738143494}, {\"Text length\": 982, \"comment_text\": 0.9637625673592728}, {\"Text length\": 983, \"comment_text\": 0.9645138663419165}, {\"Text length\": 984, \"comment_text\": 0.9653022870294548}, {\"Text length\": 985, \"comment_text\": 0.9662020728316769}, {\"Text length\": 986, \"comment_text\": 0.9670713855925677}, {\"Text length\": 987, \"comment_text\": 0.9680559418552209}, {\"Text length\": 988, \"comment_text\": 0.9690765117121749}, {\"Text length\": 989, \"comment_text\": 0.9701857304166385}, {\"Text length\": 990, \"comment_text\": 0.9714096385675676}, {\"Text length\": 991, \"comment_text\": 0.97279200653342}, {\"Text length\": 992, \"comment_text\": 0.9743566586919641}, {\"Text length\": 993, \"comment_text\": 0.9761545681305177}, {\"Text length\": 994, \"comment_text\": 0.9781951537891288}, {\"Text length\": 995, \"comment_text\": 0.9804634561747801}, {\"Text length\": 996, \"comment_text\": 0.9831051918305656}, {\"Text length\": 997, \"comment_text\": 0.9861885095580081}, {\"Text length\": 998, \"comment_text\": 0.9898447204624812}, {\"Text length\": 999, \"comment_text\": 0.9940599731615616}, {\"Text length\": 1000, \"comment_text\": 0.999993351336437}, {\"Text length\": 1006, \"comment_text\": 0.9999955675576248}, {\"Text length\": 1011, \"comment_text\": 0.9999961216129217}, {\"Text length\": 1535, \"comment_text\": 0.9999983378341095}, {\"Text length\": 1737, \"comment_text\": 0.9999988918894065}, {\"Text length\": 1891, \"comment_text\": 0.9999994459447035}, {\"Text length\": 1906, \"comment_text\": 1.0000000000000004}]}, \"encoding\": {\"tooltip\": [{\"field\": \"Text length\", \"type\": \"quantitative\"}, {\"field\": \"comment_text\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"title\": \"Text length\"}, \"field\": \"Text length\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"title\": \"Cummulative rate\"}, \"field\": \"comment_text\", \"type\": \"quantitative\"}}, \"mark\": \"line\", \"selection\": {\"selector011\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Cummulative text length\", \"width\": 400};     \n",
       "        vg_embed(\"#vega-chart-4\", spec, {defaultStyle: true}).catch(console.warn);\n",
       "        console.log(\"anything?\");\n",
       "    });\n",
       "    console.log(\"really...anything?\");\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render(alt.Chart(text_length).mark_line().encode(\n",
    "    x=alt.X(\"Text length:Q\", axis=alt.Axis(title='Text length')),\n",
    "    y=alt.Y('comment_text:Q', axis=alt.Axis(title='Cummulative rate')),\n",
    "    tooltip=['Text length', 'comment_text']\n",
    ").properties(title=\"Cummulative text length\", width=400).interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seeems that there is relatively high number of comments with length 1000. Maybe this is some kind of default max length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word_c\"></a>\n",
    "### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"vega-chart-5\"></div><script>\n",
       "    require([\"vega-embed\"], function(vg_embed) {\n",
       "        const spec = {\"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"config\": {\"view\": {\"height\": 300, \"width\": 400}}, \"data\": {\"name\": \"data-1c244982eff96285146beac7d4ea5018\"}, \"datasets\": {\"data-1c244982eff96285146beac7d4ea5018\": [{\"bins\": \"(0.684, 32.6]\", \"comment_text\": 852748}, {\"bins\": \"(32.6, 64.2]\", \"comment_text\": 439057}, {\"bins\": \"(64.2, 95.8]\", \"comment_text\": 211056}, {\"bins\": \"(95.8, 127.4]\", \"comment_text\": 120770}, {\"bins\": \"(127.4, 159.0]\", \"comment_text\": 94195}, {\"bins\": \"(159.0, 190.6]\", \"comment_text\": 84572}, {\"bins\": \"(190.6, 222.2]\", \"comment_text\": 2466}, {\"bins\": \"(222.2, 253.8]\", \"comment_text\": 2}, {\"bins\": \"(253.8, 285.4]\", \"comment_text\": 5}, {\"bins\": \"(285.4, 317.0]\", \"comment_text\": 3}]}, \"encoding\": {\"tooltip\": [{\"field\": \"comment_text\", \"type\": \"quantitative\"}, {\"field\": \"bins\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Target bins\"}, \"field\": \"bins\", \"sort\": [\"(0.684, 32.6]\", \"(32.6, 64.2]\", \"(64.2, 95.8]\", \"(95.8, 127.4]\", \"(127.4, 159.0]\", \"(159.0, 190.6]\", \"(190.6, 222.2]\", \"(222.2, 253.8]\", \"(253.8, 285.4]\", \"(285.4, 317.0]\"], \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"title\": \"Count\"}, \"field\": \"comment_text\", \"type\": \"quantitative\"}}, \"mark\": \"bar\", \"selection\": {\"selector012\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Counts of target bins of word count\", \"width\": 400};     \n",
       "        vg_embed(\"#vega-chart-5\", spec, {defaultStyle: true}).catch(console.warn);\n",
       "        console.log(\"anything?\");\n",
       "    });\n",
       "    console.log(\"really...anything?\");\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df = pd.cut(train['comment_text'].apply(lambda x: len(x.split())), 10).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n",
    "hist_df['bins'] = hist_df['bins'].astype(str)\n",
    "render(alt.Chart(hist_df).mark_bar().encode(\n",
    "    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins'), sort=list(hist_df['bins'].values)),\n",
    "    y=alt.Y('comment_text:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['comment_text', 'bins']\n",
    ").properties(title=\"Counts of target bins of word count\", width=400).interactive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"vega-chart-6\"></div><script>\n",
       "    require([\"vega-embed\"], function(vg_embed) {\n",
       "        const spec = {\"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"config\": {\"view\": {\"height\": 300, \"width\": 400}}, \"data\": {\"name\": \"data-edc092337ce94ca47138e5cf849e26e5\"}, \"datasets\": {\"data-edc092337ce94ca47138e5cf849e26e5\": [{\"Word count\": 1, \"comment_text\": 0.008526911019827423}, {\"Word count\": 2, \"comment_text\": 0.017261592776005415}, {\"Word count\": 3, \"comment_text\": 0.027886711205325133}, {\"Word count\": 4, \"comment_text\": 0.04070367238931914}, {\"Word count\": 5, \"comment_text\": 0.055205515731292044}, {\"Word count\": 6, \"comment_text\": 0.07109582164738369}, {\"Word count\": 7, \"comment_text\": 0.0881252652539734}, {\"Word count\": 8, \"comment_text\": 0.10593481871864739}, {\"Word count\": 9, \"comment_text\": 0.12422196784927922}, {\"Word count\": 10, \"comment_text\": 0.14281606361441296}, {\"Word count\": 11, \"comment_text\": 0.1615481191484835}, {\"Word count\": 12, \"comment_text\": 0.18023141781642374}, {\"Word count\": 13, \"comment_text\": 0.19872578362810922}, {\"Word count\": 14, \"comment_text\": 0.21679352686115486}, {\"Word count\": 15, \"comment_text\": 0.2345737153950913}, {\"Word count\": 16, \"comment_text\": 0.2517377944388362}, {\"Word count\": 17, \"comment_text\": 0.2685744268020926}, {\"Word count\": 18, \"comment_text\": 0.2851368017933662}, {\"Word count\": 19, \"comment_text\": 0.3012232432845728}, {\"Word count\": 20, \"comment_text\": 0.31670077800444796}, {\"Word count\": 21, \"comment_text\": 0.3318425552143806}, {\"Word count\": 22, \"comment_text\": 0.34641365546847036}, {\"Word count\": 23, \"comment_text\": 0.36065343065499306}, {\"Word count\": 24, \"comment_text\": 0.3745336239538051}, {\"Word count\": 25, \"comment_text\": 0.38800160011169754}, {\"Word count\": 26, \"comment_text\": 0.4010684402346092}, {\"Word count\": 27, \"comment_text\": 0.4138338742759883}, {\"Word count\": 28, \"comment_text\": 0.42612226670670644}, {\"Word count\": 29, \"comment_text\": 0.4382184019493882}, {\"Word count\": 30, \"comment_text\": 0.4498796037839761}, {\"Word count\": 31, \"comment_text\": 0.461354088983497}, {\"Word count\": 32, \"comment_text\": 0.47246954635060406}, {\"Word count\": 33, \"comment_text\": 0.48320104339693526}, {\"Word count\": 34, \"comment_text\": 0.4938727024711975}, {\"Word count\": 35, \"comment_text\": 0.504235752745067}, {\"Word count\": 36, \"comment_text\": 0.5142347886888504}, {\"Word count\": 37, \"comment_text\": 0.5241014054166663}, {\"Word count\": 38, \"comment_text\": 0.5335840618237063}, {\"Word count\": 39, \"comment_text\": 0.5428944070333996}, {\"Word count\": 40, \"comment_text\": 0.5518252243646926}, {\"Word count\": 41, \"comment_text\": 0.5606119873187825}, {\"Word count\": 42, \"comment_text\": 0.5691195064032172}, {\"Word count\": 43, \"comment_text\": 0.5775522280225657}, {\"Word count\": 44, \"comment_text\": 0.5857528004724984}, {\"Word count\": 45, \"comment_text\": 0.593878575457345}, {\"Word count\": 46, \"comment_text\": 0.601593241411866}, {\"Word count\": 47, \"comment_text\": 0.6092674613297107}, {\"Word count\": 48, \"comment_text\": 0.6167017753039825}, {\"Word count\": 49, \"comment_text\": 0.6238972914452755}, {\"Word count\": 50, \"comment_text\": 0.6309077531173922}, {\"Word count\": 51, \"comment_text\": 0.6379503499967312}, {\"Word count\": 52, \"comment_text\": 0.6447081624534456}, {\"Word count\": 53, \"comment_text\": 0.6513185962011754}, {\"Word count\": 54, \"comment_text\": 0.6577744485210603}, {\"Word count\": 55, \"comment_text\": 0.6640142192751406}, {\"Word count\": 56, \"comment_text\": 0.6702240710431865}, {\"Word count\": 57, \"comment_text\": 0.676488220230332}, {\"Word count\": 58, \"comment_text\": 0.6825085850868261}, {\"Word count\": 59, \"comment_text\": 0.6883754766260692}, {\"Word count\": 60, \"comment_text\": 0.6940866786268737}, {\"Word count\": 61, \"comment_text\": 0.699672110075274}, {\"Word count\": 62, \"comment_text\": 0.7051173655335498}, {\"Word count\": 63, \"comment_text\": 0.7105088776280228}, {\"Word count\": 64, \"comment_text\": 0.7157314028569308}, {\"Word count\": 65, \"comment_text\": 0.7208874414502066}, {\"Word count\": 66, \"comment_text\": 0.725902195942764}, {\"Word count\": 67, \"comment_text\": 0.7306897877635781}, {\"Word count\": 68, \"comment_text\": 0.7353737712438654}, {\"Word count\": 69, \"comment_text\": 0.7399912680885204}, {\"Word count\": 70, \"comment_text\": 0.7445600080670453}, {\"Word count\": 71, \"comment_text\": 0.7490323424239034}, {\"Word count\": 72, \"comment_text\": 0.7534553658593344}, {\"Word count\": 73, \"comment_text\": 0.7578551189722941}, {\"Word count\": 74, \"comment_text\": 0.7620864392749855}, {\"Word count\": 75, \"comment_text\": 0.7660955834036062}, {\"Word count\": 76, \"comment_text\": 0.770103065366336}, {\"Word count\": 77, \"comment_text\": 0.7741005743337209}, {\"Word count\": 78, \"comment_text\": 0.7779761911357802}, {\"Word count\": 79, \"comment_text\": 0.7817110778924181}, {\"Word count\": 80, \"comment_text\": 0.7853994240041134}, {\"Word count\": 81, \"comment_text\": 0.789001337489487}, {\"Word count\": 82, \"comment_text\": 0.7925206967356172}, {\"Word count\": 83, \"comment_text\": 0.7959868666732417}, {\"Word count\": 84, \"comment_text\": 0.799377685090483}, {\"Word count\": 85, \"comment_text\": 0.8028654631846879}, {\"Word count\": 86, \"comment_text\": 0.8061111191141322}, {\"Word count\": 87, \"comment_text\": 0.8093080181774464}, {\"Word count\": 88, \"comment_text\": 0.8124007548449367}, {\"Word count\": 89, \"comment_text\": 0.8154314373191703}, {\"Word count\": 90, \"comment_text\": 0.8184654441251856}, {\"Word count\": 91, \"comment_text\": 0.8214108020836913}, {\"Word count\": 92, \"comment_text\": 0.824305741010176}, {\"Word count\": 93, \"comment_text\": 0.8271735312271108}, {\"Word count\": 94, \"comment_text\": 0.8299681861448502}, {\"Word count\": 95, \"comment_text\": 0.8326680976068138}, {\"Word count\": 96, \"comment_text\": 0.8353752117876375}, {\"Word count\": 97, \"comment_text\": 0.8380385556000034}, {\"Word count\": 98, \"comment_text\": 0.840601615403624}, {\"Word count\": 99, \"comment_text\": 0.8431264453917561}, {\"Word count\": 100, \"comment_text\": 0.8456579240434514}, {\"Word count\": 101, \"comment_text\": 0.8480835781334322}, {\"Word count\": 102, \"comment_text\": 0.850458259136095}, {\"Word count\": 103, \"comment_text\": 0.8528562104612291}, {\"Word count\": 104, \"comment_text\": 0.8552153779155777}, {\"Word count\": 105, \"comment_text\": 0.8575507209921581}, {\"Word count\": 106, \"comment_text\": 0.8598212395989971}, {\"Word count\": 107, \"comment_text\": 0.8620668257174741}, {\"Word count\": 108, \"comment_text\": 0.8642453711450219}, {\"Word count\": 109, \"comment_text\": 0.866336375835654}, {\"Word count\": 110, \"comment_text\": 0.868437353521631}, {\"Word count\": 111, \"comment_text\": 0.8705704664148302}, {\"Word count\": 112, \"comment_text\": 0.8725661735943895}, {\"Word count\": 113, \"comment_text\": 0.8745790564881538}, {\"Word count\": 114, \"comment_text\": 0.8765393041287093}, {\"Word count\": 115, \"comment_text\": 0.8784923490504046}, {\"Word count\": 116, \"comment_text\": 0.8803850019447341}, {\"Word count\": 117, \"comment_text\": 0.8822161547011038}, {\"Word count\": 118, \"comment_text\": 0.8840234830797054}, {\"Word count\": 119, \"comment_text\": 0.8858562980019659}, {\"Word count\": 120, \"comment_text\": 0.8876780318182876}, {\"Word count\": 121, \"comment_text\": 0.88945267093437}, {\"Word count\": 122, \"comment_text\": 0.8911580531383354}, {\"Word count\": 123, \"comment_text\": 0.8928883678306629}, {\"Word count\": 124, \"comment_text\": 0.8945826689286898}, {\"Word count\": 125, \"comment_text\": 0.8962741997502319}, {\"Word count\": 126, \"comment_text\": 0.897915311539753}, {\"Word count\": 127, \"comment_text\": 0.8995813558176361}, {\"Word count\": 128, \"comment_text\": 0.901146562031477}, {\"Word count\": 129, \"comment_text\": 0.902728389904226}, {\"Word count\": 130, \"comment_text\": 0.90436174491959}, {\"Word count\": 131, \"comment_text\": 0.9059203024698678}, {\"Word count\": 132, \"comment_text\": 0.9075331574392451}, {\"Word count\": 133, \"comment_text\": 0.9091099988143218}, {\"Word count\": 134, \"comment_text\": 0.9106436238762374}, {\"Word count\": 135, \"comment_text\": 0.9121501002286033}, {\"Word count\": 136, \"comment_text\": 0.9137025631706148}, {\"Word count\": 137, \"comment_text\": 0.9151630529333351}, {\"Word count\": 138, \"comment_text\": 0.9166495832950113}, {\"Word count\": 139, \"comment_text\": 0.918166586698019}, {\"Word count\": 140, \"comment_text\": 0.919620981852473}, {\"Word count\": 141, \"comment_text\": 0.9211496204167162}, {\"Word count\": 142, \"comment_text\": 0.9227264617917929}, {\"Word count\": 143, \"comment_text\": 0.9242207489276262}, {\"Word count\": 144, \"comment_text\": 0.925710603621084}, {\"Word count\": 145, \"comment_text\": 0.9272519854571568}, {\"Word count\": 146, \"comment_text\": 0.9287324212105668}, {\"Word count\": 147, \"comment_text\": 0.9302970733691108}, {\"Word count\": 148, \"comment_text\": 0.9318938607348771}, {\"Word count\": 149, \"comment_text\": 0.9334995129853944}, {\"Word count\": 150, \"comment_text\": 0.93506748947572}, {\"Word count\": 151, \"comment_text\": 0.9367423986383541}, {\"Word count\": 152, \"comment_text\": 0.9384245105198483}, {\"Word count\": 153, \"comment_text\": 0.9401382035532678}, {\"Word count\": 154, \"comment_text\": 0.9419760049732008}, {\"Word count\": 155, \"comment_text\": 0.9437883198494746}, {\"Word count\": 156, \"comment_text\": 0.9456964862921183}, {\"Word count\": 157, \"comment_text\": 0.9476561798773769}, {\"Word count\": 158, \"comment_text\": 0.9496734952135167}, {\"Word count\": 159, \"comment_text\": 0.9517705945124152}, {\"Word count\": 160, \"comment_text\": 0.9539557886035263}, {\"Word count\": 161, \"comment_text\": 0.9560789285013807}, {\"Word count\": 162, \"comment_text\": 0.9583184200115913}, {\"Word count\": 163, \"comment_text\": 0.9605706547936315}, {\"Word count\": 164, \"comment_text\": 0.9628334166263134}, {\"Word count\": 165, \"comment_text\": 0.9651266515003267}, {\"Word count\": 166, \"comment_text\": 0.9674065890472137}, {\"Word count\": 167, \"comment_text\": 0.969632229175001}, {\"Word count\": 168, \"comment_text\": 0.9719082883348094}, {\"Word count\": 169, \"comment_text\": 0.9741134284166101}, {\"Word count\": 170, \"comment_text\": 0.9762548521392633}, {\"Word count\": 171, \"comment_text\": 0.9783984920831043}, {\"Word count\": 172, \"comment_text\": 0.9803748073272709}, {\"Word count\": 173, \"comment_text\": 0.9823112305900582}, {\"Word count\": 174, \"comment_text\": 0.9841052616415332}, {\"Word count\": 175, \"comment_text\": 0.9858034411266385}, {\"Word count\": 176, \"comment_text\": 0.9874185123172037}, {\"Word count\": 177, \"comment_text\": 0.9888773399140331}, {\"Word count\": 178, \"comment_text\": 0.9902286807832572}, {\"Word count\": 179, \"comment_text\": 0.9914365213305752}, {\"Word count\": 180, \"comment_text\": 0.9925717806339948}, {\"Word count\": 181, \"comment_text\": 0.9935884721038702}, {\"Word count\": 182, \"comment_text\": 0.9944893660166864}, {\"Word count\": 183, \"comment_text\": 0.9953237732938702}, {\"Word count\": 184, \"comment_text\": 0.9960534641199335}, {\"Word count\": 185, \"comment_text\": 0.996661262780671}, {\"Word count\": 186, \"comment_text\": 0.9971831828703837}, {\"Word count\": 187, \"comment_text\": 0.9976347379373856}, {\"Word count\": 188, \"comment_text\": 0.9980170360922705}, {\"Word count\": 189, \"comment_text\": 0.9983649828187456}, {\"Word count\": 190, \"comment_text\": 0.9986281590847897}, {\"Word count\": 191, \"comment_text\": 0.998871943415441}, {\"Word count\": 192, \"comment_text\": 0.9990885790365426}, {\"Word count\": 193, \"comment_text\": 0.9992697551186402}, {\"Word count\": 194, \"comment_text\": 0.9994066067769831}, {\"Word count\": 195, \"comment_text\": 0.9995240664999333}, {\"Word count\": 196, \"comment_text\": 0.999636539725211}, {\"Word count\": 197, \"comment_text\": 0.9997202020750482}, {\"Word count\": 198, \"comment_text\": 0.9997789319365233}, {\"Word count\": 199, \"comment_text\": 0.999824918526169}, {\"Word count\": 200, \"comment_text\": 0.9998603780651728}, {\"Word count\": 201, \"comment_text\": 0.9998980538253643}, {\"Word count\": 202, \"comment_text\": 0.9999213241478356}, {\"Word count\": 203, \"comment_text\": 0.9999373917514467}, {\"Word count\": 204, \"comment_text\": 0.9999506890785731}, {\"Word count\": 205, \"comment_text\": 0.9999639864056995}, {\"Word count\": 206, \"comment_text\": 0.999968418848075}, {\"Word count\": 207, \"comment_text\": 0.9999717431798566}, {\"Word count\": 208, \"comment_text\": 0.9999772837328259}, {\"Word count\": 209, \"comment_text\": 0.999985040506983}, {\"Word count\": 210, \"comment_text\": 0.9999867026728738}, {\"Word count\": 211, \"comment_text\": 0.9999878107834677}, {\"Word count\": 212, \"comment_text\": 0.9999889188940615}, {\"Word count\": 213, \"comment_text\": 0.9999894729493585}, {\"Word count\": 214, \"comment_text\": 0.9999905810599523}, {\"Word count\": 215, \"comment_text\": 0.9999916891705461}, {\"Word count\": 217, \"comment_text\": 0.9999922432258431}, {\"Word count\": 219, \"comment_text\": 0.9999939053917339}, {\"Word count\": 220, \"comment_text\": 0.9999944594470309}, {\"Word count\": 228, \"comment_text\": 0.9999950135023279}, {\"Word count\": 253, \"comment_text\": 0.9999955675576249}, {\"Word count\": 266, \"comment_text\": 0.9999977837788127}, {\"Word count\": 272, \"comment_text\": 0.9999983378341096}, {\"Word count\": 311, \"comment_text\": 0.9999988918894066}, {\"Word count\": 315, \"comment_text\": 0.9999994459447036}, {\"Word count\": 317, \"comment_text\": 1.0000000000000004}]}, \"encoding\": {\"tooltip\": [{\"field\": \"Word count\", \"type\": \"quantitative\"}, {\"field\": \"comment_text\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"title\": \"Text length\"}, \"field\": \"Word count\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"title\": \"Cummulative rate\"}, \"field\": \"comment_text\", \"type\": \"quantitative\"}}, \"mark\": \"line\", \"selection\": {\"selector013\": {\"bind\": \"scales\", \"encodings\": [\"x\", \"y\"], \"type\": \"interval\"}}, \"title\": \"Cummulative word cound\", \"width\": 400};     \n",
       "        vg_embed(\"#vega-chart-6\", spec, {defaultStyle: true}).catch(console.warn);\n",
       "        console.log(\"anything?\");\n",
       "    });\n",
       "    console.log(\"really...anything?\");\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = train['comment_text'].apply(lambda x: len(x.split())).value_counts(normalize=True).sort_index().cumsum().reset_index().rename(columns={'index': 'Word count'})\n",
    "render(alt.Chart(word_count).mark_line().encode(\n",
    "    x=alt.X(\"Word count:Q\", axis=alt.Axis(title='Text length')),\n",
    "    y=alt.Y('comment_text:Q', axis=alt.Axis(title='Cummulative rate')),\n",
    "    tooltip=['Word count:Q', 'comment_text']\n",
    ").properties(title=\"Cummulative word cound\", width=400).interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ~ 90% of all comments have less than 125 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identities\n",
    "\n",
    "Some of the comments are labeled with identities, but only eight of them are included into evaluation: male, female, homosexual_gay_or_lesbian, christian, jewish, muslim, black, white, psychiatric_or_mental_illness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"basic_model\"></a>\n",
    "## Basic model\n",
    "\n",
    "Let's try building a baseline logistic regression on tf-idf and see what words are considered to be toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll load processed texts from my kernel\n",
    "train = pd.read_csv('../input/jigsaw-public-files/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-public-files/test.csv')\n",
    "train['comment_text'] = train['comment_text'].fillna('')\n",
    "test['comment_text'] = test['comment_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "for col in identity_columns + ['target']:\n",
    "    train[col] = np.where(train[col] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train, test_size=0.1, stratify=train['target'])\n",
    "y_train = train_df['target']\n",
    "y_valid = valid_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 35s, sys: 11.7 s, total: 15min 46s\n",
      "Wall time: 15min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize, max_features=30000)\n",
    "vectorizer.fit(train['comment_text'].values)\n",
    "train_vectorized = vectorizer.transform(train_df['comment_text'].values)\n",
    "valid_vectorized = vectorizer.transform(valid_df['comment_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 484 ms, total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_vectorized, y_train)\n",
    "oof_name = 'predicted_target'\n",
    "valid_df[oof_name] = logreg.predict_proba(valid_vectorized)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"validation_function\"></a>\n",
    "### Validation function\n",
    "I use code from benchmark kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnsp_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>subgroup_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.954558</td>\n",
       "      <td>0.738506</td>\n",
       "      <td>black</td>\n",
       "      <td>0.768979</td>\n",
       "      <td>1481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.946132</td>\n",
       "      <td>0.792697</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>2136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.946711</td>\n",
       "      <td>0.785501</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>0.797996</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.964858</td>\n",
       "      <td>0.755173</td>\n",
       "      <td>white</td>\n",
       "      <td>0.814156</td>\n",
       "      <td>2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.958412</td>\n",
       "      <td>0.817552</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>0.852916</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.951642</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.859392</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948084</td>\n",
       "      <td>0.861685</td>\n",
       "      <td>male</td>\n",
       "      <td>0.877062</td>\n",
       "      <td>4376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.945415</td>\n",
       "      <td>0.869026</td>\n",
       "      <td>female</td>\n",
       "      <td>0.880318</td>\n",
       "      <td>5352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.936866</td>\n",
       "      <td>0.910391</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>4084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnsp_auc  bpsn_auc                       subgroup  subgroup_auc  \\\n",
       "6  0.954558  0.738506                          black      0.768979   \n",
       "5  0.946132  0.792697                         muslim      0.793814   \n",
       "2  0.946711  0.785501      homosexual_gay_or_lesbian      0.797996   \n",
       "7  0.964858  0.755173                          white      0.814156   \n",
       "8  0.958412  0.817552  psychiatric_or_mental_illness      0.852916   \n",
       "4  0.951642  0.840298                         jewish      0.859392   \n",
       "0  0.948084  0.861685                           male      0.877062   \n",
       "1  0.945415  0.869026                         female      0.880318   \n",
       "3  0.936866  0.910391                      christian      0.910400   \n",
       "\n",
       "   subgroup_size  \n",
       "6           1481  \n",
       "5           2136  \n",
       "2           1086  \n",
       "7           2549  \n",
       "8            467  \n",
       "4            757  \n",
       "0           4376  \n",
       "1           5352  \n",
       "3           4084  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, oof_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[oof_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, oof_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[oof_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, oof_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[oof_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "oof_name = 'predicted_target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, oof_name, 'target')\n",
    "bias_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8811545557310997"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_overall_auc(df, oof_name):\n",
    "    true_labels = df['target']\n",
    "    predicted_labels = df[oof_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n",
    "get_final_metric(bias_metrics_df, calculate_overall_auc(valid_df, oof_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eli5\"></a>\n",
    "## ELI5 for model interpretation\n",
    "\n",
    "And now let's use ELI5 to see how model makes predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>1.000</b>, score <b>10.195</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +10.815\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.29%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.621\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 98.87%); opacity: 0.80\" title=\"0.018\">oh</span><span style=\"opacity: 0.80\"> , </span><span style=\"background-color: hsl(120, 100.00%, 94.87%); opacity: 0.81\" title=\"0.155\">bullshit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.14%); opacity: 0.81\" title=\"0.143\">your</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.24%); opacity: 0.81\" title=\"-0.099\">opinions</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.14%); opacity: 0.81\" title=\"0.144\">are</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.98%); opacity: 0.81\" title=\"0.109\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.01%); opacity: 0.85\" title=\"0.717\">worthless</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.82%); opacity: 0.80\" title=\"0.019\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.09%); opacity: 0.81\" title=\"0.105\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.36%); opacity: 0.80\" title=\"-0.060\">word</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.36%); opacity: 0.80\" title=\"0.008\">from</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.84%); opacity: 0.82\" title=\"0.355\">trump</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.21%); opacity: 0.82\" title=\"0.231\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.97%); opacity: 0.82\" title=\"0.294\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.89%); opacity: 0.81\" title=\"0.199\">lie</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.12%); opacity: 0.80\" title=\"-0.037\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.94%); opacity: 0.81\" title=\"-0.111\">that</span><span style=\"opacity: 0.80\"> all </span><span style=\"background-color: hsl(120, 100.00%, 99.29%); opacity: 0.80\" title=\"0.009\">you</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.57%); opacity: 0.80\" title=\"-0.004\">ve</span><span style=\"opacity: 0.80\"> got , </span><span style=\"background-color: hsl(120, 100.00%, 98.00%); opacity: 0.80\" title=\"0.040\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.34%); opacity: 0.81\" title=\"-0.096\">worn</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.14%); opacity: 0.82\" title=\"0.285\">out</span><span style=\"opacity: 0.80\"> , </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"2.914\">pathetic</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.13%); opacity: 0.83\" title=\"0.453\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.76%); opacity: 0.81\" title=\"0.118\">clearly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.67%); opacity: 0.80\" title=\"-0.003\">wrong</span><span style=\"opacity: 0.80\"> &quot; </span><span style=\"background-color: hsl(120, 100.00%, 98.35%); opacity: 0.80\" title=\"0.031\">sore</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.57%); opacity: 0.92\" title=\"1.700\">loser</span><span style=\"opacity: 0.80\"> &quot; </span><span style=\"background-color: hsl(0, 100.00%, 95.74%); opacity: 0.81\" title=\"-0.119\">argument</span><span style=\"opacity: 0.80\"> ? </span><span style=\"background-color: hsl(120, 100.00%, 95.51%); opacity: 0.81\" title=\"0.128\">are</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.33%); opacity: 0.82\" title=\"0.225\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.47%); opacity: 0.81\" title=\"0.130\">federal</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.94%); opacity: 0.80\" title=\"0.042\">courts</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.34%); opacity: 0.81\" title=\"-0.178\">sore</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.65%); opacity: 0.91\" title=\"1.605\">losers</span><span style=\"opacity: 0.80\"> ? </span><span style=\"background-color: hsl(120, 100.00%, 94.61%); opacity: 0.81\" title=\"0.166\">are</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.17%); opacity: 0.81\" title=\"0.142\">all</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.16%); opacity: 0.80\" title=\"0.036\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.10%); opacity: 0.80\" title=\"0.013\">ags</span><span style=\"opacity: 0.80\"> , </span><span style=\"background-color: hsl(0, 100.00%, 96.01%); opacity: 0.81\" title=\"-0.108\">legal</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.42%); opacity: 0.82\" title=\"-0.221\">scholars</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.47%); opacity: 0.80\" title=\"-0.006\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.24%); opacity: 0.81\" title=\"0.099\">republican</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.72%); opacity: 0.81\" title=\"0.082\">attorneys</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.68%); opacity: 0.81\" title=\"0.083\">who</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.01%); opacity: 0.81\" title=\"-0.108\">have</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.48%); opacity: 0.80\" title=\"-0.027\">opined</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.46%); opacity: 0.80\" title=\"-0.006\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.00%); opacity: 0.80\" title=\"-0.072\">eo</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.70%); opacity: 0.81\" title=\"-0.083\">is</span><span style=\"opacity: 0.80\"> illegal </span><span style=\"background-color: hsl(0, 100.00%, 94.34%); opacity: 0.81\" title=\"-0.178\">sore</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.48%); opacity: 0.92\" title=\"1.797\">losers</span><span style=\"opacity: 0.80\"> ? </span><span style=\"background-color: hsl(120, 100.00%, 94.17%); opacity: 0.81\" title=\"0.186\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.82%); opacity: 0.81\" title=\"0.078\">only</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.22%); opacity: 0.81\" title=\"-0.100\">sore</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.51%); opacity: 0.92\" title=\"1.706\">losers</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.21%); opacity: 0.81\" title=\"0.101\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.13%); opacity: 0.81\" title=\"-0.104\">see</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.62%); opacity: 0.80\" title=\"0.024\">are</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.60%); opacity: 0.81\" title=\"0.213\">trump</span><span style=\"opacity: 0.80\"> , </span><span style=\"background-color: hsl(120, 100.00%, 94.76%); opacity: 0.81\" title=\"0.160\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.84%); opacity: 0.81\" title=\"0.115\">brown</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.72%); opacity: 0.80\" title=\"-0.002\">nosing</span><span style=\"opacity: 0.80\"> supporters </span><span style=\"background-color: hsl(120, 100.00%, 97.40%); opacity: 0.80\" title=\"0.059\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.20%); opacity: 0.81\" title=\"0.185\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.18%); opacity: 0.81\" title=\"-0.142\">cabinet</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.29%); opacity: 0.80\" title=\"-0.062\">it</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.31%); opacity: 0.81\" title=\"0.097\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.22%); opacity: 0.84\" title=\"0.636\">muslim</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.67%); opacity: 0.81\" title=\"-0.164\">ban</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.92%); opacity: 0.81\" title=\"0.075\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.01%); opacity: 0.80\" title=\"0.040\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.70%); opacity: 0.81\" title=\"-0.083\">is</span><span style=\"opacity: 0.80\"> illegal</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "\n",
    "te = TextExplainer(random_state=42)\n",
    "def model_predict(x):\n",
    "    return logreg.predict_proba(vectorizer.transform(x))\n",
    "te.fit(valid_df['comment_text'].values[2:3][0], model_predict)\n",
    "te.show_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.995</b>, score <b>-5.213</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.641\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 95.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.572\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 84.39%); opacity: 0.85\" title=\"-0.119\">exactly</span><span style=\"opacity: 0.80\"> , </span><span style=\"background-color: hsl(0, 100.00%, 75.05%); opacity: 0.90\" title=\"-0.233\">so</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.61%); opacity: 0.80\" title=\"-0.004\">many</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 69.34%); opacity: 0.94\" title=\"-0.312\">liberal</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.96%); opacity: 0.84\" title=\"-0.092\">based</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.43%); opacity: 0.87\" title=\"0.165\">media</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.28%); opacity: 0.83\" title=\"-0.079\">will</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 66.62%); opacity: 0.95\" title=\"-0.353\">deny</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.86%); opacity: 0.83\" title=\"-0.074\">it</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.11%); opacity: 0.81\" title=\"0.016\">or</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.69%); opacity: 0.81\" title=\"0.013\">manage</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.16%); opacity: 0.90\" title=\"0.231\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.18%); opacity: 0.84\" title=\"0.080\">utilize</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.10%); opacity: 0.83\" title=\"0.071\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 62.65%); opacity: 0.98\" title=\"0.414\">story</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.41%); opacity: 0.82\" title=\"0.051\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.06%); opacity: 0.87\" title=\"-0.157\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.27%); opacity: 0.81\" title=\"0.015\">means</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.26%); opacity: 0.81\" title=\"0.022\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.13%); opacity: 0.80\" title=\"-0.006\">distracting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.98%); opacity: 0.83\" title=\"-0.063\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.71%); opacity: 0.81\" title=\"-0.033\">ongoing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.457\">trump</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.50%); opacity: 0.81\" title=\"-0.014\">investigation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.58%); opacity: 0.82\" title=\"-0.041\">we</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.81%); opacity: 0.87\" title=\"-0.160\">will</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 76.12%); opacity: 0.90\" title=\"-0.219\">now</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 73.96%); opacity: 0.91\" title=\"-0.247\">see</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.18%); opacity: 0.84\" title=\"0.080\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.26%); opacity: 0.83\" title=\"0.070\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.64%); opacity: 0.86\" title=\"0.139\">dnc</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.01%); opacity: 0.86\" title=\"-0.134\">ended</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.51%); opacity: 0.86\" title=\"-0.140\">up</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.31%); opacity: 0.91\" title=\"-0.243\">being</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 64.64%); opacity: 0.97\" title=\"-0.383\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.21%); opacity: 0.82\" title=\"-0.044\">driving</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.83%); opacity: 0.81\" title=\"-0.032\">force</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.78%); opacity: 0.83\" title=\"-0.065\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.75%); opacity: 0.88\" title=\"-0.173\">creating</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.98%); opacity: 0.87\" title=\"-0.158\">something</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 75.65%); opacity: 0.90\" title=\"-0.225\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.77%); opacity: 0.82\" title=\"-0.056\">never</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.85%); opacity: 0.90\" title=\"0.222\">was</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.41%); opacity: 0.81\" title=\"0.021\">it</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 73.95%); opacity: 0.91\" title=\"-0.248\">getting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.35%); opacity: 0.81\" title=\"0.028\">real</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 63.10%); opacity: 0.98\" title=\"-0.407\">juicy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.81%); opacity: 0.83\" title=\"0.065\">now</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.fit(valid_df['comment_text'].values[12:13][0], model_predict)\n",
    "te.show_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectorized = vectorizer.transform(test['comment_text'].values)\n",
    "sub['prediction'] = logreg.predict_proba(test_vectorized)[:, 1]\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "del logreg, vectorizer, test_vectorized, train_vectorized, valid_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting number of words and sequence length\n",
    "\n",
    "On of important hyperparameters for our neural nets will be the number of words in tokenizer and the number of words in sequence. Let's compare model AUC for different values of these parameters.\n",
    "\n",
    "For preparing data I use code from my kernel: https://www.kaggle.com/artgor/basic-cnn-in-keras\n",
    "\n",
    "I train the same model on the same data for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "def build_model(X_train, y_train, X_valid, y_valid, max_len, max_features, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0,  dense_units=128, dr=0.1):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "    # from benchmark kernel\n",
    "    x = Conv1D(128, 2, activation='relu', padding='same')(x1)\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "    x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = Dense(2, activation = \"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_train, batch_size = 128, epochs = 3, validation_data=(X_valid, y_valid), \n",
    "                        verbose = 0, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = list(train['comment_text'].values) + list(test['comment_text'].values)\n",
    "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "embed_size = 300\n",
    "oof_name = 'oof_name'\n",
    "\n",
    "def calculate_score(num_words, max_len, full_text, train_df, valid_df, embedding_path, embed_size, identity_columns, oof_name):\n",
    "    tk = Tokenizer(lower = True, filters='', num_words=num_words)\n",
    "    tk.fit_on_texts(full_text)\n",
    "    \n",
    "    def get_coefs(word,*arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "    embedding_matrix = np.zeros((num_words + 1, embed_size))\n",
    "    for word, i in tk.word_index.items():\n",
    "        if i >= num_words: continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    del embedding_index\n",
    "            \n",
    "    train_tokenized = tk.texts_to_sequences(train_df['comment_text'])\n",
    "    valid_tokenized = tk.texts_to_sequences(valid_df['comment_text'])\n",
    "\n",
    "    X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "    X_valid = pad_sequences(valid_tokenized, maxlen = max_len)\n",
    "    \n",
    "    model = build_model(X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, max_len=max_len, max_features=embedding_matrix.shape[0], embedding_matrix=embedding_matrix,\n",
    "                        lr = 1e-3, lr_d = 0, spatial_dr = 0.0, dr=0.1)\n",
    "    \n",
    "    valid_df[oof_name] = model.predict(X_valid)\n",
    "    bias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, oof_name, 'target')\n",
    "    score = get_final_metric(bias_metrics_df, calculate_overall_auc(valid_df, oof_name))\n",
    "    del embedding_matrix, tk\n",
    "    gc.collect()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for n_words in [50000, 100000]:\n",
    "#     for seq_len in [150, 300]:\n",
    "#         loc_score = calculate_score(n_words, seq_len, full_text, train_df, valid_df, embedding_path, embed_size, identity_columns, oof_name)\n",
    "#         scores.append((n_words, seq_len, loc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this loop takes a lot of time, so here is the result:\n",
    "\n",
    "![](https://i.imgur.com/fISAEg7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lime\"></a>\n",
    "## Interpreting deep learning models with LIME\n",
    "\n",
    "Previously we were able to interpret logreg predictions, but who uses logreg in such competitions? :)\n",
    "\n",
    "So let's try using a similar method to interpret deep learning model prediction! Technically it works almost the same:\n",
    "* train DL model with 2 classes;\n",
    "* write a function to make prediction on raw texts;\n",
    "* use ELI5 with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12353, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12353 to 0.12117, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12117\n"
     ]
    }
   ],
   "source": [
    "num_words = 150000\n",
    "max_len = 220\n",
    "tk = Tokenizer(lower = True, filters='', num_words=num_words)\n",
    "tk.fit_on_texts(full_text)\n",
    "\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "embedding_matrix = np.zeros((num_words + 1, embed_size))\n",
    "for word, i in tk.word_index.items():\n",
    "    if i >= num_words: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "del embedding_index\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(train_df['comment_text'])\n",
    "valid_tokenized = tk.texts_to_sequences(valid_df['comment_text'])\n",
    "\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_valid = pad_sequences(valid_tokenized, maxlen = max_len)\n",
    "\n",
    "model = build_model(X_train=X_train, y_train=pd.get_dummies(y_train), X_valid=X_valid, y_valid=pd.get_dummies(y_valid), max_len=max_len, max_features=embedding_matrix.shape[0],\n",
    "                    embedding_matrix=embedding_matrix,\n",
    "                    lr = 1e-3, lr_d = 0, spatial_dr = 0.0, dr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>1.000</b>, score <b>-8.223</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +7.550\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.673\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 63.86%); opacity: 0.97\" title=\"0.474\">they</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.548\">stood</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 62.97%); opacity: 0.98\" title=\"0.490\">was</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 62.49%); opacity: 0.98\" title=\"0.499\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.74%); opacity: 0.87\" title=\"0.193\">not</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.06%); opacity: 0.84\" title=\"0.109\">enough</span><span style=\"opacity: 0.80\"> ? </span><span style=\"background-color: hsl(120, 100.00%, 81.97%); opacity: 0.86\" title=\"0.175\">now</span><span style=\"opacity: 0.80\"> , </span><span style=\"background-color: hsl(120, 100.00%, 98.94%); opacity: 0.80\" title=\"0.003\">can</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.54%); opacity: 0.83\" title=\"0.081\">we</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.10%); opacity: 0.88\" title=\"0.217\">please</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.17%); opacity: 0.80\" title=\"-0.012\">put</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.39%); opacity: 0.85\" title=\"0.130\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.08%); opacity: 0.90\" title=\"0.278\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.87%); opacity: 0.84\" title=\"0.100\">bed</span><span style=\"opacity: 0.80\"> ? </span><span style=\"background-color: hsl(120, 100.00%, 80.04%); opacity: 0.87\" title=\"0.203\">stop</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.10%); opacity: 0.80\" title=\"-0.013\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 73.09%); opacity: 0.91\" title=\"-0.311\">crying</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.24%); opacity: 0.80\" title=\"0.002\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.98%); opacity: 0.80\" title=\"0.008\">move</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.69%); opacity: 0.81\" title=\"0.023\">on</span><span style=\"opacity: 0.80\"> &lt; </span><span style=\"background-color: hsl(120, 100.00%, 78.06%); opacity: 0.88\" title=\"0.232\">sheesh</span><span style=\"opacity: 0.80\"> &gt;</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = TextExplainer(random_state=42)\n",
    "def dl_predict(x):\n",
    "    return model.predict(pad_sequences(tk.texts_to_sequences(np.array(x)), maxlen = max_len))\n",
    "te.fit(valid_df['comment_text'].values[3:4][0], dl_predict)\n",
    "te.show_prediction(target_names=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how our neural net makes predictions and use it to improve the model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
