{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### In this kernel, I will walk you through some extra text cleaning methods and how they work on sample comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.imgur.com/CtyQ8Ag.png\" width=\"250px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "I have borrowed the text cleaning functions from Dimitrios in [this great kernel](https://www.kaggle.com/deffro/text-pre-processing-techniques) from the Quora Insincere Questions Classification competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See first few rows of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            ...             toxicity_annotator_count\n",
       "0  59848            ...                                    4\n",
       "1  59849            ...                                    4\n",
       "2  59852            ...                                    4\n",
       "3  59855            ...                                    4\n",
       "4  59856            ...                                   47\n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract comments from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = train_df['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function for visualizing the effect of text cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_cleaning_results(function):\n",
    "    select_comments = []\n",
    "    for i, comment in enumerate(comments):\n",
    "        if comment != function(comment):\n",
    "            select_comments.append(comment)\n",
    "        if len(select_comments) == 5:\n",
    "            break\n",
    "    \n",
    "    print(\"                          \" +\\\n",
    "          f'{Style.DIM}'+\\\n",
    "          \"EXAMPLE WORKING OF TEXT CLEANING FUNCTION\"+\\\n",
    "          f'{Style.RESET_ALL}')\n",
    "    print(\"                          \" +\\\n",
    "          f'{Style.DIM}'+\\\n",
    "          \"-------------------------------------------\"+\\\n",
    "          f'{Style.RESET_ALL}')\n",
    "    print(\"\")\n",
    "\n",
    "    for comment in select_comments:\n",
    "        print(f'{Fore.YELLOW}{Style.DIM}' + comment + f'{Style.RESET_ALL}' +\\\n",
    "              '\\n\\n' + \"                                     \"+\\\n",
    "              'CHANGES TO' + '\\n\\n' +\\\n",
    "              f'{Fore.CYAN}{Style.DIM}' + function(comment) + f'{Style.RESET_ALL}')\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "        print(f'{Fore.WHITE}{Style.DIM}' +\\\n",
    "              \"-------------------------\"+\\\n",
    "              \"-------------------------\"+\\\n",
    "              \"-------------------------\"+\\\n",
    "              \"------------------\" +\\\n",
    "              f'{Style.RESET_ALL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the numbers\n",
    "This function removes all the numbers in the comment\n",
    "\n",
    "Eg. I'm 25 years old. --> I'm years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \u001b[2mEXAMPLE WORKING OF TEXT CLEANING FUNCTION\u001b[0m\n",
      "                          \u001b[2m-------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[2mAngry trolls, misogynists and Racists\", oh my. It doesn't take all of my 150 IQ to see the slant here.  it's the \"Diversity diode\" at work yet again. \"We can say anything that we want because we are Diversity. You on the other hand must only  say what we allow you to say. From now on, winning arguments against any member of diversity will be considered offensive language.  facts, cogent, linear posts and Math are now verboten.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mAngry trolls, misogynists and Racists\", oh my. It doesn't take all of my  IQ to see the slant here.  it's the \"Diversity diode\" at work yet again. \"We can say anything that we want because we are Diversity. You on the other hand must only  say what we allow you to say. From now on, winning arguments against any member of diversity will be considered offensive language.  facts, cogent, linear posts and Math are now verboten.\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mGreat question! It's one we're asked a lot. We've designed the system assuming that people *will* try to abuse it. So, in addition to the peer reviews, there are algorithms on the backend doing a lot of meta-analysis. I'm sure the system isn't 100% perfect yet, but we know from months of beta testing that it's a really solid start, and we'll keep working to improve it!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mGreat question! It's one we're asked a lot. We've designed the system assuming that people *will* try to abuse it. So, in addition to the peer reviews, there are algorithms on the backend doing a lot of meta-analysis. I'm sure the system isn't % perfect yet, but we know from months of beta testing that it's a really solid start, and we'll keep working to improve it!\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mTroll free since 2016.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mTroll free since .\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mNot for long!  \n",
      "\n",
      "(Troll-In-Training since 2016)\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mNot for long!  \n",
      "\n",
      "(Troll-In-Training since )\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mYET ANOTHER BARACK OBAMA LIBERAL MEDIA CONSPIRACY BY THE THOUGHT CONTROL POLICE. I DIDENT SPEND 30 YEARS MIXING CONCRETE TO LET AMERICA FALL TO THE COMMIES. THE TAXES HERE ARE SO HIGH I CANT EVEN AFFORD A KEYBOARD WITH A WORKING CAPS LOCK. PORTLAND HAS BEEN GOING DOWNHILL FOR YEARS, NO WONDER THE TEA PARTY IS MAKING A COMEBACK. WHATS NEXT FLOURAIDE IN THE WATER SUPPLY?\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mYET ANOTHER BARACK OBAMA LIBERAL MEDIA CONSPIRACY BY THE THOUGHT CONTROL POLICE. I DIDENT SPEND  YEARS MIXING CONCRETE TO LET AMERICA FALL TO THE COMMIES. THE TAXES HERE ARE SO HIGH I CANT EVEN AFFORD A KEYBOARD WITH A WORKING CAPS LOCK. PORTLAND HAS BEEN GOING DOWNHILL FOR YEARS, NO WONDER THE TEA PARTY IS MAKING A COMEBACK. WHATS NEXT FLOURAIDE IN THE WATER SUPPLY?\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "example_cleaning_results(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the exclamation, question and full stop marks\n",
    "This function removes the exclamation, question and full stop marks from the comment.\n",
    "\n",
    "Eg. This is awesome ! --> This is awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_multi_exclamation_mark(text):\n",
    "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
    "    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
    "    return text\n",
    "\n",
    "def replace_multi_question_mark(text):\n",
    "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
    "    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
    "    return text\n",
    "\n",
    "def replace_multi_stop_mark(text):\n",
    "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
    "    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \u001b[2mEXAMPLE WORKING OF TEXT CLEANING FUNCTION\u001b[0m\n",
      "                          \u001b[2m-------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[2mThis is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis is so cool. It's like, 'would you want your mother to read this multiQuestion ' Really great idea, well done!\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThank you multiExclamation  This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mAwesome! Signed up just to give this a shot... good luck to your enterprise!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mAwesome! Signed up just to give this a shot multiStop  good luck to your enterprise!\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2m\"winning arguments against any member of diversity\"\n",
      "\n",
      "You may have inadvertently given Civil Comments an idea for the 2.0 version of their system.\n",
      "\n",
      "1. Is this a good comment?\n",
      "\n",
      "2. Is this comment civil?\n",
      "\n",
      "3. Did this comment totally just win the argument???\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2m\"winning arguments against any member of diversity\"\n",
      "\n",
      "You may have inadvertently given Civil Comments an idea for the 2.0 version of their system.\n",
      "\n",
      "1. Is this a good comment?\n",
      "\n",
      "2. Is this comment civil?\n",
      "\n",
      "3. Did this comment totally just win the argument multiQuestion \u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThe letter writing campaign to get livestock off of Malheur Refuge started in the late '70's. The death threat phoned in to Nancy and Denzel's home was in 1979; along with two other friends I was hired a bodyguard to help protect N&D that weekend. The Hammond's ejection of the Ferguson's from the Diamond Dance was the year or two before the death threat. Great article WW and Jim D. For the last 10 days, lots of us have been wishing we could hear Denzel make witty quips about the Bundycon's occupation of the refuge hq. Sacred Cows at the public trough is easy to get on Amazon. Read and learn, read and learn ... great history of western livestock abuses ...\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThe letter writing campaign to get livestock off of Malheur Refuge started in the late '70's. The death threat phoned in to Nancy and Denzel's home was in 1979; along with two other friends I was hired a bodyguard to help protect N&D that weekend. The Hammond's ejection of the Ferguson's from the Diamond Dance was the year or two before the death threat. Great article WW and Jim D. For the last 10 days, lots of us have been wishing we could hear Denzel make witty quips about the Bundycon's occupation of the refuge hq. Sacred Cows at the public trough is easy to get on Amazon. Read and learn, read and learn  multiStop  great history of western livestock abuses  multiStop \u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "example_cleaning_results(lambda x: replace_multi_exclamation_mark(replace_multi_question_mark(replace_multi_stop_mark(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the exclamation, question and full stop marks\n",
    "This function removes the exclamation, question and full stop marks from the comment.\n",
    "\n",
    "Eg. You love cats !? I prefer dogs. --> You love cats I prefer dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_patterns = [(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\\\n",
    "                        (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'),\\\n",
    "                        (r'(\\w+)n\\'t', '\\g<1> not'),\\\n",
    "                        (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'),\\\n",
    "                        (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'),\\\n",
    "                        (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'),\\\n",
    "                        (r'wont', 'will not')]\n",
    "\n",
    "def replace_contraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \u001b[2mEXAMPLE WORKING OF TEXT CLEANING FUNCTION\u001b[0m\n",
      "                          \u001b[2m-------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[2mThis is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis is so cool. It is like, 'would you want your mother to read this??' Really great idea, well done!\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThank you!! This would make my life a lot less anxiety-inducing. Keep it up, and do not let anyone get in your way!\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mIs this something I'll be able to install on my site? When will you be releasing it?\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mIs this something I will be able to install on my site? When will you be releasing it?\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mIt was a great show. Not a combo I'd of expected to be good together but it was.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mIt was a great show. Not a combo I would of expected to be good together but it was.\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mIt's ridiculous that these guys are being called \"protesters\". Being armed is a threat of violence, which makes them terrorists.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mIt is ridiculous that these guys are being called \"protesters\". Being armed is a threat of violence, which makes them terrorists.\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "example_cleaning_results(replace_contraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the negations with antonyms\n",
    "This function rplaces negations with their respective antonyms.\n",
    "\n",
    "Eg. I am not happy. --> I am unhappy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(word, pos=None):\n",
    "    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n",
    "    antonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                antonyms.add(antonym.name())\n",
    "    if len(antonyms) == 1:\n",
    "        return antonyms.pop()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def replace_negations(text):\n",
    "    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n",
    "    i, l = 0, len(text)\n",
    "    words = []\n",
    "    while i < l:\n",
    "        word = text[i]\n",
    "        if word == 'not' and i+1 < l:\n",
    "            ant = replace(text[i+1])\n",
    "            if ant:\n",
    "                words.append(ant)\n",
    "                i += 2\n",
    "                continue\n",
    "        words.append(word)\n",
    "        i += 1\n",
    "    return words\n",
    "\n",
    "def tokenize_and_replace_negations(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = replace_negations(tokens)\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \u001b[2mEXAMPLE WORKING OF TEXT CLEANING FUNCTION\u001b[0m\n",
      "                          \u001b[2m-------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[2mThis is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis is so cool . It 's like , 'would you want your mother to read this ? ? ' Really great idea , well done !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThank you ! ! This would make my life a lot less anxiety-inducing . Keep it up , and do n't let anyone get in your way !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThis is such an urgent design problem; kudos to you for taking it on. Very impressive!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis is such an urgent design problem ; kudos to you for taking it on . Very impressive !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mIs this something I'll be able to install on my site? When will you be releasing it?\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mIs this something I 'll be able to install on my site ? When will you be releasing it ?\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mhaha you guys are a bunch of losers.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mhaha you guys are a bunch of losers .\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "example_cleaning_results(tokenize_and_replace_negations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "This function removes the most common words used in English (stop words) like 'a', 'is', 'are' etc.\n",
    "\n",
    "Eg. He is a very humorous person. --> He very humorous person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    finalTokens = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        if (w not in stoplist):\n",
    "            finalTokens.append(w)\n",
    "    text = \" \".join(finalTokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \u001b[2mEXAMPLE WORKING OF TEXT CLEANING FUNCTION\u001b[0m\n",
      "                          \u001b[2m-------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[2mThis is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis cool . It 's like , 'would want mother read ? ? ' Really great idea , well done !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThank ! ! This would make life lot less anxiety-inducing . Keep , n't let anyone get way !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThis is such an urgent design problem; kudos to you for taking it on. Very impressive!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis urgent design problem ; kudos taking . Very impressive !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mIs this something I'll be able to install on my site? When will you be releasing it?\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mIs something I 'll able install site ? When releasing ?\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mhaha you guys are a bunch of losers.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mhaha guys bunch losers .\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "example_cleaning_results(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace elongated words with the basic form\n",
    "This function replaces elongated words with its basic form.\n",
    "\n",
    "Eg. I eat little food --> I eat litle food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_elongated(word):\n",
    "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
    "\n",
    "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "    if repl_word != word:      \n",
    "        return replace_elongated(repl_word)\n",
    "    else:       \n",
    "        return repl_word\n",
    "    \n",
    "def replace_elongated_words(text):\n",
    "    finalTokens = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        finalTokens.append(replace_elongated(w))\n",
    "    text = \" \".join(finalTokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \u001b[2mEXAMPLE WORKING OF TEXT CLEANING FUNCTION\u001b[0m\n",
      "                          \u001b[2m-------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[2mThis is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis is so cool . It 's like , 'would you want your mother to read this ? ? ' Really great idea , well done !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThank you ! ! This would make my life a lot less anxiety-inducing . Keep it up , and do n't let anyone get in your way !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThis is such an urgent design problem; kudos to you for taking it on. Very impressive!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis is such an urgent design problem ; kudos to you for taking it on . Very impressive !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mIs this something I'll be able to install on my site? When will you be releasing it?\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mIs this something I 'l be able to install on my site ? When will you be releasing it ?\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mhaha you guys are a bunch of losers.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mhaha you guys are a bunch of losers .\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "example_cleaning_results(replace_elongated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem words\n",
    "This function \"stems\" the words in the comments. It only keeps the stem of the word, which need not be an actual word.\n",
    "\n",
    "Eg. I love swimming and driving happily --> I love swimm and driv happi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    finalTokens = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        finalTokens.append(stemmer.stem(w))\n",
    "    text = \" \".join(finalTokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \u001b[2mEXAMPLE WORKING OF TEXT CLEANING FUNCTION\u001b[0m\n",
      "                          \u001b[2m-------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[2mThis is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mthi is so cool . It 's like , 'would you want your mother to read thi ? ? ' realli great idea , well done !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mthank you ! ! thi would make my life a lot less anxiety-induc . keep it up , and do n't let anyon get in your way !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThis is such an urgent design problem; kudos to you for taking it on. Very impressive!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mthi is such an urgent design problem ; kudo to you for take it on . veri impress !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mIs this something I'll be able to install on my site? When will you be releasing it?\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mIs thi someth I 'll be abl to instal on my site ? when will you be releas it ?\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mhaha you guys are a bunch of losers.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mhaha you guy are a bunch of loser .\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "example_cleaning_results(stem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize words\n",
    "The function lemmatizes the words in the comments. It only keeps the lemma of the actual words, which needs to be an actual word.\n",
    "\n",
    "Eg. I love swimming and driving happily --> I love swim and drive happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    finalTokens = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        finalTokens.append(lemmatizer.lemmatize(w))\n",
    "    text = \" \".join(finalTokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \u001b[2mEXAMPLE WORKING OF TEXT CLEANING FUNCTION\u001b[0m\n",
      "                          \u001b[2m-------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[2mThis is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis is so cool . It 's like , 'would you want your mother to read this ? ? ' Really great idea , well done !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThank you ! ! This would make my life a lot le anxiety-inducing . Keep it up , and do n't let anyone get in your way !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mThis is such an urgent design problem; kudos to you for taking it on. Very impressive!\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mThis is such an urgent design problem ; kudos to you for taking it on . Very impressive !\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mIs this something I'll be able to install on my site? When will you be releasing it?\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mIs this something I 'll be able to install on my site ? When will you be releasing it ?\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[33m\u001b[2mhaha you guys are a bunch of losers.\u001b[0m\n",
      "\n",
      "                                     CHANGES TO\n",
      "\n",
      "\u001b[36m\u001b[2mhaha you guy are a bunch of loser .\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[2m---------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "example_cleaning_results(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it ! Thanks for reading my kernel and I hope you found it useful. Your upvote will be appreciated :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
