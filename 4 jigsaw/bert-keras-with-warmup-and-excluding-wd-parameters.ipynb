{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We have already seen many BERT model in raw tensorflow and pytorch. \n",
    "As keras is with user-friendly UI and easy to use, I want to find out whether there is BERT model in keras.\n",
    "Finally, https://github.com/CyberZHG/keras-bert is what I need. I have packed it into my database (https://www.kaggle.com/httpwwwfszyc/kerasbert).\n",
    "\n",
    "Here I just redo the thing similar to what taindow did in keras bert(https://www.kaggle.com/taindow/bert-a-fine-tuning-example)\n",
    "* No data prepocessing and ~~no warm up~~\n",
    "* We only use 1/100 of the training data as a demo\n",
    "* We use a maximum sequence length of 72\n",
    "\n",
    "Similarly, thanks for Jon Mischo (https://www.kaggle.com/supertaz) for uploading BERT Models + Scripts\n",
    "\n",
    "## Update on May 18\n",
    "\n",
    "Warmup optimizer is folked from the raw author https://github.com/CyberZHG/keras-bert. But I am not sure whether *layernormlayers* are also excluded in weight-decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** BERT pretrained directory: ../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12 *****\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import json\n",
    "sys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n",
    "!cp -r '../input/kerasbert/keras_bert' '/kaggle/working'\n",
    "BERT_PRETRAINED_DIR = '../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12'\n",
    "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
    "import tokenization  #Actually keras_bert contains tokenization part, here just for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "weight_decay = 0.001\n",
    "nb_epochs=1\n",
    "bsz = 32\n",
    "maxlen=72\n",
    "## Training data\n",
    "train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "train_df = train_df.sample(frac=0.001,random_state = 42)\n",
    "#train_df['comment_text'] = train_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "train_lines, train_labels = train_df['comment_text'].values, train_df.target.values \n",
    "## step parameter \n",
    "decay_steps = int(nb_epochs*train_lines.shape[0]/bsz)\n",
    "warmup_steps = int(0.1*decay_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin_build\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras_bert.keras_bert.bert import get_model\n",
    "from keras_bert.keras_bert.loader import load_trained_model_from_checkpoint\n",
    "print('begin_build')\n",
    "\n",
    "config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "model = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True,seq_len=maxlen)\n",
    "#model.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classification model with adamwarmup\n",
    "\n",
    "First folk the optimizer with excluding \"bias\" and \"Norm\" parameters from weight decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "class AdamWarmup(keras.optimizers.Optimizer):\n",
    "    def __init__(self, decay_steps, warmup_steps, min_lr=0.0,\n",
    "                 lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, kernel_weight_decay=0., bias_weight_decay=0.,\n",
    "                 amsgrad=False, **kwargs):\n",
    "        super(AdamWarmup, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.decay_steps = K.variable(decay_steps, name='decay_steps')\n",
    "            self.warmup_steps = K.variable(warmup_steps, name='warmup_steps')\n",
    "            self.min_lr = K.variable(min_lr, name='min_lr')\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.kernel_weight_decay = K.variable(kernel_weight_decay, name='kernel_weight_decay')\n",
    "            self.bias_weight_decay = K.variable(bias_weight_decay, name='bias_weight_decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_kernel_weight_decay = kernel_weight_decay\n",
    "        self.initial_bias_weight_decay = bias_weight_decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        lr = K.switch(\n",
    "            t <= self.warmup_steps,\n",
    "            self.lr * (t / self.warmup_steps),\n",
    "            self.lr * (1.0 - K.minimum(t, self.decay_steps) / self.decay_steps),\n",
    "        )\n",
    "\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            if 'bias' in p.name or 'Norm' in p.name:\n",
    "                if self.initial_bias_weight_decay > 0.0:\n",
    "                    p_t += self.bias_weight_decay * p\n",
    "            else:\n",
    "                if self.initial_kernel_weight_decay > 0.0:\n",
    "                    p_t += self.kernel_weight_decay * p\n",
    "            p_t = p - lr_t * p_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'decay_steps': float(K.get_value(self.decay_steps)),\n",
    "            'warmup_steps': float(K.get_value(self.warmup_steps)),\n",
    "            'min_lr': float(K.get_value(self.min_lr)),\n",
    "            'lr': float(K.get_value(self.lr)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'kernel_weight_decay': float(K.get_value(self.kernel_weight_decay)),\n",
    "            'bias_weight_decay': float(K.get_value(self.bias_weight_decay)),\n",
    "            'amsgrad': self.amsgrad,\n",
    "        }\n",
    "        base_config = super(AdamWarmup, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Extract layer extracts only the first token where \"['CLS']\" used to be, we just take the layer and connect to the single neuron output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 72, 768), (3 23440896    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 72, 768)      1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 72, 768)      0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 72, 768)      55296       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 72, 768)      0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 72, 768)      1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 72, 768)      2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 72, 768)      0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 72, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 72, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 72, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 72, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 72, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 72, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 72, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 72, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 72, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 72, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 72, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 72, 768)      0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 72, 768)      0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 72, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 72, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 72, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 72, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 72, 768)      1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 72, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 72, 768)      0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 72, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 72, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 72, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 72, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 72, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 72, 768)      1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 72, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 72, 768)      0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 72, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 72, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 72, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 72, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 72, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 72, 768)      1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 72, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 72, 768)      0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 72, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 72, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "real_output (Dense)             (None, 1)            769         Extract[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 108,554,497\n",
      "Trainable params: 108,554,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import re\n",
    "import codecs\n",
    "adamwarm = AdamWarmup(lr=lr,decay_steps = decay_steps, warmup_steps = warmup_steps,kernel_weight_decay = weight_decay)\n",
    "sequence_output  = model.layers[-6].output\n",
    "pool_output = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_output)\n",
    "model3  = Model(inputs=model.input, outputs=pool_output)\n",
    "model3.compile(loss='binary_crossentropy', optimizer=adamwarm)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding-Token/embeddings:0 (30522, 768)\n",
      "Embedding-Segment/embeddings:0 (2, 768)\n",
      "Embedding-Position/embeddings:0 (72, 768)\n",
      "Embedding-Norm/gamma:0 (768,)\n",
      "Embedding-Norm/beta:0 (768,)\n",
      "Encoder-1-MultiHeadSelfAttention/Encoder-1-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-1-MultiHeadSelfAttention/Encoder-1-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-1-MultiHeadSelfAttention/Encoder-1-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-1-MultiHeadSelfAttention/Encoder-1-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-1-MultiHeadSelfAttention/Encoder-1-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-1-MultiHeadSelfAttention/Encoder-1-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-1-MultiHeadSelfAttention/Encoder-1-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-1-MultiHeadSelfAttention/Encoder-1-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-1-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-1-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-1-FeedForward/Encoder-1-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-1-FeedForward/Encoder-1-FeedForward_b1:0 (3072,)\n",
      "Encoder-1-FeedForward/Encoder-1-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-1-FeedForward/Encoder-1-FeedForward_b2:0 (768,)\n",
      "Encoder-1-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-1-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-2-MultiHeadSelfAttention/Encoder-2-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-2-MultiHeadSelfAttention/Encoder-2-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-2-MultiHeadSelfAttention/Encoder-2-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-2-MultiHeadSelfAttention/Encoder-2-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-2-MultiHeadSelfAttention/Encoder-2-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-2-MultiHeadSelfAttention/Encoder-2-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-2-MultiHeadSelfAttention/Encoder-2-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-2-MultiHeadSelfAttention/Encoder-2-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-2-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-2-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-2-FeedForward/Encoder-2-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-2-FeedForward/Encoder-2-FeedForward_b1:0 (3072,)\n",
      "Encoder-2-FeedForward/Encoder-2-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-2-FeedForward/Encoder-2-FeedForward_b2:0 (768,)\n",
      "Encoder-2-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-2-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-3-MultiHeadSelfAttention/Encoder-3-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-3-MultiHeadSelfAttention/Encoder-3-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-3-MultiHeadSelfAttention/Encoder-3-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-3-MultiHeadSelfAttention/Encoder-3-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-3-MultiHeadSelfAttention/Encoder-3-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-3-MultiHeadSelfAttention/Encoder-3-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-3-MultiHeadSelfAttention/Encoder-3-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-3-MultiHeadSelfAttention/Encoder-3-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-3-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-3-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-3-FeedForward/Encoder-3-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-3-FeedForward/Encoder-3-FeedForward_b1:0 (3072,)\n",
      "Encoder-3-FeedForward/Encoder-3-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-3-FeedForward/Encoder-3-FeedForward_b2:0 (768,)\n",
      "Encoder-3-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-3-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-4-MultiHeadSelfAttention/Encoder-4-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-4-MultiHeadSelfAttention/Encoder-4-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-4-MultiHeadSelfAttention/Encoder-4-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-4-MultiHeadSelfAttention/Encoder-4-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-4-MultiHeadSelfAttention/Encoder-4-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-4-MultiHeadSelfAttention/Encoder-4-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-4-MultiHeadSelfAttention/Encoder-4-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-4-MultiHeadSelfAttention/Encoder-4-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-4-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-4-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-4-FeedForward/Encoder-4-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-4-FeedForward/Encoder-4-FeedForward_b1:0 (3072,)\n",
      "Encoder-4-FeedForward/Encoder-4-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-4-FeedForward/Encoder-4-FeedForward_b2:0 (768,)\n",
      "Encoder-4-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-4-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-5-MultiHeadSelfAttention/Encoder-5-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-5-MultiHeadSelfAttention/Encoder-5-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-5-MultiHeadSelfAttention/Encoder-5-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-5-MultiHeadSelfAttention/Encoder-5-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-5-MultiHeadSelfAttention/Encoder-5-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-5-MultiHeadSelfAttention/Encoder-5-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-5-MultiHeadSelfAttention/Encoder-5-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-5-MultiHeadSelfAttention/Encoder-5-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-5-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-5-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-5-FeedForward/Encoder-5-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-5-FeedForward/Encoder-5-FeedForward_b1:0 (3072,)\n",
      "Encoder-5-FeedForward/Encoder-5-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-5-FeedForward/Encoder-5-FeedForward_b2:0 (768,)\n",
      "Encoder-5-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-5-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-6-MultiHeadSelfAttention/Encoder-6-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-6-MultiHeadSelfAttention/Encoder-6-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-6-MultiHeadSelfAttention/Encoder-6-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-6-MultiHeadSelfAttention/Encoder-6-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-6-MultiHeadSelfAttention/Encoder-6-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-6-MultiHeadSelfAttention/Encoder-6-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-6-MultiHeadSelfAttention/Encoder-6-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-6-MultiHeadSelfAttention/Encoder-6-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-6-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-6-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-6-FeedForward/Encoder-6-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-6-FeedForward/Encoder-6-FeedForward_b1:0 (3072,)\n",
      "Encoder-6-FeedForward/Encoder-6-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-6-FeedForward/Encoder-6-FeedForward_b2:0 (768,)\n",
      "Encoder-6-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-6-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-7-MultiHeadSelfAttention/Encoder-7-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-7-MultiHeadSelfAttention/Encoder-7-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-7-MultiHeadSelfAttention/Encoder-7-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-7-MultiHeadSelfAttention/Encoder-7-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-7-MultiHeadSelfAttention/Encoder-7-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-7-MultiHeadSelfAttention/Encoder-7-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-7-MultiHeadSelfAttention/Encoder-7-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-7-MultiHeadSelfAttention/Encoder-7-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-7-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-7-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-7-FeedForward/Encoder-7-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-7-FeedForward/Encoder-7-FeedForward_b1:0 (3072,)\n",
      "Encoder-7-FeedForward/Encoder-7-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-7-FeedForward/Encoder-7-FeedForward_b2:0 (768,)\n",
      "Encoder-7-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-7-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-8-MultiHeadSelfAttention/Encoder-8-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-8-MultiHeadSelfAttention/Encoder-8-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-8-MultiHeadSelfAttention/Encoder-8-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-8-MultiHeadSelfAttention/Encoder-8-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-8-MultiHeadSelfAttention/Encoder-8-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-8-MultiHeadSelfAttention/Encoder-8-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-8-MultiHeadSelfAttention/Encoder-8-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-8-MultiHeadSelfAttention/Encoder-8-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-8-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-8-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-8-FeedForward/Encoder-8-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-8-FeedForward/Encoder-8-FeedForward_b1:0 (3072,)\n",
      "Encoder-8-FeedForward/Encoder-8-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-8-FeedForward/Encoder-8-FeedForward_b2:0 (768,)\n",
      "Encoder-8-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-8-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-9-MultiHeadSelfAttention/Encoder-9-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-9-MultiHeadSelfAttention/Encoder-9-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-9-MultiHeadSelfAttention/Encoder-9-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-9-MultiHeadSelfAttention/Encoder-9-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-9-MultiHeadSelfAttention/Encoder-9-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-9-MultiHeadSelfAttention/Encoder-9-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-9-MultiHeadSelfAttention/Encoder-9-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-9-MultiHeadSelfAttention/Encoder-9-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-9-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-9-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-9-FeedForward/Encoder-9-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-9-FeedForward/Encoder-9-FeedForward_b1:0 (3072,)\n",
      "Encoder-9-FeedForward/Encoder-9-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-9-FeedForward/Encoder-9-FeedForward_b2:0 (768,)\n",
      "Encoder-9-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-9-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-10-MultiHeadSelfAttention/Encoder-10-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-10-MultiHeadSelfAttention/Encoder-10-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-10-MultiHeadSelfAttention/Encoder-10-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-10-MultiHeadSelfAttention/Encoder-10-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-10-MultiHeadSelfAttention/Encoder-10-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-10-MultiHeadSelfAttention/Encoder-10-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-10-MultiHeadSelfAttention/Encoder-10-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-10-MultiHeadSelfAttention/Encoder-10-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-10-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-10-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-10-FeedForward/Encoder-10-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-10-FeedForward/Encoder-10-FeedForward_b1:0 (3072,)\n",
      "Encoder-10-FeedForward/Encoder-10-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-10-FeedForward/Encoder-10-FeedForward_b2:0 (768,)\n",
      "Encoder-10-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-10-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-11-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-11-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-11-FeedForward/Encoder-11-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-11-FeedForward/Encoder-11-FeedForward_b1:0 (3072,)\n",
      "Encoder-11-FeedForward/Encoder-11-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-11-FeedForward/Encoder-11-FeedForward_b2:0 (768,)\n",
      "Encoder-11-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-11-FeedForward-Norm/beta:0 (768,)\n",
      "Encoder-12-MultiHeadSelfAttention/Encoder-12-MultiHeadSelfAttention_Wq:0 (768, 768)\n",
      "Encoder-12-MultiHeadSelfAttention/Encoder-12-MultiHeadSelfAttention_bq:0 (768,)\n",
      "Encoder-12-MultiHeadSelfAttention/Encoder-12-MultiHeadSelfAttention_Wk:0 (768, 768)\n",
      "Encoder-12-MultiHeadSelfAttention/Encoder-12-MultiHeadSelfAttention_bk:0 (768,)\n",
      "Encoder-12-MultiHeadSelfAttention/Encoder-12-MultiHeadSelfAttention_Wv:0 (768, 768)\n",
      "Encoder-12-MultiHeadSelfAttention/Encoder-12-MultiHeadSelfAttention_bv:0 (768,)\n",
      "Encoder-12-MultiHeadSelfAttention/Encoder-12-MultiHeadSelfAttention_Wo:0 (768, 768)\n",
      "Encoder-12-MultiHeadSelfAttention/Encoder-12-MultiHeadSelfAttention_bo:0 (768,)\n",
      "Encoder-12-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Encoder-12-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Encoder-12-FeedForward/Encoder-12-FeedForward_W1:0 (768, 3072)\n",
      "Encoder-12-FeedForward/Encoder-12-FeedForward_b1:0 (3072,)\n",
      "Encoder-12-FeedForward/Encoder-12-FeedForward_W2:0 (3072, 768)\n",
      "Encoder-12-FeedForward/Encoder-12-FeedForward_b2:0 (768,)\n",
      "Encoder-12-FeedForward-Norm/gamma:0 (768,)\n",
      "Encoder-12-FeedForward-Norm/beta:0 (768,)\n",
      "real_output/kernel:0 (768, 1)\n",
      "real_output/bias:0 (1,)\n"
     ]
    }
   ],
   "source": [
    "names = [weight.name for layer in model3.layers for weight in layer.weights]\n",
    "weights = model3.get_weights()\n",
    "\n",
    "for name, weight in zip(names, weights):\n",
    "    print(name, weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data, Training, Predicting\n",
    "\n",
    "First the model need train data like [token_input,seg_input,masked input], here we set all segment input to 0 and all masked input to 1.\n",
    "\n",
    "Still I am finding a more efficient way to do token-convert-to-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build tokenizer done\n",
      "sample used (1805,)\n",
      "611\n",
      "(1805, 72)\n",
      "(1805, 72)\n",
      "(1805, 72)\n",
      "begin training\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      "1805/1805 [==============================] - 34s 19ms/step - loss: 0.3541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f498ee36eb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for i in range(example.shape[0]):\n",
    "      tokens_a = tokenizer.tokenize(example[i])\n",
    "      if len(tokens_a)>max_seq_length:\n",
    "        tokens_a = tokens_a[:max_seq_length]\n",
    "        longer += 1\n",
    "      one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "      all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)\n",
    "    \n",
    "\n",
    "dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\n",
    "print('build tokenizer done')\n",
    "print('sample used',train_lines.shape)\n",
    "token_input = convert_lines(train_lines,maxlen,tokenizer)\n",
    "seg_input = np.zeros((token_input.shape[0],maxlen))\n",
    "mask_input = np.ones((token_input.shape[0],maxlen))\n",
    "print(token_input.shape)\n",
    "print(seg_input.shape)\n",
    "print(mask_input.shape)\n",
    "print('begin training')\n",
    "model3.fit([token_input, seg_input, mask_input],train_labels,batch_size=bsz,epochs=nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97320,)\n",
      "load data done\n",
      "33518\n",
      "test data done\n",
      "(97320, 72)\n",
      "(97320, 72)\n",
      "(97320, 72)\n",
      "97320/97320 [==============================] - 250s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#load test data\n",
    "test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "#test_df['comment_text'] = test_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "eval_lines = test_df['comment_text'].values\n",
    "print(eval_lines.shape)\n",
    "print('load data done')\n",
    "token_input2 = convert_lines(eval_lines,maxlen,tokenizer)\n",
    "seg_input2 = np.zeros((token_input2.shape[0],maxlen))\n",
    "mask_input2 = np.ones((token_input2.shape[0],maxlen))\n",
    "print('test data done')\n",
    "print(token_input2.shape)\n",
    "print(seg_input2.shape)\n",
    "print(mask_input2.shape)\n",
    "hehe = model3.predict([token_input2, seg_input2, mask_input2],verbose=1,batch_size=bsz)\n",
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "submission['prediction'] = hehe\n",
    "submission.reset_index(drop=False, inplace=True)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
