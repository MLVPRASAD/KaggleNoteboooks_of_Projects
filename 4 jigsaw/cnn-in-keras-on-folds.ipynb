{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information\n",
    "This is a basic kernel with CNN. In this kernel I train a CNN model on folds and calculate the competition metric (not simple auc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "* [1 Loading and processing data](#load)\n",
    "* [2 Validation functions](#valid)\n",
    "* [3 Model](#model)\n",
    "* [4 Training function](#train)\n",
    "* [5 Train and predict](#run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jigsaw-public-files', 'glove840b300dtxt', 'fasttext-crawl-300d-2m', 'jigsaw-unintended-bias-in-toxicity-classification']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "pd.set_option('max_colwidth',400)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "\n",
    "import time\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## Loading data\n",
    "\n",
    "I'll load preprocessed data from my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-public-files/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-public-files/test.csv')\n",
    "# after processing some of the texts are emply\n",
    "train['comment_text'] = train['comment_text'].fillna('')\n",
    "test['comment_text'] = test['comment_text'].fillna('')\n",
    "sub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "full_text = list(train['comment_text'].values) + list(test['comment_text'].values)\n",
    "max_features = 300000\n",
    "tk = Tokenizer(lower = True, filters='', num_words=max_features)\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path1 = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "embedding_path2 = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def build_matrix(embedding_path, tokenizer):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "    word_index = tk.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# combining embeddings from this kernel: https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution\n",
    "embedding_matrix = np.concatenate([build_matrix(embedding_path1, tk), build_matrix(embedding_path2, tk)], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(train['target'] >= 0.5, True, False) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "for col in identity_columns + ['target']:\n",
    "    train[col] = np.where(train[col] >= 0.5, True, False)\n",
    "\n",
    "n_fold = 5\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"valid\"></a>\n",
    "## Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, oof_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[oof_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, oof_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[oof_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, oof_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[oof_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "def calculate_overall_auc(df, oof_name):\n",
    "    true_labels = df['target']\n",
    "    predicted_labels = df[oof_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "# adding attention from this kernel: https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "\n",
    "def build_model(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0,\n",
    "                dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size * 2, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "    att = Attention(max_len)(x1)\n",
    "    # from benchmark kernel\n",
    "    x = Conv1D(conv_size, 2, activation='relu', padding='same')(x1)\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "    x = Conv1D(conv_size, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = concatenate([x, att])\n",
    "    \n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=(X_valid, y_valid), \n",
    "                        verbose=2, callbacks=[early_stop, check_point])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## Training fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, tokenizer, max_len):\n",
    "    \n",
    "    oof = np.zeros((len(X), 1))\n",
    "    prediction = np.zeros((len(X_test), 1))\n",
    "    scores = []\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test['comment_text'])\n",
    "    X_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        valid_df = X_valid.copy()    \n",
    "\n",
    "        train_tokenized = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "        valid_tokenized = tokenizer.texts_to_sequences(X_valid['comment_text'])\n",
    "\n",
    "        X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "        X_valid = pad_sequences(valid_tokenized, maxlen = max_len)\n",
    "        \n",
    "        model = build_model(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix,\n",
    "                            lr = 1e-3, lr_d = 0, spatial_dr = 0.1, dense_units=128, conv_size=128, dr=0.1, patience=3, fold_id=fold_n)\n",
    "        \n",
    "        pred_valid = model.predict(X_valid)\n",
    "        oof[valid_index] = pred_valid\n",
    "        valid_df[oof_name] = pred_valid\n",
    "        \n",
    "        bias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, oof_name, 'target')\n",
    "        scores.append(get_final_metric(bias_metrics_df, calculate_overall_auc(valid_df, oof_name)))\n",
    "        \n",
    "        prediction += model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "    \n",
    "    prediction /= n_fold\n",
    "    \n",
    "    # print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"run\"></a>\n",
    "## Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun May 12 17:03:22 2019\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/3\n",
      " - 167s - loss: 0.1348 - acc: 0.9477 - val_loss: 0.1245 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12447, saving model to best_model_fold_0.hdf5\n",
      "Epoch 2/3\n",
      " - 165s - loss: 0.1233 - acc: 0.9511 - val_loss: 0.1201 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12447 to 0.12005, saving model to best_model_fold_0.hdf5\n",
      "Epoch 3/3\n",
      " - 165s - loss: 0.1192 - acc: 0.9523 - val_loss: 0.1196 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12005 to 0.11957, saving model to best_model_fold_0.hdf5\n",
      "97320/97320 [==============================] - 3s 33us/step\n",
      "Fold 1 started at Sun May 12 17:13:53 2019\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/3\n",
      " - 167s - loss: 0.1354 - acc: 0.9475 - val_loss: 0.1241 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12411, saving model to best_model_fold_1.hdf5\n",
      "Epoch 2/3\n",
      " - 166s - loss: 0.1235 - acc: 0.9510 - val_loss: 0.1206 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12411 to 0.12059, saving model to best_model_fold_1.hdf5\n",
      "Epoch 3/3\n",
      " - 166s - loss: 0.1194 - acc: 0.9522 - val_loss: 0.1196 - val_acc: 0.9520\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12059 to 0.11958, saving model to best_model_fold_1.hdf5\n",
      "97320/97320 [==============================] - 3s 32us/step\n",
      "Fold 2 started at Sun May 12 17:24:24 2019\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/3\n",
      " - 167s - loss: 0.1351 - acc: 0.9475 - val_loss: 0.1241 - val_acc: 0.9505\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12411, saving model to best_model_fold_2.hdf5\n",
      "Epoch 2/3\n",
      " - 166s - loss: 0.1231 - acc: 0.9512 - val_loss: 0.1226 - val_acc: 0.9515\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12411 to 0.12259, saving model to best_model_fold_2.hdf5\n",
      "Epoch 3/3\n",
      " - 167s - loss: 0.1192 - acc: 0.9522 - val_loss: 0.1217 - val_acc: 0.9517\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12259 to 0.12168, saving model to best_model_fold_2.hdf5\n",
      "97320/97320 [==============================] - 3s 33us/step\n",
      "Fold 3 started at Sun May 12 17:34:58 2019\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/3\n",
      " - 168s - loss: 0.1350 - acc: 0.9473 - val_loss: 0.1251 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12510, saving model to best_model_fold_3.hdf5\n",
      "Epoch 2/3\n",
      " - 166s - loss: 0.1232 - acc: 0.9510 - val_loss: 0.1238 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12510 to 0.12383, saving model to best_model_fold_3.hdf5\n",
      "Epoch 3/3\n",
      " - 167s - loss: 0.1193 - acc: 0.9522 - val_loss: 0.1208 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12383 to 0.12083, saving model to best_model_fold_3.hdf5\n",
      "97320/97320 [==============================] - 3s 33us/step\n",
      "Fold 4 started at Sun May 12 17:45:34 2019\n",
      "Train on 1443900 samples, validate on 360974 samples\n",
      "Epoch 1/3\n",
      " - 168s - loss: 0.1351 - acc: 0.9474 - val_loss: 0.1251 - val_acc: 0.9507\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12509, saving model to best_model_fold_4.hdf5\n",
      "Epoch 2/3\n",
      " - 167s - loss: 0.1233 - acc: 0.9510 - val_loss: 0.1206 - val_acc: 0.9521\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12509 to 0.12063, saving model to best_model_fold_4.hdf5\n",
      "Epoch 3/3\n",
      " - 167s - loss: 0.1193 - acc: 0.9521 - val_loss: 0.1196 - val_acc: 0.9523\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12063 to 0.11965, saving model to best_model_fold_4.hdf5\n",
      "97320/97320 [==============================] - 3s 33us/step\n",
      "CV mean score: 0.9182, std: 0.0018.\n"
     ]
    }
   ],
   "source": [
    "oof_name = 'predicted_target'\n",
    "max_len = 250\n",
    "oof, prediction, scores = train_model(X=train, X_test=test, y=train['target'], tokenizer=tk, max_len=max_len)\n",
    "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(prediction);\n",
    "plt.hist(oof);\n",
    "plt.title('Distribution of predictions vs oof predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['prediction'] = prediction\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
