{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A loss function for the Jigsaw Unintended Bias in Toxicity Classification competition \n",
    "### (Public LB rank = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)       \n",
    "    \n",
    "def convert_dataframe_to_bool(df, columns):        \n",
    "    bool_df = df.copy()\n",
    "    for col in columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "aux_target_columns = ['sexual_explicit', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "num_identity = len(identity_columns)\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "train.fillna(0, inplace = True)\n",
    "train = convert_dataframe_to_bool(train, ['target'] + identity_columns)\n",
    "\n",
    "\n",
    "weights = np.ones((len(train),))\n",
    "\n",
    "# Positive and negative examples get balanced weights in each part\n",
    "\n",
    "# These samples participate in the over all AUC term\n",
    "weights[train['target']]   =  1 / train['target'].sum()                \n",
    "weights[~train['target']]   =  1 / (~train['target']).sum()\n",
    "for col in identity_columns:\n",
    "    hasIdentity = train[col]\n",
    "    # These samples participate in the subgroup AUC and BNSP terms    \n",
    "    weights[hasIdentity & train['target']]   +=  2 / (( hasIdentity &  train['target']).sum() * num_identity)\n",
    "    # These samples participate in the subgroup AUC and BPSN terms\n",
    "    weights[hasIdentity & ~train['target']]  +=  2 / (( hasIdentity & ~train['target']).sum() * num_identity)\n",
    "    # These samples participate in the BPSN term\n",
    "    weights[~hasIdentity & train['target']]  +=  1 / ((~hasIdentity &  train['target']).sum() * num_identity)\n",
    "    # These samples participate in the BNSP term\n",
    "    weights[~hasIdentity & ~train['target']] +=  1 / ((~hasIdentity & ~train['target']).sum() * num_identity)\n",
    "    \n",
    "    \n",
    "    \n",
    "weights = weights / weights.max()\n",
    "\n",
    "\n",
    "y_train = train['target'].values\n",
    "y_aux_train = train[aux_target_columns].values\n",
    "y_combined =  np.concatenate((y_train.reshape((-1, 1)), weights.reshape((-1, 1)), y_aux_train.reshape((-1, len(aux_target_columns)))), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loss weights, equal weights (both = 1) gave me the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(data, targets):    \n",
    "    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "    return (bce_loss_1 * 1) + (bce_loss_2 * 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
