{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Who needs models, just do statstics\n",
    "First of all, 95% of the kernel is stolen from Chris, I just exchanged QDA with heuristic statistics. The main point is, that if we assume our variables to be multivariate normal distributed (which we know from the make_classification function) we can just calcuate the probability that a data point belongs to either of the two ellipses by calculating probablities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Intro\n",
    "Luckily Chris already put some illustrative pictures in his kernel, I can steal.\n",
    "Here is a pictorial explanation using sythetic 2D data. \n",
    "  \n",
    "## Step 1 - Build first model\n",
    "Given 50 training observations (25 target=1 yellow points, 25 target=0 blue points) we can estimate the multivariate (approx 40 dimensions) normal distributions of each of the two target types (0 & 1) by calculating empiral covariance and mean (see np.cov and np.mean) and then calculate that a given datapoint belongs to distribution A or B using scipy.stats.multivariate.\n",
    "\n",
    "\n",
    "![image](http://playagricola.com/Kaggle/p16419.png)\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html#scipy.stats.multivariate_normal\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Statistics to Instant Gratification Comp\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>muggy-smalt-axolotl-pembus</th>\n",
       "      <th>dorky-peach-sheepdog-ordinal</th>\n",
       "      <th>slimy-seashell-cassowary-goose</th>\n",
       "      <th>snazzy-harlequin-chicken-distraction</th>\n",
       "      <th>frumpy-smalt-mau-ordinal</th>\n",
       "      <th>stealthy-beige-pinscher-golden</th>\n",
       "      <th>chummy-cream-tarantula-entropy</th>\n",
       "      <th>hazy-emerald-cuttlefish-unsorted</th>\n",
       "      <th>nerdy-indigo-wolfhound-sorted</th>\n",
       "      <th>leaky-amaranth-lizard-sorted</th>\n",
       "      <th>ugly-tangerine-chihuahua-important</th>\n",
       "      <th>shaggy-silver-indri-fimbus</th>\n",
       "      <th>flaky-chocolate-beetle-grandmaster</th>\n",
       "      <th>squirrely-harlequin-sheep-sumble</th>\n",
       "      <th>freaky-tan-angelfish-noise</th>\n",
       "      <th>lousy-plum-penguin-sumble</th>\n",
       "      <th>bluesy-rose-wallaby-discard</th>\n",
       "      <th>baggy-copper-oriole-dummy</th>\n",
       "      <th>stealthy-scarlet-hound-fepid</th>\n",
       "      <th>greasy-cinnamon-bonobo-contributor</th>\n",
       "      <th>cranky-cardinal-dogfish-ordinal</th>\n",
       "      <th>snippy-auburn-vole-learn</th>\n",
       "      <th>greasy-sepia-coral-dataset</th>\n",
       "      <th>flabby-tangerine-fowl-entropy</th>\n",
       "      <th>lousy-smalt-pinscher-dummy</th>\n",
       "      <th>bluesy-brass-chihuahua-distraction</th>\n",
       "      <th>goopy-eggplant-indri-entropy</th>\n",
       "      <th>homey-sepia-bombay-sorted</th>\n",
       "      <th>homely-ruby-bulldog-entropy</th>\n",
       "      <th>hasty-blue-sheep-contributor</th>\n",
       "      <th>blurry-wisteria-oyster-master</th>\n",
       "      <th>snoopy-auburn-dogfish-expert</th>\n",
       "      <th>stinky-maroon-blue-kernel</th>\n",
       "      <th>bumpy-amaranth-armadillo-important</th>\n",
       "      <th>slaphappy-peach-oyster-master</th>\n",
       "      <th>dorky-tomato-ragdoll-dataset</th>\n",
       "      <th>messy-mauve-wolverine-ordinal</th>\n",
       "      <th>geeky-pumpkin-moorhen-important</th>\n",
       "      <th>crabby-teal-otter-unsorted</th>\n",
       "      <th>...</th>\n",
       "      <th>beady-mauve-frog-distraction</th>\n",
       "      <th>surly-brass-maltese-ordinal</th>\n",
       "      <th>beady-asparagus-opossum-expert</th>\n",
       "      <th>beady-rust-impala-dummy</th>\n",
       "      <th>droopy-amethyst-dachshund-hint</th>\n",
       "      <th>homey-crimson-budgerigar-grandmaster</th>\n",
       "      <th>droopy-cardinal-impala-important</th>\n",
       "      <th>woozy-apricot-moose-hint</th>\n",
       "      <th>paltry-sapphire-labradoodle-dummy</th>\n",
       "      <th>crappy-carmine-eagle-entropy</th>\n",
       "      <th>greasy-magnolia-spider-grandmaster</th>\n",
       "      <th>crabby-carmine-flounder-sorted</th>\n",
       "      <th>skimpy-copper-fowl-grandmaster</th>\n",
       "      <th>hasty-seashell-woodpecker-hint</th>\n",
       "      <th>snappy-purple-bobcat-important</th>\n",
       "      <th>thirsty-carmine-corgi-ordinal</th>\n",
       "      <th>homely-auburn-reindeer-unsorted</th>\n",
       "      <th>crappy-beige-tiger-fepid</th>\n",
       "      <th>cranky-auburn-swan-novice</th>\n",
       "      <th>chewy-bistre-buzzard-expert</th>\n",
       "      <th>skinny-cyan-macaque-pembus</th>\n",
       "      <th>slimy-periwinkle-otter-expert</th>\n",
       "      <th>snazzy-burgundy-clam-novice</th>\n",
       "      <th>cozy-ochre-gorilla-gaussian</th>\n",
       "      <th>homey-sangria-wolfhound-dummy</th>\n",
       "      <th>snazzy-asparagus-hippopotamus-contributor</th>\n",
       "      <th>paltry-red-hamster-sorted</th>\n",
       "      <th>zippy-dandelion-insect-golden</th>\n",
       "      <th>baggy-coral-bandicoot-unsorted</th>\n",
       "      <th>goopy-lavender-wolverine-fimbus</th>\n",
       "      <th>wheezy-myrtle-mandrill-entropy</th>\n",
       "      <th>wiggy-lilac-lemming-sorted</th>\n",
       "      <th>gloppy-cerise-snail-contributor</th>\n",
       "      <th>woozy-silver-havanese-gaussian</th>\n",
       "      <th>jumpy-thistle-discus-sorted</th>\n",
       "      <th>muggy-turquoise-donkey-important</th>\n",
       "      <th>blurry-buff-hyena-entropy</th>\n",
       "      <th>bluesy-chocolate-kudu-fepid</th>\n",
       "      <th>gamy-white-monster-expert</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>707b395ecdcbb4dc2eabea00e4d1b179</td>\n",
       "      <td>-2.070654</td>\n",
       "      <td>1.018160</td>\n",
       "      <td>0.228643</td>\n",
       "      <td>0.857221</td>\n",
       "      <td>0.052271</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>-6.385090</td>\n",
       "      <td>0.439369</td>\n",
       "      <td>-0.721946</td>\n",
       "      <td>-0.227027</td>\n",
       "      <td>0.575964</td>\n",
       "      <td>1.541908</td>\n",
       "      <td>1.745286</td>\n",
       "      <td>-0.624271</td>\n",
       "      <td>3.600958</td>\n",
       "      <td>1.176489</td>\n",
       "      <td>-0.182776</td>\n",
       "      <td>-0.228391</td>\n",
       "      <td>1.682263</td>\n",
       "      <td>-0.833236</td>\n",
       "      <td>-4.377688</td>\n",
       "      <td>-5.372410</td>\n",
       "      <td>-0.477742</td>\n",
       "      <td>-0.179005</td>\n",
       "      <td>-0.516475</td>\n",
       "      <td>0.127391</td>\n",
       "      <td>-0.857591</td>\n",
       "      <td>-0.461500</td>\n",
       "      <td>2.160303</td>\n",
       "      <td>-2.118371</td>\n",
       "      <td>0.515493</td>\n",
       "      <td>-1.201493</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>-1.154024</td>\n",
       "      <td>0.753204</td>\n",
       "      <td>-0.179651</td>\n",
       "      <td>-0.807341</td>\n",
       "      <td>-1.663626</td>\n",
       "      <td>0.893806</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.829848</td>\n",
       "      <td>2.347131</td>\n",
       "      <td>0.082462</td>\n",
       "      <td>-1.012654</td>\n",
       "      <td>0.593752</td>\n",
       "      <td>2.904654</td>\n",
       "      <td>-0.428974</td>\n",
       "      <td>-0.919979</td>\n",
       "      <td>2.849575</td>\n",
       "      <td>-0.906744</td>\n",
       "      <td>0.729459</td>\n",
       "      <td>0.386140</td>\n",
       "      <td>0.319814</td>\n",
       "      <td>-0.407682</td>\n",
       "      <td>-0.170667</td>\n",
       "      <td>-1.242919</td>\n",
       "      <td>-1.719046</td>\n",
       "      <td>-0.132395</td>\n",
       "      <td>-0.368991</td>\n",
       "      <td>-5.112553</td>\n",
       "      <td>-2.085988</td>\n",
       "      <td>-0.897257</td>\n",
       "      <td>1.080671</td>\n",
       "      <td>-0.273262</td>\n",
       "      <td>0.342824</td>\n",
       "      <td>0.640177</td>\n",
       "      <td>-0.415298</td>\n",
       "      <td>-0.483126</td>\n",
       "      <td>-0.080799</td>\n",
       "      <td>2.416224</td>\n",
       "      <td>0.351895</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>-1.542423</td>\n",
       "      <td>0.598175</td>\n",
       "      <td>0.611757</td>\n",
       "      <td>0.678772</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>-0.806677</td>\n",
       "      <td>-0.193649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5880c03c6582a7b42248668e56b4bdec</td>\n",
       "      <td>-0.491702</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>-0.011193</td>\n",
       "      <td>1.071266</td>\n",
       "      <td>-0.346347</td>\n",
       "      <td>-0.082209</td>\n",
       "      <td>0.110579</td>\n",
       "      <td>-0.382374</td>\n",
       "      <td>-0.229620</td>\n",
       "      <td>0.783980</td>\n",
       "      <td>-1.280579</td>\n",
       "      <td>-1.003480</td>\n",
       "      <td>-7.753201</td>\n",
       "      <td>-1.320547</td>\n",
       "      <td>0.919078</td>\n",
       "      <td>-1.036068</td>\n",
       "      <td>0.030213</td>\n",
       "      <td>0.910172</td>\n",
       "      <td>-0.905345</td>\n",
       "      <td>0.646641</td>\n",
       "      <td>-0.465291</td>\n",
       "      <td>-0.531735</td>\n",
       "      <td>-0.756781</td>\n",
       "      <td>0.193724</td>\n",
       "      <td>0.224277</td>\n",
       "      <td>-0.474412</td>\n",
       "      <td>1.885805</td>\n",
       "      <td>0.205439</td>\n",
       "      <td>-6.481422</td>\n",
       "      <td>1.035620</td>\n",
       "      <td>-0.453623</td>\n",
       "      <td>0.375936</td>\n",
       "      <td>-0.320670</td>\n",
       "      <td>-0.144646</td>\n",
       "      <td>-0.220129</td>\n",
       "      <td>0.577826</td>\n",
       "      <td>-0.360512</td>\n",
       "      <td>-0.600107</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982205</td>\n",
       "      <td>-1.161978</td>\n",
       "      <td>0.532269</td>\n",
       "      <td>1.133215</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>-1.390962</td>\n",
       "      <td>0.158572</td>\n",
       "      <td>0.143794</td>\n",
       "      <td>-0.317185</td>\n",
       "      <td>1.017192</td>\n",
       "      <td>-0.395342</td>\n",
       "      <td>-0.642357</td>\n",
       "      <td>-0.627209</td>\n",
       "      <td>0.257271</td>\n",
       "      <td>-1.461564</td>\n",
       "      <td>0.325613</td>\n",
       "      <td>1.628369</td>\n",
       "      <td>0.640040</td>\n",
       "      <td>0.750735</td>\n",
       "      <td>1.164573</td>\n",
       "      <td>0.900373</td>\n",
       "      <td>0.063489</td>\n",
       "      <td>0.948158</td>\n",
       "      <td>0.273014</td>\n",
       "      <td>-1.269147</td>\n",
       "      <td>-0.251101</td>\n",
       "      <td>-2.271731</td>\n",
       "      <td>-0.044167</td>\n",
       "      <td>-0.443766</td>\n",
       "      <td>-1.144794</td>\n",
       "      <td>-0.645115</td>\n",
       "      <td>-1.246090</td>\n",
       "      <td>2.613357</td>\n",
       "      <td>-0.479664</td>\n",
       "      <td>1.581289</td>\n",
       "      <td>0.931258</td>\n",
       "      <td>0.151937</td>\n",
       "      <td>-0.766595</td>\n",
       "      <td>0.474351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ccbcb3d13e5072ff1d9c61afe2c4f77</td>\n",
       "      <td>-1.680473</td>\n",
       "      <td>0.860529</td>\n",
       "      <td>-1.076195</td>\n",
       "      <td>0.740124</td>\n",
       "      <td>3.678445</td>\n",
       "      <td>0.288558</td>\n",
       "      <td>0.515875</td>\n",
       "      <td>0.920590</td>\n",
       "      <td>-1.223277</td>\n",
       "      <td>-1.029780</td>\n",
       "      <td>-2.203397</td>\n",
       "      <td>-7.088717</td>\n",
       "      <td>0.438218</td>\n",
       "      <td>-0.848173</td>\n",
       "      <td>1.542666</td>\n",
       "      <td>-2.166858</td>\n",
       "      <td>-0.867670</td>\n",
       "      <td>-0.980947</td>\n",
       "      <td>0.567793</td>\n",
       "      <td>1.323430</td>\n",
       "      <td>-2.076700</td>\n",
       "      <td>-0.291598</td>\n",
       "      <td>-1.564816</td>\n",
       "      <td>-8.718695</td>\n",
       "      <td>0.340144</td>\n",
       "      <td>-0.566402</td>\n",
       "      <td>0.844324</td>\n",
       "      <td>0.816421</td>\n",
       "      <td>-1.019114</td>\n",
       "      <td>-0.881431</td>\n",
       "      <td>-2.285710</td>\n",
       "      <td>-0.090958</td>\n",
       "      <td>-0.898440</td>\n",
       "      <td>-0.584417</td>\n",
       "      <td>-0.143660</td>\n",
       "      <td>-0.182084</td>\n",
       "      <td>0.798516</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>-0.347155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829467</td>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.427946</td>\n",
       "      <td>-0.563037</td>\n",
       "      <td>-0.103990</td>\n",
       "      <td>-0.817698</td>\n",
       "      <td>1.251046</td>\n",
       "      <td>-0.977157</td>\n",
       "      <td>2.732600</td>\n",
       "      <td>1.997984</td>\n",
       "      <td>-0.214285</td>\n",
       "      <td>-0.389428</td>\n",
       "      <td>-1.007633</td>\n",
       "      <td>0.336435</td>\n",
       "      <td>-0.851292</td>\n",
       "      <td>-0.024184</td>\n",
       "      <td>0.455908</td>\n",
       "      <td>0.458753</td>\n",
       "      <td>-0.267230</td>\n",
       "      <td>-2.032402</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>0.654107</td>\n",
       "      <td>-3.512338</td>\n",
       "      <td>-0.840937</td>\n",
       "      <td>0.519407</td>\n",
       "      <td>-0.028053</td>\n",
       "      <td>-1.621083</td>\n",
       "      <td>0.142132</td>\n",
       "      <td>1.514664</td>\n",
       "      <td>0.828815</td>\n",
       "      <td>0.516422</td>\n",
       "      <td>0.130521</td>\n",
       "      <td>-0.459210</td>\n",
       "      <td>2.028205</td>\n",
       "      <td>-0.093968</td>\n",
       "      <td>-0.218274</td>\n",
       "      <td>-0.163136</td>\n",
       "      <td>-0.870289</td>\n",
       "      <td>0.064038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e350f17a357f12a1941f0837afb7eb8d</td>\n",
       "      <td>0.183774</td>\n",
       "      <td>0.919134</td>\n",
       "      <td>-0.946958</td>\n",
       "      <td>0.918492</td>\n",
       "      <td>0.862278</td>\n",
       "      <td>1.155287</td>\n",
       "      <td>0.911106</td>\n",
       "      <td>0.562598</td>\n",
       "      <td>-1.349685</td>\n",
       "      <td>-1.182729</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>-0.626847</td>\n",
       "      <td>0.368980</td>\n",
       "      <td>1.560784</td>\n",
       "      <td>0.502851</td>\n",
       "      <td>-0.108050</td>\n",
       "      <td>0.633208</td>\n",
       "      <td>-0.411502</td>\n",
       "      <td>-3.201592</td>\n",
       "      <td>-0.710612</td>\n",
       "      <td>0.786816</td>\n",
       "      <td>0.500979</td>\n",
       "      <td>-1.040048</td>\n",
       "      <td>-1.369170</td>\n",
       "      <td>0.987666</td>\n",
       "      <td>-0.681838</td>\n",
       "      <td>-0.331372</td>\n",
       "      <td>2.254289</td>\n",
       "      <td>-0.009330</td>\n",
       "      <td>2.007067</td>\n",
       "      <td>1.203750</td>\n",
       "      <td>-2.003928</td>\n",
       "      <td>-0.566088</td>\n",
       "      <td>0.223452</td>\n",
       "      <td>0.434202</td>\n",
       "      <td>-1.203766</td>\n",
       "      <td>-0.103490</td>\n",
       "      <td>0.441111</td>\n",
       "      <td>1.818458</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.231836</td>\n",
       "      <td>0.833236</td>\n",
       "      <td>-0.454226</td>\n",
       "      <td>-1.614694</td>\n",
       "      <td>0.159948</td>\n",
       "      <td>-0.150059</td>\n",
       "      <td>-1.570599</td>\n",
       "      <td>0.960839</td>\n",
       "      <td>0.102214</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.852834</td>\n",
       "      <td>-1.265608</td>\n",
       "      <td>-3.219190</td>\n",
       "      <td>0.251194</td>\n",
       "      <td>0.215861</td>\n",
       "      <td>-0.009520</td>\n",
       "      <td>1.611203</td>\n",
       "      <td>1.679806</td>\n",
       "      <td>-0.008419</td>\n",
       "      <td>0.658384</td>\n",
       "      <td>-0.132437</td>\n",
       "      <td>-1.466823</td>\n",
       "      <td>-1.577080</td>\n",
       "      <td>-0.800346</td>\n",
       "      <td>1.960795</td>\n",
       "      <td>-4.042900</td>\n",
       "      <td>1.722143</td>\n",
       "      <td>-0.261888</td>\n",
       "      <td>-1.145005</td>\n",
       "      <td>-1.864582</td>\n",
       "      <td>-1.168967</td>\n",
       "      <td>1.385089</td>\n",
       "      <td>-0.353028</td>\n",
       "      <td>3.316150</td>\n",
       "      <td>-0.524087</td>\n",
       "      <td>-0.794327</td>\n",
       "      <td>3.936365</td>\n",
       "      <td>0.682989</td>\n",
       "      <td>-2.521211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a8f910ea6075b6376af079055965ff68</td>\n",
       "      <td>-0.203933</td>\n",
       "      <td>-0.177252</td>\n",
       "      <td>0.368074</td>\n",
       "      <td>-0.701320</td>\n",
       "      <td>-1.104391</td>\n",
       "      <td>0.735760</td>\n",
       "      <td>0.894273</td>\n",
       "      <td>-1.375826</td>\n",
       "      <td>-5.144946</td>\n",
       "      <td>-2.048711</td>\n",
       "      <td>0.629773</td>\n",
       "      <td>-4.252669</td>\n",
       "      <td>-0.087420</td>\n",
       "      <td>-0.794367</td>\n",
       "      <td>-1.063963</td>\n",
       "      <td>0.115997</td>\n",
       "      <td>0.895180</td>\n",
       "      <td>3.184848</td>\n",
       "      <td>2.057840</td>\n",
       "      <td>-0.950821</td>\n",
       "      <td>0.961059</td>\n",
       "      <td>-1.837828</td>\n",
       "      <td>-0.437156</td>\n",
       "      <td>-0.828433</td>\n",
       "      <td>0.373747</td>\n",
       "      <td>-0.099787</td>\n",
       "      <td>-0.976280</td>\n",
       "      <td>-0.165921</td>\n",
       "      <td>3.297221</td>\n",
       "      <td>3.914132</td>\n",
       "      <td>-4.971376</td>\n",
       "      <td>-0.286520</td>\n",
       "      <td>-0.160133</td>\n",
       "      <td>-3.301453</td>\n",
       "      <td>-1.021032</td>\n",
       "      <td>-0.562744</td>\n",
       "      <td>0.574065</td>\n",
       "      <td>-0.368194</td>\n",
       "      <td>-0.507458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178099</td>\n",
       "      <td>-0.410396</td>\n",
       "      <td>-1.184236</td>\n",
       "      <td>1.681727</td>\n",
       "      <td>0.589606</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>0.258885</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>-1.545597</td>\n",
       "      <td>0.822283</td>\n",
       "      <td>1.518209</td>\n",
       "      <td>0.460143</td>\n",
       "      <td>0.822488</td>\n",
       "      <td>1.362718</td>\n",
       "      <td>0.218560</td>\n",
       "      <td>-1.038514</td>\n",
       "      <td>1.000763</td>\n",
       "      <td>-0.975878</td>\n",
       "      <td>-0.551268</td>\n",
       "      <td>-0.133044</td>\n",
       "      <td>-0.393092</td>\n",
       "      <td>1.236473</td>\n",
       "      <td>1.657100</td>\n",
       "      <td>0.833020</td>\n",
       "      <td>0.665379</td>\n",
       "      <td>-0.900025</td>\n",
       "      <td>0.291908</td>\n",
       "      <td>0.482727</td>\n",
       "      <td>0.552399</td>\n",
       "      <td>0.970496</td>\n",
       "      <td>-0.279168</td>\n",
       "      <td>1.544356</td>\n",
       "      <td>2.959727</td>\n",
       "      <td>1.641201</td>\n",
       "      <td>-0.130818</td>\n",
       "      <td>-0.264292</td>\n",
       "      <td>-0.748668</td>\n",
       "      <td>0.964218</td>\n",
       "      <td>0.087079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id   ...    target\n",
       "0  707b395ecdcbb4dc2eabea00e4d1b179   ...         0\n",
       "1  5880c03c6582a7b42248668e56b4bdec   ...         0\n",
       "2  4ccbcb3d13e5072ff1d9c61afe2c4f77   ...         1\n",
       "3  e350f17a357f12a1941f0837afb7eb8d   ...         0\n",
       "4  a8f910ea6075b6376af079055965ff68   ...         0\n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, os\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 2 main functions: Emperical Covariance and calculate probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next functions calculates empirical covariance and mean using numpy per label type and returns the two multivariate normal distibutions as instance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def get_mv(x,y):\n",
    "    ones = (y==1).astype(bool)\n",
    "    x2 = x[ones]\n",
    "    cov1 = np.cov(x2.T)\n",
    "    m1 = np.mean(x2, axis = 0)\n",
    "    \n",
    "    zeros = (y==0).astype(bool)\n",
    "    x2b = x[zeros]\n",
    "    cov2 = np.cov(x2b.T)\n",
    "    m2 = np.mean(x2b, axis = 0)\n",
    "    \n",
    "    mv1 = multivariate_normal(mean=m1, cov=cov1)\n",
    "    mv2 = multivariate_normal(mean=m2, cov=cov2)\n",
    "    \n",
    "    return mv1, mv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function calculates the probability that a datapoint belongs to a multivariate normal distribution given two alernatives. first we calculate the probability for a specific datapoint to come from each of the two multivariate distributions using `multivariate_normal.pdf`. Then we use bayes formula to get the overall probabiity. See e.g. https://math.stackexchange.com/questions/825455/probability-that-a-sample-comes-from-one-of-two-distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(x,mv1,mv2):\n",
    "    y_pred2 = np.zeros((len(x),))\n",
    "    for i in range(len(x)):\n",
    "        a = mv1.pdf(x[i])\n",
    "        b = mv2.pdf(x[i])\n",
    "        y_pred2[i] = a/(a+b)\n",
    "    return y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:53<00:00,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA scores CV = 0.96504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE VARIABLES\n",
    "cols = [c for c in train.columns if c not in ['id', 'target']]\n",
    "cols.remove('wheezy-copper-turtle-magic')\n",
    "oof = np.zeros(len(train))\n",
    "preds = np.zeros(len(test))\n",
    "\n",
    "# BUILD 512 SEPARATE MODELS\n",
    "for i in tqdm(range(512)):\n",
    "    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n",
    "    train2 = train[train['wheezy-copper-turtle-magic']==i]\n",
    "    test2 = test[test['wheezy-copper-turtle-magic']==i]\n",
    "    idx1 = train2.index; idx2 = test2.index\n",
    "    train2.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n",
    "    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n",
    "    train3 = sel.transform(train2[cols])\n",
    "    test3 = sel.transform(test2[cols])\n",
    "    \n",
    "    # STRATIFIED K-FOLD\n",
    "    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in skf.split(train3, train2['target']):\n",
    "        \n",
    "        # MODEL AND PREDICT WITH QDA\n",
    "        #clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n",
    "        \n",
    "        \n",
    "        mv1, mv2 = get_mv(train3[train_index,:],train2.loc[train_index]['target'].values)\n",
    "        oof[idx1[test_index]] = calc_prob(train3[test_index,:],mv1,mv2)\n",
    "        preds[idx2] += calc_prob(test3,mv1,mv2)/ skf.n_splits\n",
    "       \n",
    "    #if i%64==0: print(i)\n",
    "        \n",
    "# PRINT CV AUC\n",
    "auc = roc_auc_score(train['target'],oof)\n",
    "print('QDA scores CV =',round(auc,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 & 4 - Add pseudo label data and recalculate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in the original kernel we use pseudlabelling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [02:03<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Labeled QDA scores CV = 0.97027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE VARIABLES\n",
    "test['target'] = preds\n",
    "oof = np.zeros(len(train))\n",
    "preds = np.zeros(len(test))\n",
    "\n",
    "# BUILD 512 SEPARATE MODELS\n",
    "for k in tqdm(range(512)):\n",
    "    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n",
    "    train2 = train[train['wheezy-copper-turtle-magic']==k] \n",
    "    train2p = train2.copy(); idx1 = train2.index \n",
    "    test2 = test[test['wheezy-copper-turtle-magic']==k]\n",
    "    \n",
    "    # ADD PSEUDO LABELED DATA\n",
    "    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n",
    "    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n",
    "    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n",
    "    train2p = pd.concat([train2p,test2p],axis=0)\n",
    "    train2p.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n",
    "    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n",
    "    train3p = sel.transform(train2p[cols])\n",
    "    train3 = sel.transform(train2[cols])\n",
    "    test3 = sel.transform(test2[cols])\n",
    "        \n",
    "    # STRATIFIED K FOLD\n",
    "    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in skf.split(train3p, train2p['target']):\n",
    "        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n",
    "        \n",
    "        mv1, mv2 = get_mv(train3p[train_index,:],train2p.loc[train_index]['target'].values)\n",
    "        oof[idx1[test_index3]] = calc_prob(train3[test_index3,:],mv1,mv2)\n",
    "        preds[test2.index] += calc_prob(test3,mv1,mv2) / skf.n_splits\n",
    "\n",
    "       \n",
    "    #if k%64==0: print(k)\n",
    "        \n",
    "# PRINT CV AUC\n",
    "auc = roc_auc_score(train['target'],oof)\n",
    "print('Pseudo Labeled QDA scores CV =',round(auc,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub['target'] = preds\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(preds,bins=100)\n",
    "plt.title('Final Test.csv predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this kernel, we learned what pseudo labeling is, why it works, and how to deploy it. Using it on the data from Instant Gratification competition we observed it increase CV by an impressive 0.005! Pseudo labeling QDA achieved CV 0.970 and LB 0.969. Without pseudo labeling, QDA achieved CV 0.965 and LB 0.965.\n",
    "\n",
    "When you run your kernel locally, it will only pseudo label the public test data (because that is all that `test.csv` contains). When you submit this solution to Kaggle, your submission will load the full `test.csv` and pseudo label both the public and private test data set. Thus you will approximately double your amount of training data for your submissions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
