{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train', 'train.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#read in all our files\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "train_images = '../input/train/*'\n",
    "test_images = '../input/test/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b5cc75b012b6551901570515c2623ce58bb15e4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  has_cactus\n",
       "0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
       "1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
       "2  000d1e9a533f62e55c289303b072733d.jpg           1\n",
       "3  0011485b40695e9138e92d0b3fb55128.jpg           1\n",
       "4  0014d7a11e90b62848904c1418fc8cf2.jpg           1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "72c64ea60813ecb730405a33c8a33b4d50a33012"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f98563abac8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style = 'darkgrid')\n",
    "plt.figure(figsize = (12,10))\n",
    "sns.countplot(train_df['has_cactus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "df11394a68c5584fa1cf569e73ee8565c3425320"
   },
   "outputs": [],
   "source": [
    "#let's visualize some cactus images\n",
    "IMAGES = os.path.join(train_images, \"*\")\n",
    "all_images = glob.glob(IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "518c83ba4bc18813bc4660defe2e74feebd7e7ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], <a list of 0 Text yticklabel objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize some images\n",
    "\n",
    "plt.figure(figsize = (12,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(plt.imread(all_images[0]))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(plt.imread(all_images[10]))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(plt.imread(all_images[20]))\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e3499ef413eebeb263f4c5b3827a4f4cc54e5a9c"
   },
   "outputs": [],
   "source": [
    "train_path = '../input/train/train/'\n",
    "test_path = '../input/test/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "5b47335ea0ee75549042f7ce4445cd60d959ca2b"
   },
   "outputs": [],
   "source": [
    "#let's get our image data and image labels toegether\n",
    "#read in all the images\n",
    "images_id = train_df['id'].values\n",
    "X = [] #this list will contain all our images\n",
    "for id_ in images_id:\n",
    "    img = cv2.imread(train_path + id_)\n",
    "    X.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "37d8def8265b7536585c148951584ca39af0af4c"
   },
   "outputs": [],
   "source": [
    "#now let's get our labels\n",
    "label_list = [] #will contain all our labels\n",
    "for img_id in images_id:\n",
    "    label_list.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "1899dd5ea26b8233d90d8abd7b409cdd5beb978f"
   },
   "outputs": [],
   "source": [
    "#now we can convert our images list and the labels list into numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "fcb45b36e5c81ce2b859941f5c63d7e57e7f3035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SIZE OF OUR TRAINING DATA : (17500, 32, 32, 3)\n",
      "THE SIZE OF OUR TRAINING LABELS : (17500,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"THE SIZE OF OUR TRAINING DATA : {X.shape}\")\n",
    "print(f\"THE SIZE OF OUR TRAINING LABELS : {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "da0e8b075a71c3b29ff53ae32d9bf7d2a773a329"
   },
   "outputs": [],
   "source": [
    "#let's do some preprocessing such as normalizing our data\n",
    "X = X.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "8f8e9ffdbdfc62f46795dddc77026176de0275f8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82801b03d8da4278a95e861189b1b711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#loading in and preprocessing the test data\n",
    "X_test = []\n",
    "test_images = []\n",
    "for img_id in tqdm_notebook(os.listdir(test_path)):\n",
    "    X_test.append(cv2.imread(test_path + img_id))     \n",
    "    test_images.append(img_id)\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac134f28dcb86eab91448fe7089577b5004f77b1"
   },
   "source": [
    "## BUILD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "0979f6994d07070e01a9c93c47061a58183ccfd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import keras\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "1fa0c332dd19fac460e281a80e72ee5316f87e2b"
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def build(height, width, classes, channels):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, channels)\n",
    "        chanDim = -1\n",
    "        \n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            inputShape = (channels, height, width)\n",
    "            chanDim = 1\n",
    "        model.add(Conv2D(32, (3,3), padding = 'same', input_shape = inputShape))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (3,3), padding = 'same'))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(128, (3,3), padding = 'same', input_shape = inputShape))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(128, (3,3), padding = 'same'))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256, (3,3), padding = 'same', input_shape = inputShape))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(256, (3,3), padding = 'same'))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(128, activation = 'relu'))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(32, activation = 'relu'))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(classes, activation = 'sigmoid'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9e933cf85e8c4a3c494f9cef44c9d175e78aee0"
   },
   "source": [
    "## ENSEMBLE NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "22415149124c2d96e16ab5bded020644d0276cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL NO : 0\n",
      "Epoch 1/200\n",
      "17500/17500 [==============================] - 12s 711us/step - loss: 0.2101 - acc: 0.9271\n",
      "Epoch 2/200\n",
      "17500/17500 [==============================] - 8s 461us/step - loss: 0.0847 - acc: 0.9743\n",
      "Epoch 3/200\n",
      "17500/17500 [==============================] - 8s 467us/step - loss: 0.0669 - acc: 0.9787\n",
      "Epoch 4/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0560 - acc: 0.9831\n",
      "Epoch 5/200\n",
      "17500/17500 [==============================] - 8s 463us/step - loss: 0.0398 - acc: 0.9883\n",
      "Epoch 6/200\n",
      "17500/17500 [==============================] - 8s 461us/step - loss: 0.0435 - acc: 0.9875\n",
      "Epoch 7/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0310 - acc: 0.9905\n",
      "Epoch 8/200\n",
      "17500/17500 [==============================] - 8s 461us/step - loss: 0.0294 - acc: 0.9914\n",
      "Epoch 9/200\n",
      "17500/17500 [==============================] - 8s 457us/step - loss: 0.0223 - acc: 0.9937\n",
      "Epoch 10/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0260 - acc: 0.9921\n",
      "Epoch 11/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0248 - acc: 0.9931\n",
      "Epoch 12/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0168 - acc: 0.9946\n",
      "Epoch 13/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0230 - acc: 0.9933\n",
      "Epoch 14/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0149 - acc: 0.9955\n",
      "Epoch 15/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0171 - acc: 0.9949\n",
      "Epoch 16/200\n",
      "17500/17500 [==============================] - 8s 459us/step - loss: 0.0141 - acc: 0.9964\n",
      "Epoch 17/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0107 - acc: 0.9969\n",
      "Epoch 18/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0118 - acc: 0.9966\n",
      "Epoch 19/200\n",
      "17500/17500 [==============================] - 8s 459us/step - loss: 0.0116 - acc: 0.9966\n",
      "Epoch 20/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0113 - acc: 0.9966\n",
      "Epoch 21/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0081 - acc: 0.9980\n",
      "Epoch 22/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0102 - acc: 0.9969\n",
      "Epoch 23/200\n",
      "17500/17500 [==============================] - 8s 459us/step - loss: 0.0080 - acc: 0.9978\n",
      "Epoch 24/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0103 - acc: 0.9973\n",
      "Epoch 25/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0071 - acc: 0.9981\n",
      "Epoch 26/200\n",
      "17500/17500 [==============================] - 9s 504us/step - loss: 0.0060 - acc: 0.9982\n",
      "Epoch 27/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0087 - acc: 0.9975\n",
      "Epoch 28/200\n",
      "17500/17500 [==============================] - 8s 457us/step - loss: 0.0079 - acc: 0.9974\n",
      "Epoch 29/200\n",
      "17500/17500 [==============================] - 8s 461us/step - loss: 0.0036 - acc: 0.9987\n",
      "Epoch 30/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0045 - acc: 0.9986\n",
      "Epoch 31/200\n",
      "17500/17500 [==============================] - 8s 457us/step - loss: 0.0095 - acc: 0.9975\n",
      "Epoch 32/200\n",
      "17500/17500 [==============================] - 9s 494us/step - loss: 0.0064 - acc: 0.9976\n",
      "Epoch 33/200\n",
      "17500/17500 [==============================] - 9s 514us/step - loss: 0.0058 - acc: 0.9982\n",
      "Epoch 34/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0061 - acc: 0.9984\n",
      "Epoch 35/200\n",
      "17500/17500 [==============================] - 8s 459us/step - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 36/200\n",
      "17500/17500 [==============================] - 8s 459us/step - loss: 0.0065 - acc: 0.9981\n",
      "Epoch 37/200\n",
      "17500/17500 [==============================] - 8s 461us/step - loss: 0.0058 - acc: 0.9982\n",
      "Epoch 38/200\n",
      "17500/17500 [==============================] - 8s 459us/step - loss: 0.0035 - acc: 0.9989\n",
      "Epoch 39/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0030 - acc: 0.9991\n",
      "Epoch 40/200\n",
      "17500/17500 [==============================] - 8s 459us/step - loss: 0.0049 - acc: 0.9985\n",
      "Epoch 41/200\n",
      "17500/17500 [==============================] - 8s 465us/step - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 42/200\n",
      "17500/17500 [==============================] - 8s 459us/step - loss: 0.0032 - acc: 0.9990\n",
      "Epoch 43/200\n",
      "17500/17500 [==============================] - 8s 458us/step - loss: 0.0086 - acc: 0.9974\n",
      "Epoch 44/200\n",
      "17500/17500 [==============================] - 8s 460us/step - loss: 0.0027 - acc: 0.9991\n",
      "Epoch 45/200\n",
      "17500/17500 [==============================] - 8s 461us/step - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 46/200\n",
      " 3488/17500 [====>.........................] - ETA: 6s - loss: 2.9141e-04 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "input_dim = X.shape[1:]\n",
    "activation = 'relu'\n",
    "classes = 1\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "history = dict() #dictionery to store the history of individual models for later visualization\n",
    "prediction_scores = dict() #dictionery to store the predicted scores of individual models on the test dataset\n",
    "\n",
    "#here we will be training the same model for a total of 10 times and will be considering the mean of the output values for predictions\n",
    "for i in np.arange(0, 5):\n",
    "    optim = optimizers.Adam(lr = 0.001)\n",
    "    ensemble_model = CNN.build(height = height, width = width, classes = classes, channels = channels)\n",
    "    ensemble_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])\n",
    "    print('TRAINING MODEL NO : {}'.format(i))\n",
    "    H = ensemble_model.fit(X, y,\n",
    "                           batch_size = 32,\n",
    "                           epochs = 200,\n",
    "                           verbose = 1)\n",
    "    history[i] = H\n",
    "    \n",
    "    ensemble_model.save('MODEL_{}.model'.format(i))\n",
    "    \n",
    "    predictions = ensemble_model.predict(X_test, verbose = 1, batch_size = 32)\n",
    "    prediction_scores[i] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0dfd9d63c07555d743d89486fddf75a7b7d713c"
   },
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "044cde863466736fb8bd56d1a175ce992eda09ef"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "f2d267f87f08d0ef0159253980fe0a98537c530a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16 = VGG16(weights = 'imagenet', input_shape = (32, 32, 3), include_top = False)\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "25adae25c6ca3fa5b49aa342aac80fccbbf614ff"
   },
   "outputs": [],
   "source": [
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "a231df553c50b67dfe3e67dc62ea06c8f7e0e0a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 14,880,577\n",
      "Trainable params: 165,121\n",
      "Non-trainable params: 14,715,456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model = Sequential()\n",
    "vgg_model.add(vgg16)\n",
    "vgg_model.add(Flatten())\n",
    "vgg_model.add(Dense(256, activation = 'relu'))\n",
    "vgg_model.add(BatchNormalization())\n",
    "vgg_model.add(Dropout(0.5))\n",
    "vgg_model.add(Dense(128, activation = 'relu'))\n",
    "vgg_model.add(BatchNormalization())\n",
    "vgg_model.add(Dropout(0.5))\n",
    "vgg_model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "ec18638aa2d978032f4d0f560152702305d51f1c"
   },
   "outputs": [],
   "source": [
    "#compile the model\n",
    "vgg_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "81aca67136b4eb92293a4937c0a567ed62bcff1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "17500/17500 [==============================] - 6s 320us/step - loss: 0.1719 - acc: 0.9358\n",
      "Epoch 2/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.1074 - acc: 0.9611\n",
      "Epoch 3/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0990 - acc: 0.9636\n",
      "Epoch 4/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0892 - acc: 0.9658\n",
      "Epoch 5/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0869 - acc: 0.9683\n",
      "Epoch 6/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0828 - acc: 0.9701\n",
      "Epoch 7/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0766 - acc: 0.9723\n",
      "Epoch 8/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0802 - acc: 0.9686\n",
      "Epoch 9/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0755 - acc: 0.9718\n",
      "Epoch 10/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0752 - acc: 0.9714\n",
      "Epoch 11/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0719 - acc: 0.9731\n",
      "Epoch 12/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0718 - acc: 0.9762\n",
      "Epoch 13/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0702 - acc: 0.9744\n",
      "Epoch 14/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0694 - acc: 0.9745\n",
      "Epoch 15/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0687 - acc: 0.9751\n",
      "Epoch 16/500\n",
      "17500/17500 [==============================] - 3s 200us/step - loss: 0.0699 - acc: 0.9752\n",
      "Epoch 17/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0667 - acc: 0.9755\n",
      "Epoch 18/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0639 - acc: 0.9765\n",
      "Epoch 19/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0636 - acc: 0.9763\n",
      "Epoch 20/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0661 - acc: 0.9759\n",
      "Epoch 21/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0607 - acc: 0.9777\n",
      "Epoch 22/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0627 - acc: 0.9763\n",
      "Epoch 23/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0609 - acc: 0.9781\n",
      "Epoch 24/500\n",
      "17500/17500 [==============================] - 4s 205us/step - loss: 0.0618 - acc: 0.9776\n",
      "Epoch 25/500\n",
      "17500/17500 [==============================] - 4s 229us/step - loss: 0.0636 - acc: 0.9764\n",
      "Epoch 26/500\n",
      "17500/17500 [==============================] - 4s 229us/step - loss: 0.0623 - acc: 0.9777\n",
      "Epoch 27/500\n",
      "17500/17500 [==============================] - 4s 215us/step - loss: 0.0576 - acc: 0.9797\n",
      "Epoch 28/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0604 - acc: 0.9779\n",
      "Epoch 29/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0590 - acc: 0.9782\n",
      "Epoch 30/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0600 - acc: 0.9779\n",
      "Epoch 31/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0557 - acc: 0.9782\n",
      "Epoch 32/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0586 - acc: 0.9778\n",
      "Epoch 33/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0575 - acc: 0.9791\n",
      "Epoch 34/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0586 - acc: 0.9777\n",
      "Epoch 35/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0609 - acc: 0.9779\n",
      "Epoch 36/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0544 - acc: 0.9805\n",
      "Epoch 37/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0550 - acc: 0.9800\n",
      "Epoch 38/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0560 - acc: 0.9797\n",
      "Epoch 39/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0554 - acc: 0.9796\n",
      "Epoch 40/500\n",
      "17500/17500 [==============================] - 4s 204us/step - loss: 0.0581 - acc: 0.9791\n",
      "Epoch 41/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0549 - acc: 0.9795\n",
      "Epoch 42/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0544 - acc: 0.9799\n",
      "Epoch 43/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0544 - acc: 0.9801\n",
      "Epoch 44/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0534 - acc: 0.9803\n",
      "Epoch 45/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0526 - acc: 0.9812\n",
      "Epoch 46/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0547 - acc: 0.9801\n",
      "Epoch 47/500\n",
      "17500/17500 [==============================] - 4s 204us/step - loss: 0.0536 - acc: 0.9800\n",
      "Epoch 48/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0492 - acc: 0.9817\n",
      "Epoch 49/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0553 - acc: 0.9802\n",
      "Epoch 50/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0502 - acc: 0.9820\n",
      "Epoch 51/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0505 - acc: 0.9823\n",
      "Epoch 52/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0534 - acc: 0.9801\n",
      "Epoch 53/500\n",
      "17500/17500 [==============================] - 4s 204us/step - loss: 0.0505 - acc: 0.9809\n",
      "Epoch 54/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0460 - acc: 0.9831\n",
      "Epoch 55/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0492 - acc: 0.9819\n",
      "Epoch 56/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0481 - acc: 0.9826\n",
      "Epoch 57/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0515 - acc: 0.9814\n",
      "Epoch 58/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0495 - acc: 0.9816\n",
      "Epoch 59/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0476 - acc: 0.9837\n",
      "Epoch 60/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0483 - acc: 0.9821\n",
      "Epoch 61/500\n",
      "17500/17500 [==============================] - 4s 216us/step - loss: 0.0456 - acc: 0.9830\n",
      "Epoch 62/500\n",
      "17500/17500 [==============================] - 4s 227us/step - loss: 0.0472 - acc: 0.9829\n",
      "Epoch 63/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0462 - acc: 0.9833\n",
      "Epoch 64/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0496 - acc: 0.9828\n",
      "Epoch 65/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0482 - acc: 0.9826\n",
      "Epoch 66/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0477 - acc: 0.9828\n",
      "Epoch 67/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0465 - acc: 0.9825\n",
      "Epoch 68/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0481 - acc: 0.9827\n",
      "Epoch 69/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0470 - acc: 0.9830\n",
      "Epoch 70/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0469 - acc: 0.9833\n",
      "Epoch 71/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0470 - acc: 0.9819\n",
      "Epoch 72/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0471 - acc: 0.9829\n",
      "Epoch 73/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0466 - acc: 0.9835\n",
      "Epoch 74/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0461 - acc: 0.9838\n",
      "Epoch 75/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0439 - acc: 0.9841\n",
      "Epoch 76/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0462 - acc: 0.9830\n",
      "Epoch 77/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0465 - acc: 0.9831\n",
      "Epoch 78/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0452 - acc: 0.9832\n",
      "Epoch 79/500\n",
      "17500/17500 [==============================] - 4s 205us/step - loss: 0.0452 - acc: 0.9829\n",
      "Epoch 80/500\n",
      "17500/17500 [==============================] - 4s 204us/step - loss: 0.0444 - acc: 0.9835\n",
      "Epoch 81/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0463 - acc: 0.9823\n",
      "Epoch 82/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0461 - acc: 0.9829\n",
      "Epoch 83/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0436 - acc: 0.9845\n",
      "Epoch 84/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0468 - acc: 0.9837\n",
      "Epoch 85/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0405 - acc: 0.9850\n",
      "Epoch 86/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0416 - acc: 0.9845\n",
      "Epoch 87/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0419 - acc: 0.9842\n",
      "Epoch 88/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0439 - acc: 0.9841\n",
      "Epoch 89/500\n",
      "17500/17500 [==============================] - 4s 200us/step - loss: 0.0393 - acc: 0.9858\n",
      "Epoch 90/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0440 - acc: 0.9853\n",
      "Epoch 91/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0428 - acc: 0.9847\n",
      "Epoch 92/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0451 - acc: 0.9838\n",
      "Epoch 93/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0410 - acc: 0.9849\n",
      "Epoch 94/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0419 - acc: 0.9846\n",
      "Epoch 95/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0427 - acc: 0.9847\n",
      "Epoch 96/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0394 - acc: 0.9855\n",
      "Epoch 97/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0419 - acc: 0.9844\n",
      "Epoch 98/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0412 - acc: 0.9850\n",
      "Epoch 99/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0390 - acc: 0.9859\n",
      "Epoch 100/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0442 - acc: 0.9843\n",
      "Epoch 101/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0414 - acc: 0.9853\n",
      "Epoch 102/500\n",
      "17500/17500 [==============================] - 4s 204us/step - loss: 0.0414 - acc: 0.9844\n",
      "Epoch 103/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0390 - acc: 0.9864\n",
      "Epoch 104/500\n",
      "17500/17500 [==============================] - 4s 204us/step - loss: 0.0426 - acc: 0.9846\n",
      "Epoch 105/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0394 - acc: 0.9859\n",
      "Epoch 106/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0399 - acc: 0.9861\n",
      "Epoch 107/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0398 - acc: 0.9857\n",
      "Epoch 108/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0426 - acc: 0.9851\n",
      "Epoch 109/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0407 - acc: 0.9852\n",
      "Epoch 110/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0421 - acc: 0.9847\n",
      "Epoch 111/500\n",
      "17500/17500 [==============================] - 4s 204us/step - loss: 0.0386 - acc: 0.9860\n",
      "Epoch 112/500\n",
      "17500/17500 [==============================] - 4s 227us/step - loss: 0.0422 - acc: 0.9847\n",
      "Epoch 113/500\n",
      "17500/17500 [==============================] - 4s 230us/step - loss: 0.0382 - acc: 0.9863\n",
      "Epoch 114/500\n",
      "17500/17500 [==============================] - 4s 219us/step - loss: 0.0386 - acc: 0.9865\n",
      "Epoch 115/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0391 - acc: 0.9857\n",
      "Epoch 116/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0406 - acc: 0.9843\n",
      "Epoch 117/500\n",
      "17500/17500 [==============================] - 4s 200us/step - loss: 0.0376 - acc: 0.9864\n",
      "Epoch 118/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0397 - acc: 0.9853\n",
      "Epoch 119/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0394 - acc: 0.9861\n",
      "Epoch 120/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0365 - acc: 0.9866\n",
      "Epoch 121/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0391 - acc: 0.9855\n",
      "Epoch 122/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0383 - acc: 0.9861\n",
      "Epoch 123/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0360 - acc: 0.9874\n",
      "Epoch 124/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0357 - acc: 0.9869\n",
      "Epoch 125/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0376 - acc: 0.9862\n",
      "Epoch 126/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0389 - acc: 0.9855\n",
      "Epoch 127/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0405 - acc: 0.9852\n",
      "Epoch 128/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0370 - acc: 0.9861\n",
      "Epoch 129/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0378 - acc: 0.9867\n",
      "Epoch 130/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0367 - acc: 0.9861\n",
      "Epoch 131/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0375 - acc: 0.9861\n",
      "Epoch 132/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0376 - acc: 0.9860\n",
      "Epoch 133/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0380 - acc: 0.9867\n",
      "Epoch 134/500\n",
      "17500/17500 [==============================] - 3s 200us/step - loss: 0.0394 - acc: 0.9866\n",
      "Epoch 135/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0371 - acc: 0.9875\n",
      "Epoch 136/500\n",
      "17500/17500 [==============================] - 4s 204us/step - loss: 0.0399 - acc: 0.9859\n",
      "Epoch 137/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0377 - acc: 0.9874\n",
      "Epoch 138/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0368 - acc: 0.9874\n",
      "Epoch 139/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0400 - acc: 0.9851\n",
      "Epoch 140/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0395 - acc: 0.9854\n",
      "Epoch 141/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0366 - acc: 0.9869\n",
      "Epoch 142/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0375 - acc: 0.9871\n",
      "Epoch 143/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0354 - acc: 0.9881\n",
      "Epoch 144/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0367 - acc: 0.9861\n",
      "Epoch 145/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0380 - acc: 0.9864\n",
      "Epoch 146/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0352 - acc: 0.9876\n",
      "Epoch 147/500\n",
      "17500/17500 [==============================] - 4s 221us/step - loss: 0.0343 - acc: 0.9874\n",
      "Epoch 148/500\n",
      "17500/17500 [==============================] - 4s 219us/step - loss: 0.0351 - acc: 0.9873\n",
      "Epoch 149/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0364 - acc: 0.9863\n",
      "Epoch 150/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0376 - acc: 0.9870\n",
      "Epoch 151/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0385 - acc: 0.9869\n",
      "Epoch 152/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0342 - acc: 0.9883\n",
      "Epoch 153/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0355 - acc: 0.9867\n",
      "Epoch 154/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0338 - acc: 0.9874\n",
      "Epoch 155/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0360 - acc: 0.9868\n",
      "Epoch 156/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0329 - acc: 0.9874\n",
      "Epoch 157/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0383 - acc: 0.9856\n",
      "Epoch 158/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0362 - acc: 0.9872\n",
      "Epoch 159/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0361 - acc: 0.9862\n",
      "Epoch 160/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0361 - acc: 0.9866\n",
      "Epoch 161/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0342 - acc: 0.9875\n",
      "Epoch 162/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0348 - acc: 0.9873\n",
      "Epoch 163/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0350 - acc: 0.9871\n",
      "Epoch 164/500\n",
      "17500/17500 [==============================] - 4s 206us/step - loss: 0.0364 - acc: 0.9859\n",
      "Epoch 165/500\n",
      "17500/17500 [==============================] - 4s 205us/step - loss: 0.0317 - acc: 0.9881\n",
      "Epoch 166/500\n",
      "17500/17500 [==============================] - 3s 200us/step - loss: 0.0355 - acc: 0.9872\n",
      "Epoch 167/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0406 - acc: 0.9864\n",
      "Epoch 168/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0364 - acc: 0.9866\n",
      "Epoch 169/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0352 - acc: 0.9879\n",
      "Epoch 170/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0366 - acc: 0.9869\n",
      "Epoch 171/500\n",
      "17500/17500 [==============================] - 4s 203us/step - loss: 0.0362 - acc: 0.9872\n",
      "Epoch 172/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0359 - acc: 0.9866\n",
      "Epoch 173/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0345 - acc: 0.9869\n",
      "Epoch 174/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0364 - acc: 0.9877\n",
      "Epoch 175/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0339 - acc: 0.9875\n",
      "Epoch 176/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0356 - acc: 0.9871\n",
      "Epoch 177/500\n",
      "17500/17500 [==============================] - 4s 201us/step - loss: 0.0323 - acc: 0.9886\n",
      "Epoch 178/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0336 - acc: 0.9882\n",
      "Epoch 179/500\n",
      "17500/17500 [==============================] - 4s 202us/step - loss: 0.0336 - acc: 0.9886\n",
      "Epoch 180/500\n",
      " 4928/17500 [=======>......................] - ETA: 2s - loss: 0.0328 - acc: 0.9874"
     ]
    }
   ],
   "source": [
    "#fit the model on our data\n",
    "vgg_history = vgg_model.fit(X, y,\n",
    "                            batch_size = 64,\n",
    "                            epochs = 500,\n",
    "                            verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "7388a94a2f5a8dc2e89eea4ac4f0b91971016cd2"
   },
   "outputs": [],
   "source": [
    "#making predictions on test dat\n",
    "predictions_vgg = vgg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "882c13e695514726859076845bd02ce128b82fbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_vgg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e97a73a092dbd98892c2e7d80112326e419ecf06"
   },
   "source": [
    "## RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "953f5af3fea4a6eb453e71531af0b98fb5281ce1"
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "0fc9ffe7f7e0e3c9787c338c29b24e623df87e77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 1s 0us/step\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 16, 16, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 8, 8, 64)     4160        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 8, 8, 64)     256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 64)     0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 8, 8, 64)     36928       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 8, 8, 64)     256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 64)     0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 8, 8, 256)    16640       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 8, 8, 256)    16640       max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 8, 8, 256)    1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 8, 8, 256)    1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8, 8, 256)    0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 8, 8, 64)     16448       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 8, 8, 64)     256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 64)     0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 8, 8, 64)     36928       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 8, 8, 64)     256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 64)     0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 8, 8, 256)    16640       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 8, 8, 256)    1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 256)    0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 256)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 8, 8, 64)     16448       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 8, 8, 64)     256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 64)     0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 8, 8, 64)     36928       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 8, 8, 64)     256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 64)     0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 8, 8, 256)    16640       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 8, 8, 256)    1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 256)    0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 256)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 4, 4, 128)    32896       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 4, 4, 128)    512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 128)    0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 4, 4, 128)    147584      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 4, 4, 128)    512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 128)    0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 4, 4, 512)    66048       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 4, 4, 512)    131584      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 4, 4, 512)    2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 4, 4, 512)    2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 4, 4, 512)    0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 4, 4, 128)    65664       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 4, 4, 128)    512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 128)    0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 4, 4, 128)    147584      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 4, 4, 128)    512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 128)    0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 4, 4, 512)    66048       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 4, 4, 512)    2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 512)    0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 4, 4, 128)    65664       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 4, 4, 128)    512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 128)    0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 4, 4, 128)    147584      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 4, 4, 128)    512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 128)    0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 4, 4, 512)    66048       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 4, 4, 512)    2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 512)    0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 4, 4, 128)    65664       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 4, 4, 128)    512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 128)    0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 4, 4, 128)    147584      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 4, 4, 128)    512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 128)    0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 4, 4, 512)    66048       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 4, 4, 512)    2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 4, 512)    0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 512)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 2, 2, 256)    131328      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 2, 2, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 2, 2, 256)    590080      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 2, 2, 256)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 2, 2, 1024)   263168      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 2, 2, 1024)   525312      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 2, 2, 1024)   4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 2, 2, 1024)   0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 2, 2, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 2, 2, 256)    262400      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 2, 2, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 2, 2, 256)    590080      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 2, 2, 256)    0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 2, 2, 1024)   263168      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 2, 2, 1024)   0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 2, 2, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 2, 2, 256)    262400      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 2, 2, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 2, 2, 256)    590080      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 2, 2, 256)    0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 2, 2, 1024)   263168      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 2, 2, 1024)   0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 2, 2, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 2, 2, 256)    262400      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 2, 2, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 2, 2, 256)    590080      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 2, 2, 256)    0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 2, 2, 1024)   263168      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 2, 2, 1024)   0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 2, 2, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 2, 2, 256)    262400      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 2, 2, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 2, 2, 256)    590080      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 2, 2, 256)    0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 2, 2, 1024)   263168      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 2, 2, 1024)   0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 2, 2, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 2, 2, 256)    262400      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 2, 2, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 2, 2, 256)    590080      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 2, 2, 256)    0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 2, 2, 1024)   263168      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 2, 2, 1024)   0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 2, 2, 1024)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 1, 1, 512)    524800      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 1, 1, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 1, 1, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 1, 1, 512)    2359808     activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 1, 1, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 1, 1, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 1, 1, 2048)   1050624     activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 1, 1, 2048)   2099200     activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 1, 1, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 1, 1, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 1, 1, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 1, 1, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 1, 1, 512)    1049088     activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 1, 1, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 1, 1, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 1, 1, 512)    2359808     activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 1, 1, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 1, 1, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 1, 1, 2048)   1050624     activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 1, 1, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 1, 1, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 1, 1, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 1, 1, 512)    1049088     activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 1, 1, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 1, 1, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 1, 1, 512)    2359808     activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 1, 1, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 1, 1, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 1, 1, 2048)   1050624     activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 1, 1, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 1, 1, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 1, 1, 2048)   0           add_16[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNet50(weights = 'imagenet', input_shape = (32, 32, 3), include_top = False)\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "8050809366644f4adab48f9313b2e25a3fcf1c08"
   },
   "outputs": [],
   "source": [
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "5cc08afc652d3f3c260743de416789081f4cb66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 1, 1, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 24,146,817\n",
      "Trainable params: 558,337\n",
      "Non-trainable params: 23,588,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model = Sequential()\n",
    "resnet_model.add(resnet)\n",
    "resnet_model.add(Flatten())\n",
    "resnet_model.add(Dense(256, activation = 'relu'))\n",
    "resnet_model.add(BatchNormalization())\n",
    "resnet_model.add(Dropout(0.5))\n",
    "resnet_model.add(Dense(128, activation = 'relu'))\n",
    "resnet_model.add(BatchNormalization())\n",
    "resnet_model.add(Dropout(0.5))\n",
    "resnet_model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "cfc46699cf2ab754df6e729d43013e6e0e728dfe"
   },
   "outputs": [],
   "source": [
    "#compile the model\n",
    "resnet_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "0b404766f1fdf9bed46bcdc0ec93f51b7d3449be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "17500/17500 [==============================] - 10s 574us/step - loss: 0.1821 - acc: 0.9312\n",
      "Epoch 2/500\n",
      "17500/17500 [==============================] - 5s 298us/step - loss: 0.1203 - acc: 0.9552\n",
      "Epoch 3/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0996 - acc: 0.9625\n",
      "Epoch 4/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0987 - acc: 0.9617\n",
      "Epoch 5/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0914 - acc: 0.9627\n",
      "Epoch 6/500\n",
      "17500/17500 [==============================] - 5s 299us/step - loss: 0.0881 - acc: 0.9666\n",
      "Epoch 7/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0886 - acc: 0.9674\n",
      "Epoch 8/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0824 - acc: 0.9683\n",
      "Epoch 9/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0769 - acc: 0.9713\n",
      "Epoch 10/500\n",
      "17500/17500 [==============================] - 5s 293us/step - loss: 0.0718 - acc: 0.9721\n",
      "Epoch 11/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0688 - acc: 0.9737\n",
      "Epoch 12/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0660 - acc: 0.9747\n",
      "Epoch 13/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0658 - acc: 0.9750\n",
      "Epoch 14/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0670 - acc: 0.9743\n",
      "Epoch 15/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0645 - acc: 0.9766\n",
      "Epoch 16/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0617 - acc: 0.9771\n",
      "Epoch 17/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0586 - acc: 0.9785\n",
      "Epoch 18/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0560 - acc: 0.9789\n",
      "Epoch 19/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0584 - acc: 0.9782\n",
      "Epoch 20/500\n",
      "17500/17500 [==============================] - 5s 293us/step - loss: 0.0611 - acc: 0.9762\n",
      "Epoch 21/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0542 - acc: 0.9798\n",
      "Epoch 22/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0596 - acc: 0.9769\n",
      "Epoch 23/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0545 - acc: 0.9807\n",
      "Epoch 24/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0498 - acc: 0.9818\n",
      "Epoch 25/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0532 - acc: 0.9797\n",
      "Epoch 26/500\n",
      "17500/17500 [==============================] - 5s 301us/step - loss: 0.0511 - acc: 0.9816\n",
      "Epoch 27/500\n",
      "17500/17500 [==============================] - 6s 326us/step - loss: 0.0536 - acc: 0.9803\n",
      "Epoch 28/500\n",
      "17500/17500 [==============================] - 5s 312us/step - loss: 0.0502 - acc: 0.9813\n",
      "Epoch 29/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0491 - acc: 0.9823\n",
      "Epoch 30/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0485 - acc: 0.9818\n",
      "Epoch 31/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0461 - acc: 0.9831\n",
      "Epoch 32/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0423 - acc: 0.9842\n",
      "Epoch 33/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0476 - acc: 0.9832\n",
      "Epoch 34/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0431 - acc: 0.9836\n",
      "Epoch 35/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0428 - acc: 0.9842\n",
      "Epoch 36/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0466 - acc: 0.9827\n",
      "Epoch 37/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0439 - acc: 0.9844\n",
      "Epoch 38/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0403 - acc: 0.9850\n",
      "Epoch 39/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0407 - acc: 0.9847\n",
      "Epoch 40/500\n",
      "17500/17500 [==============================] - 5s 297us/step - loss: 0.0430 - acc: 0.9846\n",
      "Epoch 41/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0402 - acc: 0.9855\n",
      "Epoch 42/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0366 - acc: 0.9869\n",
      "Epoch 43/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0399 - acc: 0.9857\n",
      "Epoch 44/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0408 - acc: 0.9838\n",
      "Epoch 45/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0391 - acc: 0.9856\n",
      "Epoch 46/500\n",
      "17500/17500 [==============================] - 6s 319us/step - loss: 0.0378 - acc: 0.9862\n",
      "Epoch 47/500\n",
      "17500/17500 [==============================] - 5s 301us/step - loss: 0.0400 - acc: 0.9851\n",
      "Epoch 48/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0375 - acc: 0.9865\n",
      "Epoch 49/500\n",
      "17500/17500 [==============================] - 5s 293us/step - loss: 0.0368 - acc: 0.9874\n",
      "Epoch 50/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0420 - acc: 0.9849\n",
      "Epoch 51/500\n",
      "17500/17500 [==============================] - 5s 297us/step - loss: 0.0394 - acc: 0.9851\n",
      "Epoch 52/500\n",
      "17500/17500 [==============================] - 5s 297us/step - loss: 0.0363 - acc: 0.9873\n",
      "Epoch 53/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0351 - acc: 0.9863\n",
      "Epoch 54/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0323 - acc: 0.9881\n",
      "Epoch 55/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0395 - acc: 0.9861\n",
      "Epoch 56/500\n",
      "17500/17500 [==============================] - 5s 299us/step - loss: 0.0378 - acc: 0.9872\n",
      "Epoch 57/500\n",
      "17500/17500 [==============================] - 5s 297us/step - loss: 0.0376 - acc: 0.9866\n",
      "Epoch 58/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0332 - acc: 0.9883\n",
      "Epoch 59/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0351 - acc: 0.9881\n",
      "Epoch 60/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0341 - acc: 0.9888\n",
      "Epoch 61/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0381 - acc: 0.9850\n",
      "Epoch 62/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0372 - acc: 0.9881\n",
      "Epoch 63/500\n",
      "17500/17500 [==============================] - 5s 297us/step - loss: 0.0328 - acc: 0.9874\n",
      "Epoch 64/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0323 - acc: 0.9883\n",
      "Epoch 65/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0340 - acc: 0.9878\n",
      "Epoch 66/500\n",
      "17500/17500 [==============================] - 5s 297us/step - loss: 0.0295 - acc: 0.9891\n",
      "Epoch 67/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0342 - acc: 0.9878\n",
      "Epoch 68/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0318 - acc: 0.9885\n",
      "Epoch 69/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0345 - acc: 0.9873\n",
      "Epoch 70/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0301 - acc: 0.9895\n",
      "Epoch 71/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0321 - acc: 0.9880\n",
      "Epoch 72/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0295 - acc: 0.9894\n",
      "Epoch 73/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0318 - acc: 0.9883\n",
      "Epoch 74/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0308 - acc: 0.9890\n",
      "Epoch 75/500\n",
      "17500/17500 [==============================] - 5s 298us/step - loss: 0.0296 - acc: 0.9898\n",
      "Epoch 76/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0274 - acc: 0.9899\n",
      "Epoch 77/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0292 - acc: 0.9902\n",
      "Epoch 78/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0267 - acc: 0.9902\n",
      "Epoch 79/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0295 - acc: 0.9897\n",
      "Epoch 80/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0309 - acc: 0.9894\n",
      "Epoch 81/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0294 - acc: 0.9901\n",
      "Epoch 82/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0276 - acc: 0.9905\n",
      "Epoch 83/500\n",
      "17500/17500 [==============================] - 5s 299us/step - loss: 0.0308 - acc: 0.9891\n",
      "Epoch 84/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0263 - acc: 0.9903\n",
      "Epoch 85/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0292 - acc: 0.9899\n",
      "Epoch 86/500\n",
      "17500/17500 [==============================] - 5s 307us/step - loss: 0.0277 - acc: 0.9895\n",
      "Epoch 87/500\n",
      "17500/17500 [==============================] - 6s 325us/step - loss: 0.0271 - acc: 0.9901\n",
      "Epoch 88/500\n",
      "17500/17500 [==============================] - 5s 306us/step - loss: 0.0263 - acc: 0.9910\n",
      "Epoch 89/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0280 - acc: 0.9900\n",
      "Epoch 90/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0280 - acc: 0.9895\n",
      "Epoch 91/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0262 - acc: 0.9907\n",
      "Epoch 92/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0290 - acc: 0.9902\n",
      "Epoch 93/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0278 - acc: 0.9899\n",
      "Epoch 94/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0264 - acc: 0.9902\n",
      "Epoch 95/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0268 - acc: 0.9905\n",
      "Epoch 96/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0266 - acc: 0.9908\n",
      "Epoch 97/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0299 - acc: 0.9896\n",
      "Epoch 98/500\n",
      "17500/17500 [==============================] - 5s 298us/step - loss: 0.0242 - acc: 0.9924\n",
      "Epoch 99/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0305 - acc: 0.9905\n",
      "Epoch 100/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0224 - acc: 0.9920\n",
      "Epoch 101/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0268 - acc: 0.9901\n",
      "Epoch 102/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0269 - acc: 0.9899\n",
      "Epoch 103/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0261 - acc: 0.9916\n",
      "Epoch 104/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0245 - acc: 0.9914\n",
      "Epoch 105/500\n",
      "17500/17500 [==============================] - 6s 325us/step - loss: 0.0243 - acc: 0.9916\n",
      "Epoch 106/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0239 - acc: 0.9911\n",
      "Epoch 107/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0211 - acc: 0.9924\n",
      "Epoch 108/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0250 - acc: 0.9908\n",
      "Epoch 109/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0225 - acc: 0.9919\n",
      "Epoch 110/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0283 - acc: 0.9908\n",
      "Epoch 111/500\n",
      "17500/17500 [==============================] - 5s 293us/step - loss: 0.0264 - acc: 0.9906\n",
      "Epoch 112/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0244 - acc: 0.9921\n",
      "Epoch 113/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0251 - acc: 0.9917\n",
      "Epoch 114/500\n",
      "17500/17500 [==============================] - 5s 297us/step - loss: 0.0196 - acc: 0.9936\n",
      "Epoch 115/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0235 - acc: 0.9916\n",
      "Epoch 116/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0221 - acc: 0.9921\n",
      "Epoch 117/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0237 - acc: 0.9910\n",
      "Epoch 118/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0222 - acc: 0.9909\n",
      "Epoch 119/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0226 - acc: 0.9925\n",
      "Epoch 120/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0205 - acc: 0.9930\n",
      "Epoch 121/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0219 - acc: 0.9922\n",
      "Epoch 122/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0243 - acc: 0.9910\n",
      "Epoch 123/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0238 - acc: 0.9915\n",
      "Epoch 124/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0213 - acc: 0.9919\n",
      "Epoch 125/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0238 - acc: 0.9914\n",
      "Epoch 126/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0214 - acc: 0.9926\n",
      "Epoch 127/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0218 - acc: 0.9925\n",
      "Epoch 128/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0243 - acc: 0.9915\n",
      "Epoch 129/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0217 - acc: 0.9921\n",
      "Epoch 130/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0244 - acc: 0.9916\n",
      "Epoch 131/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0232 - acc: 0.9922\n",
      "Epoch 132/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0210 - acc: 0.9928\n",
      "Epoch 133/500\n",
      "17500/17500 [==============================] - 5s 296us/step - loss: 0.0210 - acc: 0.9925\n",
      "Epoch 134/500\n",
      "17500/17500 [==============================] - 5s 294us/step - loss: 0.0230 - acc: 0.9917\n",
      "Epoch 135/500\n",
      "17500/17500 [==============================] - 5s 295us/step - loss: 0.0209 - acc: 0.9927\n",
      "Epoch 136/500\n",
      " 1984/17500 [==>...........................] - ETA: 4s - loss: 0.0189 - acc: 0.9940"
     ]
    }
   ],
   "source": [
    "#fit the model on our data\n",
    "resnet_history = resnet_model.fit(X, y,\n",
    "                                  batch_size = 64, \n",
    "                                  epochs = 500,\n",
    "                                  verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "067a2fcc93b2d735675e1509d74d70c2fed8db61"
   },
   "outputs": [],
   "source": [
    "resnet_predictions = resnet_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3b493df9c409606f80ef0ddd230e75be5495a6e"
   },
   "source": [
    "## MAKING SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bfafa9561be723a07d10adc6b3b8119b2066c447"
   },
   "source": [
    "1. Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "f5febc9497bfd57c5eec8c17593a8898220db36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "#making predictions\n",
    "prediction = np.hstack([p.reshape(-1,1) for p in prediction_scores.values()]) #taking the scores of all the trained models\n",
    "predictions_ensemble = np.mean(prediction, axis = 1)\n",
    "print(predictions_ensemble.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "39489c88e66a3580eea77c49c272bb812c685785"
   },
   "outputs": [],
   "source": [
    "df_ensemble = pd.DataFrame(predictions_ensemble, columns = ['has_cactus'])\n",
    "df_ensemble['has_cactus'] = df_ensemble['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "67d1dc60d27d6b4e7718d1955584eef95a5ec83e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "df_ensemble['id'] = ''\n",
    "cols = df_ensemble.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_ensemble = df_ensemble[cols]\n",
    "\n",
    "for i, img in enumerate(test_images):\n",
    "    df_ensemble.set_value(i,'id',img)\n",
    "\n",
    "#making submission\n",
    "df_ensemble.to_csv('ensemble_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f3c6706028aeddb58e21501560b109ba422969d"
   },
   "source": [
    "2. VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "bbc6e6b80af6c869d565f6da7d8ad67b5b1e97eb"
   },
   "outputs": [],
   "source": [
    "df_vgg = pd.DataFrame(predictions_vgg, columns = ['has_cactus'])\n",
    "df_vgg['has_cactus'] = df_vgg['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "65c26995d8a74f430c7dabbd8d6e836b0f95d61d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "df_vgg['id'] = ''\n",
    "cols = df_vgg.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_vgg = df_vgg[cols]\n",
    "\n",
    "for i, img in enumerate(test_images):\n",
    "    df_vgg.set_value(i,'id',img)\n",
    "\n",
    "#making submission\n",
    "df_vgg.to_csv('vgg_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "f7d0d6d67bf961f725cc1dfcf8815d47eeac498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c662bde123f0f83b3caae0ffda237a93.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9553eed7793d4cf88b5226d446d93dae.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19f059a7ce41b25be1548bc4049b45ec.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fb4f464486f4894330273346ce939252.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b52558a522db6ec2501ae188b6d6e526.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  has_cactus\n",
       "0  c662bde123f0f83b3caae0ffda237a93.jpg           1\n",
       "1  9553eed7793d4cf88b5226d446d93dae.jpg           0\n",
       "2  19f059a7ce41b25be1548bc4049b45ec.jpg           1\n",
       "3  fb4f464486f4894330273346ce939252.jpg           1\n",
       "4  b52558a522db6ec2501ae188b6d6e526.jpg           1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vgg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d075c633d351290efbccc01b917a9e804ebcead7"
   },
   "source": [
    "3. Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "7f453d154986313bb381fdb0a5811836f163faa0"
   },
   "outputs": [],
   "source": [
    "df_resnet = pd.DataFrame(resnet_predictions, columns = ['has_cactus'])\n",
    "df_resnet['has_cactus'] = df_resnet['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "f47f0ada4f2e32a666253d5ce1896a205d6f4039"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "df_resnet['id'] = ''\n",
    "cols = df_resnet.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_resnet = df_resnet[cols]\n",
    "\n",
    "for i, img in enumerate(test_images):\n",
    "    df_resnet.set_value(i,'id',img)\n",
    "\n",
    "#making submission\n",
    "df_resnet.to_csv('resnet_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "397876aebeda9b4062fb1cca48a04a3d198004b6"
   },
   "source": [
    "4. Ensemble and VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "64810050d54d134bbb64fb355c840a219668c28a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_vgg1 = pd.DataFrame(predictions_vgg, columns = ['has_cactus'])\n",
    "df_ensemble1 = pd.DataFrame(predictions_ensemble, columns = ['has_cactus'])\n",
    "\n",
    "df_t = 0.5 * df_vgg1['has_cactus'] + 0.5 * df_ensemble1['has_cactus']\n",
    "df_t = pd.DataFrame(df_t, columns = ['has_cactus'])\n",
    "df_t['has_cactus'] = df_t['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)\n",
    "\n",
    "df_t['id'] = ''\n",
    "cols = df_t.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_t = df_t[cols]\n",
    "\n",
    "for i, img in enumerate(test_images):\n",
    "    df_t.set_value(i,'id',img)\n",
    "\n",
    "#making submission\n",
    "df_t.to_csv('vgg_ensemble_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90e53c5d630ea5f2c24aac30187ab290a8d5acf2"
   },
   "source": [
    "5. Ensemble, VGG16 and ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "9fdb8d40e2c2bc314088cb6a82a38b29a90c11d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df_vgg2 = pd.DataFrame(predictions_vgg, columns = ['has_cactus'])\n",
    "df_ensemble2 = pd.DataFrame(predictions_ensemble, columns = ['has_cactus'])\n",
    "df_resnet2 = pd.DataFrame(resnet_predictions, columns = ['has_cactus'])\n",
    "\n",
    "df_t2 = 0.45 * df_vgg2['has_cactus'] + 0.45 * df_ensemble2['has_cactus'] + 0.10 * df_resnet2['has_cactus']\n",
    "df_t2 = pd.DataFrame(df_t2, columns = ['has_cactus'])\n",
    "df_t2['has_cactus'] = df_t2['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)\n",
    "\n",
    "df_t2['id'] = ''\n",
    "cols = df_t2.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_t2 = df_t2[cols]\n",
    "\n",
    "for i, img in enumerate(test_images):\n",
    "    df_t2.set_value(i,'id',img)\n",
    "\n",
    "#making submission\n",
    "df_t2.to_csv('vgg_ensemble_resnet_submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
