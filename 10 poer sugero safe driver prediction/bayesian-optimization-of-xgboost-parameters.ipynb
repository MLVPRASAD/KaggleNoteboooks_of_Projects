{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to install the superb [__Bayesian Optimization__](https://github.com/fmfn/BayesianOptimization) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "242f8c7e-4e80-4b1c-ae28-07b49a55145b",
    "_uuid": "d1eab8b46d36b46fecb475ef17a9806d98d8d770"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This line is needed for python 2.7 ; probably not for python 3\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be used to capture stderr and stdout without having anything print on screen.\n",
    "\n",
    "**It turns out that Kaggle does not have \"cStringIO\", so I will comment out this portion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@contextlib.contextmanager\n",
    "#def capture():\n",
    "#    import sys\n",
    "#    from cStringIO import StringIO\n",
    "#    olderr, oldout = sys.stderr, sys.stdout\n",
    "#    try:\n",
    "#        out=[StringIO(), StringIO()]\n",
    "#        sys.stderr,sys.stdout = out\n",
    "#        yield out\n",
    "#    finally:\n",
    "#        sys.stderr,sys.stdout = olderr,oldout\n",
    "#        out[0] = out[0].getvalue().splitlines()\n",
    "#        out[1] = out[1].getvalue().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is really not needed for XGBoost, but I leave it here in case if you do the optimization using ML approaches that need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../input/train.csv'\n",
    "DATA_TEST_PATH = '../input/test.csv'\n",
    "\n",
    "def load_data(path_train=DATA_TRAIN_PATH, path_test=DATA_TEST_PATH):\n",
    "    train_loader = pd.read_csv(path_train, dtype={'target': np.int8, 'id': np.int32})\n",
    "    train = train_loader.drop(['target', 'id'], axis=1)\n",
    "    train_labels = train_loader['target'].values\n",
    "    train_ids = train_loader['id'].values\n",
    "    print('\\n Shape of raw train data:', train.shape)\n",
    "\n",
    "    test_loader = pd.read_csv(path_test, dtype={'id': np.int32})\n",
    "    test = test_loader.drop(['id'], axis=1)\n",
    "    test_ids = test_loader['id'].values\n",
    "    print(' Shape of raw test data:', test.shape)\n",
    "\n",
    "    return train, train_labels, test, train_ids, test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cross-validation variables that are used for parameter search. Each parameter has its own line, so it is easy to comment something out if you wish. Keep in mind that in such a case you must comment out the matching lines in optimization and explore sections below.\n",
    "\n",
    "*Note that the learning rate (\"eta\") is set to 0.1 below. That is certainly not optimal, but it will make the search go faster. You will probably want to experiment with values in 0.01-0.05 range, but beware that it will significantly slow down the process because more iterations will be required to get to early stopping. Doing 10-fold instead of 5-fold cross-validation will also result in a small gain, but will double the search time.*\n",
    "\n",
    "XGBoost outputs lots of interesting info, but it is not very helpful and clutters the screen when doing grid search. So we will run XGboost CV with verbose turned on, but will capture stderr in result[0] and stdout in result[1]. We will extract the relevant info from these variables later, and will print the record of each CV run into a log file.\n",
    "\n",
    "AUC will be optimized here. We can go with separately defined gini scorer and use **feval=gini** below but I don't think it makes any difference because AUC and gini are directly correlated.\n",
    "\n",
    "**Commenting out the capture so there will be no record of xgb.cv in log file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comment out any parameter you don't want to test\n",
    "def XGB_CV(\n",
    "          max_depth,\n",
    "          gamma,\n",
    "          min_child_weight,\n",
    "          max_delta_step,\n",
    "          subsample,\n",
    "          colsample_bytree\n",
    "         ):\n",
    "\n",
    "    global AUCbest\n",
    "    global ITERbest\n",
    "\n",
    "#\n",
    "# Define all XGboost parameters\n",
    "#\n",
    "\n",
    "    paramt = {\n",
    "              'booster' : 'gbtree',\n",
    "              'max_depth' : int(max_depth),\n",
    "              'gamma' : gamma,\n",
    "              'eta' : 0.1,\n",
    "              'objective' : 'binary:logistic',\n",
    "              'nthread' : 4,\n",
    "              'silent' : True,\n",
    "              'eval_metric': 'auc',\n",
    "              'subsample' : max(min(subsample, 1), 0),\n",
    "              'colsample_bytree' : max(min(colsample_bytree, 1), 0),\n",
    "              'min_child_weight' : min_child_weight,\n",
    "              'max_delta_step' : int(max_delta_step),\n",
    "              'seed' : 1001\n",
    "              }\n",
    "\n",
    "    folds = 5\n",
    "    cv_score = 0\n",
    "\n",
    "    print(\"\\n Search parameters (%d-fold validation):\\n %s\" % (folds, paramt), file=log_file )\n",
    "    log_file.flush()\n",
    "\n",
    "    xgbc = xgb.cv(\n",
    "                    paramt,\n",
    "                    dtrain,\n",
    "                    num_boost_round = 20000,\n",
    "                    stratified = True,\n",
    "                    nfold = folds,\n",
    "#                    verbose_eval = 10,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    metrics = 'auc',\n",
    "                    show_stdv = True\n",
    "               )\n",
    "\n",
    "# This line would have been on top of this section\n",
    "#    with capture() as result:\n",
    "\n",
    "# After xgb.cv is done, this section puts its output into log file. Train and validation scores \n",
    "# are also extracted in this section. Note the \"diff\" part in the printout below, which is the \n",
    "# difference between the two scores. Large diff values may indicate that a particular set of \n",
    "# parameters is overfitting, especially if you check the CV portion of it in the log file and find \n",
    "# out that train scores were improving much faster than validation scores.\n",
    "\n",
    "#    print('', file=log_file)\n",
    "#    for line in result[1]:\n",
    "#        print(line, file=log_file)\n",
    "#    log_file.flush()\n",
    "\n",
    "    val_score = xgbc['test-auc-mean'].iloc[-1]\n",
    "    train_score = xgbc['train-auc-mean'].iloc[-1]\n",
    "    print(' Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % ( len(xgbc), train_score, val_score, (train_score - val_score), (train_score*2-1),\n",
    "(val_score*2-1)) )\n",
    "    if ( val_score > AUCbest ):\n",
    "        AUCbest = val_score\n",
    "        ITERbest = len(xgbc)\n",
    "\n",
    "    return (val_score*2) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"real\" code starts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Shape of raw train data: (595212, 57)\n",
      " Shape of raw test data: (892816, 57)\n",
      "\n",
      " Shape of processed train data: (595212, 227)\n",
      " Shape of processed test data: (892816, 227)\n"
     ]
    }
   ],
   "source": [
    "# Define the log file. If you repeat this run, new output will be added to it\n",
    "log_file = open('Porto-AUC-5fold-XGB-run-01-v1-full.log', 'a')\n",
    "AUCbest = -1.\n",
    "ITERbest = 0\n",
    "\n",
    "# Load data set and target values\n",
    "train, target, test, tr_ids, te_ids = load_data()\n",
    "n_train = train.shape[0]\n",
    "train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "col_to_drop = train.columns[train.columns.str.endswith('_cat')]\n",
    "col_to_dummify = train.columns[train.columns.str.endswith('_cat')].astype(str).tolist()\n",
    "\n",
    "for col in col_to_dummify:\n",
    "    dummy = pd.get_dummies(train_test[col].astype('category'))\n",
    "    columns = dummy.columns.astype(str).tolist()\n",
    "    columns = [col + '_' + w for w in columns]\n",
    "    dummy.columns = columns\n",
    "    train_test = pd.concat((train_test, dummy), axis=1)\n",
    "\n",
    "train_test.drop(col_to_dummify, axis=1, inplace=True)\n",
    "train_test_scaled, scaler = scale_data(train_test)\n",
    "train = train_test_scaled[:n_train, :]\n",
    "test = train_test_scaled[n_train:, :]\n",
    "print('\\n Shape of processed train data:', train.shape)\n",
    "print(' Shape of processed test data:', test.shape)\n",
    "\n",
    "# We really didn't need to load the test data in the first place unless you are planning to make\n",
    "# a prediction at the end of this run.\n",
    "# del test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am doing a stratified split and using only 25% of the data. Obviously, this is done to make sure that this notebook can run to completion on Kaggle. In a production version, you should uncomment the first line in the section below, and comment out or delete everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dtrain = xgb.DMatrix(train, label = target)\n",
    "\n",
    "sss = StratifiedShuffleSplit(target, random_state=1001, test_size=0.75)\n",
    "for train_index, test_index in sss:\n",
    "    break\n",
    "X_train, y_train = train[train_index], target[train_index]\n",
    "del train, target\n",
    "gc.collect()\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters and their ranges that will be used during optimization. They must match the parameters that are passed above to the XGB_CV function. If you commented out any of them above, you should do the same here. Note that these are pretty wide ranges for most parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XGB_BO = BayesianOptimization(XGB_CV, {\n",
    "                                     'max_depth': (2, 12),\n",
    "                                     'gamma': (0.001, 10.0),\n",
    "                                     'min_child_weight': (0, 20),\n",
    "                                     'max_delta_step': (0, 10),\n",
    "                                     'subsample': (0.4, 1.0),\n",
    "                                     'colsample_bytree' :(0.4, 1.0)\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the code is not necessary. You can simply specify that 10-20 random parameter combinations (**init_points** below) be used. However, I like to try couple of high- and low-end values for each parameter as a starting point, and after that fewer random points are needed. Note that a number of options must be the same for each parameter, and they are applied vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XGB_BO.explore({\n",
    "              'max_depth':            [3, 8, 3, 8, 8, 3, 8, 3],\n",
    "              'gamma':                [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n",
    "              'min_child_weight':     [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n",
    "              'max_delta_step':       [1, 2, 2, 1, 2, 1, 1, 2],\n",
    "              'subsample':            [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              'colsample_bytree':     [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my version of sklearn there are many warning thrown out by the GP portion of this code. This is set to prevent them from showing on screen.\n",
    "\n",
    "If you have a special relationship with your computer and want to know everything it is saying back, you'd probably want to remove the two \"warnings\" lines and slide the XGB_BO line all the way left.\n",
    "\n",
    "I am doing only 2 initial points, which along with 8 exploratory points above makes it 10 \"random\" parameter combinations. I'd say that 15-20 is usually adequate. For n_iter 25-50 is usually enough.\n",
    "\n",
    "There are several commented out maximize lines that could be worth exploring. The exact combination of parameters determines **[exploitation vs. exploration](https://github.com/fmfn/BayesianOptimization/blob/master/examples/exploitation%20vs%20exploration.ipynb)**. It is tough to know which would work better without actually trying, though in my hands exploitation with \"expected improvement\" usually works the best. That's what the XGB_BO.maximize line below is specifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_delta_step |   max_depth |   min_child_weight |   subsample | \n",
      " Stopped after 143 iterations with train-auc = 0.682293 val-auc = 0.632955 ( diff = 0.049338 ) train-gini = 0.364586 val-gini = 0.265910\n",
      "    1 | 02m54s | \u001b[35m   0.26591\u001b[0m | \u001b[32m            0.6000\u001b[0m | \u001b[32m   0.5000\u001b[0m | \u001b[32m          1.0000\u001b[0m | \u001b[32m     3.0000\u001b[0m | \u001b[32m            0.2000\u001b[0m | \u001b[32m     0.6000\u001b[0m | \n",
      " Stopped after 62 iterations with train-auc = 0.759147 val-auc = 0.628424 ( diff = 0.130724 ) train-gini = 0.518295 val-gini = 0.256847\n",
      "    2 | 04m42s |    0.25685 |             0.8000 |    8.0000 |           2.0000 |      8.0000 |             0.2000 |      0.8000 | \n",
      " Stopped after 110 iterations with train-auc = 0.675709 val-auc = 0.632341 ( diff = 0.043368 ) train-gini = 0.351418 val-gini = 0.264682\n",
      "    3 | 02m33s |    0.26468 |             0.6000 |    0.2000 |           2.0000 |      3.0000 |             0.2000 |      0.6000 | \n",
      " Stopped after 82 iterations with train-auc = 0.732382 val-auc = 0.632923 ( diff = 0.099459 ) train-gini = 0.464764 val-gini = 0.265846\n",
      "    4 | 05m17s |    0.26585 |             0.8000 |    9.0000 |           1.0000 |      8.0000 |             0.2000 |      0.8000 | \n",
      " Stopped after 43 iterations with train-auc = 0.750511 val-auc = 0.627800 ( diff = 0.122711 ) train-gini = 0.501022 val-gini = 0.255601\n",
      "    5 | 03m54s |    0.25560 |             0.6000 |    0.5000 |           2.0000 |      8.0000 |            12.0000 |      0.6000 | \n",
      " Stopped after 131 iterations with train-auc = 0.665860 val-auc = 0.633802 ( diff = 0.032058 ) train-gini = 0.331719 val-gini = 0.267604\n",
      "    6 | 02m51s | \u001b[35m   0.26760\u001b[0m | \u001b[32m            0.8000\u001b[0m | \u001b[32m   8.0000\u001b[0m | \u001b[32m          1.0000\u001b[0m | \u001b[32m     3.0000\u001b[0m | \u001b[32m           12.0000\u001b[0m | \u001b[32m     0.8000\u001b[0m | \n",
      " Stopped after 66 iterations with train-auc = 0.778898 val-auc = 0.623904 ( diff = 0.154994 ) train-gini = 0.557796 val-gini = 0.247809\n",
      "    7 | 04m13s |    0.24781 |             0.6000 |    0.2000 |           1.0000 |      8.0000 |            12.0000 |      0.6000 | \n",
      " Stopped after 177 iterations with train-auc = 0.666864 val-auc = 0.634059 ( diff = 0.032806 ) train-gini = 0.333729 val-gini = 0.268118\n",
      "    8 | 03m22s | \u001b[35m   0.26812\u001b[0m | \u001b[32m            0.8000\u001b[0m | \u001b[32m   9.0000\u001b[0m | \u001b[32m          2.0000\u001b[0m | \u001b[32m     3.0000\u001b[0m | \u001b[32m           12.0000\u001b[0m | \u001b[32m     0.8000\u001b[0m | \n",
      " Stopped after 44 iterations with train-auc = 0.793774 val-auc = 0.625044 ( diff = 0.168731 ) train-gini = 0.587549 val-gini = 0.250088\n",
      "    9 | 05m16s |    0.25009 |             0.6735 |    2.0900 |           8.3846 |     11.5866 |            14.1570 |      0.6837 | \n",
      " Stopped after 96 iterations with train-auc = 0.679450 val-auc = 0.632742 ( diff = 0.046708 ) train-gini = 0.358900 val-gini = 0.265484\n",
      "   10 | 02m15s |    0.26548 |             0.6166 |    7.1624 |           1.1996 |      4.2885 |             4.8484 |      0.8939 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_delta_step |   max_depth |   min_child_weight |   subsample | \n",
      " Stopped after 92 iterations with train-auc = 0.646113 val-auc = 0.631452 ( diff = 0.014662 ) train-gini = 0.292226 val-gini = 0.262903\n",
      "   11 | 01m54s |    0.26290 |             0.4000 |    0.0010 |          10.0000 |      2.0000 |            20.0000 |      0.4000 | \n",
      " Stopped after 131 iterations with train-auc = 0.656160 val-auc = 0.632474 ( diff = 0.023686 ) train-gini = 0.312320 val-gini = 0.264948\n",
      "   12 | 02m46s |    0.26495 |             0.9805 |    0.2672 |           8.2532 |      2.3673 |            11.4080 |      0.4340 | \n",
      " Stopped after 317 iterations with train-auc = 0.656083 val-auc = 0.631757 ( diff = 0.024326 ) train-gini = 0.312166 val-gini = 0.263513\n",
      "   13 | 02m57s |    0.26351 |             0.4291 |    9.9392 |           0.7272 |      2.0804 |             0.3626 |      0.4178 | \n",
      " Stopped after 113 iterations with train-auc = 0.691003 val-auc = 0.633145 ( diff = 0.057857 ) train-gini = 0.382005 val-gini = 0.266291\n",
      "   14 | 05m08s |    0.26629 |             0.4244 |    9.9293 |           3.0611 |     11.4499 |            19.1442 |      0.6561 | \n",
      " Stopped after 81 iterations with train-auc = 0.689061 val-auc = 0.633094 ( diff = 0.055967 ) train-gini = 0.378123 val-gini = 0.266188\n",
      "   15 | 05m01s |    0.26619 |             0.6321 |    9.9996 |           0.1717 |      9.1506 |             7.6553 |      0.4015 | \n"
     ]
    }
   ],
   "source": [
    "print('-'*130)\n",
    "print('-'*130, file=log_file)\n",
    "log_file.flush()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    XGB_BO.maximize(init_points=2, n_iter=5, acq='ei', xi=0.0)\n",
    "\n",
    "# XGB_BO.maximize(init_points=10, n_iter=50, acq='ei', xi=0.0)\n",
    "# XGB_BO.maximize(init_points=10, n_iter=50, acq='ei', xi=0.01)\n",
    "# XGB_BO.maximize(init_points=10, n_iter=50, acq='ucb', kappa=10)\n",
    "# XGB_BO.maximize(init_points=10, n_iter=50, acq='ucb', kappa=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portions gives the summary and creates a CSV file with results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Final Results\n",
      "Maximum XGBOOST value: 0.268118\n",
      "Best XGBOOST parameters:  {'max_depth': 3.0, 'gamma': 9.0, 'min_child_weight': 12.0, 'max_delta_step': 2.0, 'subsample': 0.80000000000000004, 'colsample_bytree': 0.80000000000000004}\n"
     ]
    }
   ],
   "source": [
    "print('-'*130)\n",
    "print('Final Results')\n",
    "print('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'])\n",
    "print('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'])\n",
    "print('-'*130, file=log_file)\n",
    "print('Final Result:', file=log_file)\n",
    "print('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'], file=log_file)\n",
    "print('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'], file=log_file)\n",
    "log_file.flush()\n",
    "log_file.close()\n",
    "\n",
    "history_df = pd.DataFrame(XGB_BO.res['all']['params'])\n",
    "history_df2 = pd.DataFrame(XGB_BO.res['all']['values'])\n",
    "history_df = pd.concat((history_df, history_df2), axis=1)\n",
    "history_df.rename(columns = { 0 : 'gini'}, inplace=True)\n",
    "history_df['AUC'] = ( history_df['gini'] + 1 ) / 2\n",
    "history_df.to_csv('Porto-AUC-5fold-XGB-run-01-v1-grid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck! Let me know how it works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
