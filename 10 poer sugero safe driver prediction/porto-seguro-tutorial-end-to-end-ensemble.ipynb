{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "288e711f-b3ff-422e-a480-f4d7d15ddf03",
    "_uuid": "d0673b9bd61398c6fe3d238710fd4ad725db7df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note to others\n",
    "\n",
    "This is the second of a series of notebooks that I am working on for educational purpose, to demonstrate some run-of-the mill techniques we use on Kaggle for a student audience.\n",
    "\n",
    "So when most folks here are competiting to overfit the LB, we are doing some small effort to fill our students with handy knowledge and the virtue of solid CV. \n",
    "\n",
    "Anyhow, since our students are also competing as teams in this competition, we decided to use the kernel facility as a way to share knowledge. Nevertheless, all comments are welcome, and let's all enjoy the last days of this competition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porto Seguo - End-to-end Ensemble\n",
    "\n",
    "In this competition we are tasked with making predictive models that can predict if a given driver will make insurance claim. In a [\"previous kernel\"](https://www.kaggle.com/yifanxie/porto-seguro-tutorial-simple-e2e-pipeline) we have breifly explored the data, did some useful categorical feature encoding, and presented a simple model building pipeline.\n",
    "\n",
    "In this kernel, we are going to progress a bit more on the model building aspect. Firstly, we will introduce the technique to generate out-out-fold train and test predictions for several models, we will then use these out-of-fold predictions to be  ensemble model. \n",
    "\n",
    "Strickly speaking, the ensemble method we use here is referred as **Stacked generalization** as very well ilustrated already by the following blog/articles:\n",
    "* [*Kaggle Ensemble Guide*](https://mlwave.com/kaggle-ensembling-guide/) by [Triskelion](https://www.kaggle.com/triskelion)\n",
    "* [*Stacking Made Easy: An Introduction to StackNet*](http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/) by Competitions Grandmaster [Marios Michailidis (KazAnova)](https://www.kaggle.com/kazanova)\n",
    "\n",
    "So without further ado, let's get technical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../input/train.csv')\n",
    "test=pd.read_csv('../input/test.csv')\n",
    "sample_submission=pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Categorical feature encoding and feature reduction\n",
    "The following part is a strict copy & paste from the first kernel, so for detailed explaination please check it out there.\n",
    "\n",
    "## 1.1 Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function late in a list of features 'cols' from train and test dataset, \n",
    "# and performing frequency encoding. \n",
    "def freq_encoding(cols, train_df, test_df):\n",
    "    # we are going to store our new dataset in these two resulting datasets\n",
    "    result_train_df=pd.DataFrame()\n",
    "    result_test_df=pd.DataFrame()\n",
    "    \n",
    "    # loop through each feature column to do this\n",
    "    for col in cols:\n",
    "        \n",
    "        # capture the frequency of a feature in the training set in the form of a dataframe\n",
    "        col_freq=col+'_freq'\n",
    "        freq=train_df[col].value_counts()\n",
    "        freq=pd.DataFrame(freq)\n",
    "        freq.reset_index(inplace=True)\n",
    "        freq.columns=[[col,col_freq]]\n",
    "\n",
    "        # merge ths 'freq' datafarme with the train data\n",
    "        temp_train_df=pd.merge(train_df[[col]], freq, how='left', on=col)\n",
    "        temp_train_df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        # merge this 'freq' dataframe with the test data\n",
    "        temp_test_df=pd.merge(test_df[[col]], freq, how='left', on=col)\n",
    "        temp_test_df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        # if certain levels in the test dataset is not observed in the train dataset, \n",
    "        # we assign frequency of zero to them\n",
    "        temp_test_df.fillna(0, inplace=True)\n",
    "        temp_test_df[col_freq]=temp_test_df[col_freq].astype(np.int32)\n",
    "\n",
    "        if result_train_df.shape[0]==0:\n",
    "            result_train_df=temp_train_df\n",
    "            result_test_df=temp_test_df\n",
    "        else:\n",
    "            result_train_df=pd.concat([result_train_df, temp_train_df],axis=1)\n",
    "            result_test_df=pd.concat([result_test_df, temp_test_df],axis=1)\n",
    "    \n",
    "    return result_train_df, result_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's run the frequency encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols=['ps_ind_02_cat','ps_car_04_cat', 'ps_car_09_cat',\n",
    "          'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_11_cat']\n",
    "\n",
    "# generate dataframe for frequency features for the train and test dataset\n",
    "train_freq, test_freq=freq_encoding(cat_cols, train, test)\n",
    "\n",
    "# merge them into the original train and test dataset\n",
    "train=pd.concat([train, train_freq], axis=1)\n",
    "test=pd.concat([test,test_freq], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform binary encoding for categorical variable\n",
    "# this function take in a pair of train and test data set, and the feature that need to be encode.\n",
    "# it returns the two dataset with input feature encoded in binary representation\n",
    "# this function assumpt that the feature to be encoded is already been encoded in a numeric manner \n",
    "# ranging from 0 to n-1 (n = number of levels in the feature). \n",
    "\n",
    "def binary_encoding(train_df, test_df, feat):\n",
    "    # calculate the highest numerical value used for numeric encoding\n",
    "    train_feat_max = train_df[feat].max()\n",
    "    test_feat_max = test_df[feat].max()\n",
    "    if train_feat_max > test_feat_max:\n",
    "        feat_max = train_feat_max\n",
    "    else:\n",
    "        feat_max = test_feat_max\n",
    "        \n",
    "    # use the value of feat_max+1 to represent missing value\n",
    "    train_df.loc[train_df[feat] == -1, feat] = feat_max + 1\n",
    "    test_df.loc[test_df[feat] == -1, feat] = feat_max + 1\n",
    "    \n",
    "    # create a union set of all possible values of the feature\n",
    "    union_val = np.union1d(train_df[feat].unique(), test_df[feat].unique())\n",
    "\n",
    "    # extract the highest value from from the feature in decimal format.\n",
    "    max_dec = union_val.max()\n",
    "    \n",
    "    # work out how the ammount of digtis required to be represent max_dev in binary representation\n",
    "    max_bin_len = len(\"{0:b}\".format(max_dec))\n",
    "    index = np.arange(len(union_val))\n",
    "    columns = list([feat])\n",
    "    \n",
    "    # create a binary encoding feature dataframe to capture all the levels for the feature\n",
    "    bin_df = pd.DataFrame(index=index, columns=columns)\n",
    "    bin_df[feat] = union_val\n",
    "    \n",
    "    # capture the binary representation for each level of the feature \n",
    "    feat_bin = bin_df[feat].apply(lambda x: \"{0:b}\".format(x).zfill(max_bin_len))\n",
    "    \n",
    "    # split the binary representation into different bit of digits \n",
    "    splitted = feat_bin.apply(lambda x: pd.Series(list(x)).astype(np.uint8))\n",
    "    splitted.columns = [feat + '_bin_' + str(x) for x in splitted.columns]\n",
    "    bin_df = bin_df.join(splitted)\n",
    "    \n",
    "    # merge the binary feature encoding dataframe with the train and test dataset - Done! \n",
    "    train_df = pd.merge(train_df, bin_df, how='left', on=[feat])\n",
    "    test_df = pd.merge(test_df, bin_df, how='left', on=[feat])\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's run the binary encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols=['ps_ind_02_cat','ps_car_04_cat', 'ps_car_09_cat',\n",
    "          'ps_ind_05_cat', 'ps_car_01_cat']\n",
    "\n",
    "train, test=binary_encoding(train, test, 'ps_ind_02_cat')\n",
    "train, test=binary_encoding(train, test, 'ps_car_04_cat')\n",
    "train, test=binary_encoding(train, test, 'ps_car_09_cat')\n",
    "train, test=binary_encoding(train, test, 'ps_ind_05_cat')\n",
    "train, test=binary_encoding(train, test, 'ps_car_01_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optionally, you can also choose to drop the original categorical features. Shoud you do it? I say trust your CV :)\n",
    "let's do this here jut for demonstration purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train.drop(col_to_drop, axis=1, inplace=True)  \n",
    "test.drop(col_to_drop, axis=1, inplace=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Feature Reduction\n",
    "Let's now drop all the features with the wording \"cal\" - \"Cal, you are FIRED\" ^ ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train.drop(col_to_drop, axis=1, inplace=True)  \n",
    "test.drop(col_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, after the above data manipulation, we can now take a brief look at our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_ind_02_cat_freq</th>\n",
       "      <th>ps_car_04_cat_freq</th>\n",
       "      <th>ps_car_09_cat_freq</th>\n",
       "      <th>ps_ind_05_cat_freq</th>\n",
       "      <th>ps_car_01_cat_freq</th>\n",
       "      <th>ps_car_11_cat_freq</th>\n",
       "      <th>ps_ind_02_cat_bin_0</th>\n",
       "      <th>ps_ind_02_cat_bin_1</th>\n",
       "      <th>ps_ind_02_cat_bin_2</th>\n",
       "      <th>ps_car_04_cat_bin_0</th>\n",
       "      <th>ps_car_04_cat_bin_1</th>\n",
       "      <th>ps_car_04_cat_bin_2</th>\n",
       "      <th>ps_car_04_cat_bin_3</th>\n",
       "      <th>ps_car_09_cat_bin_0</th>\n",
       "      <th>ps_car_09_cat_bin_1</th>\n",
       "      <th>ps_car_09_cat_bin_2</th>\n",
       "      <th>ps_ind_05_cat_bin_0</th>\n",
       "      <th>ps_ind_05_cat_bin_1</th>\n",
       "      <th>ps_ind_05_cat_bin_2</th>\n",
       "      <th>ps_car_01_cat_bin_0</th>\n",
       "      <th>ps_car_01_cat_bin_1</th>\n",
       "      <th>ps_car_01_cat_bin_2</th>\n",
       "      <th>ps_car_01_cat_bin_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>123573</td>\n",
       "      <td>496581</td>\n",
       "      <td>194518</td>\n",
       "      <td>528009</td>\n",
       "      <td>50087</td>\n",
       "      <td>7246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>431859</td>\n",
       "      <td>496581</td>\n",
       "      <td>353482</td>\n",
       "      <td>528009</td>\n",
       "      <td>207573</td>\n",
       "      <td>5097</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>11378</td>\n",
       "      <td>496581</td>\n",
       "      <td>353482</td>\n",
       "      <td>528009</td>\n",
       "      <td>179247</td>\n",
       "      <td>7992</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>431859</td>\n",
       "      <td>496581</td>\n",
       "      <td>14756</td>\n",
       "      <td>528009</td>\n",
       "      <td>179247</td>\n",
       "      <td>85083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>123573</td>\n",
       "      <td>496581</td>\n",
       "      <td>353482</td>\n",
       "      <td>528009</td>\n",
       "      <td>207573</td>\n",
       "      <td>10470</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  \\\n",
       "0   7       0          2              2          5              1   \n",
       "1   9       0          1              1          7              0   \n",
       "2  13       0          5              4          9              1   \n",
       "3  16       0          0              1          2              0   \n",
       "4  17       0          0              2          0              1   \n",
       "\n",
       "   ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n",
       "0              0              0              1              0              0   \n",
       "1              0              0              0              1              0   \n",
       "2              0              0              0              1              0   \n",
       "3              0              1              0              0              0   \n",
       "4              0              1              0              0              0   \n",
       "\n",
       "   ps_ind_10_bin  ps_ind_11_bin  ps_ind_12_bin  ps_ind_13_bin  ps_ind_14  \\\n",
       "0              0              0              0              0          0   \n",
       "1              0              0              0              0          0   \n",
       "2              0              0              0              0          0   \n",
       "3              0              0              0              0          0   \n",
       "4              0              0              0              0          0   \n",
       "\n",
       "   ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ps_reg_01  \\\n",
       "0         11              0              1              0        0.7   \n",
       "1          3              0              0              1        0.8   \n",
       "2         12              1              0              0        0.0   \n",
       "3          8              1              0              0        0.9   \n",
       "4          9              1              0              0        0.7   \n",
       "\n",
       "   ps_reg_02  ps_reg_03  ps_car_01_cat  ps_car_02_cat  ps_car_03_cat  \\\n",
       "0        0.2   0.718070             10              1             -1   \n",
       "1        0.4   0.766078             11              1             -1   \n",
       "2        0.0  -1.000000              7              1             -1   \n",
       "3        0.2   0.580948              7              1              0   \n",
       "4        0.6   0.840759             11              1             -1   \n",
       "\n",
       "   ps_car_04_cat  ps_car_05_cat  ps_car_06_cat  ps_car_07_cat  ps_car_08_cat  \\\n",
       "0              0              1              4              1              0   \n",
       "1              0             -1             11              1              1   \n",
       "2              0             -1             14              1              1   \n",
       "3              0              1             11              1              1   \n",
       "4              0             -1             14              1              1   \n",
       "\n",
       "   ps_car_09_cat  ps_car_10_cat  ps_car_11_cat  ps_car_11  ps_car_12  \\\n",
       "0              0              1             12          2   0.400000   \n",
       "1              2              1             19          3   0.316228   \n",
       "2              2              1             60          1   0.316228   \n",
       "3              3              1            104          1   0.374166   \n",
       "4              2              1             82          3   0.316070   \n",
       "\n",
       "   ps_car_13  ps_car_14  ps_car_15  ps_ind_02_cat_freq  ps_car_04_cat_freq  \\\n",
       "0   0.883679   0.370810   3.605551              123573              496581   \n",
       "1   0.618817   0.388716   2.449490              431859              496581   \n",
       "2   0.641586   0.347275   3.316625               11378              496581   \n",
       "3   0.542949   0.294958   2.000000              431859              496581   \n",
       "4   0.565832   0.365103   2.000000              123573              496581   \n",
       "\n",
       "   ps_car_09_cat_freq  ps_ind_05_cat_freq  ps_car_01_cat_freq  \\\n",
       "0              194518              528009               50087   \n",
       "1              353482              528009              207573   \n",
       "2              353482              528009              179247   \n",
       "3               14756              528009              179247   \n",
       "4              353482              528009              207573   \n",
       "\n",
       "   ps_car_11_cat_freq  ps_ind_02_cat_bin_0  ps_ind_02_cat_bin_1  \\\n",
       "0                7246                    0                    1   \n",
       "1                5097                    0                    0   \n",
       "2                7992                    1                    0   \n",
       "3               85083                    0                    0   \n",
       "4               10470                    0                    1   \n",
       "\n",
       "   ps_ind_02_cat_bin_2  ps_car_04_cat_bin_0  ps_car_04_cat_bin_1  \\\n",
       "0                    0                    0                    0   \n",
       "1                    1                    0                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    1                    0                    0   \n",
       "4                    0                    0                    0   \n",
       "\n",
       "   ps_car_04_cat_bin_2  ps_car_04_cat_bin_3  ps_car_09_cat_bin_0  \\\n",
       "0                    0                    0                    0   \n",
       "1                    0                    0                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    0                    0   \n",
       "4                    0                    0                    0   \n",
       "\n",
       "   ps_car_09_cat_bin_1  ps_car_09_cat_bin_2  ps_ind_05_cat_bin_0  \\\n",
       "0                    0                    0                    0   \n",
       "1                    1                    0                    0   \n",
       "2                    1                    0                    0   \n",
       "3                    1                    1                    0   \n",
       "4                    1                    0                    0   \n",
       "\n",
       "   ps_ind_05_cat_bin_1  ps_ind_05_cat_bin_2  ps_car_01_cat_bin_0  \\\n",
       "0                    0                    0                    1   \n",
       "1                    0                    0                    1   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    0                    0   \n",
       "4                    0                    0                    1   \n",
       "\n",
       "   ps_car_01_cat_bin_1  ps_car_01_cat_bin_2  ps_car_01_cat_bin_3  \n",
       "0                    0                    1                    0  \n",
       "1                    0                    1                    1  \n",
       "2                    1                    1                    1  \n",
       "3                    1                    1                    1  \n",
       "4                    0                    1                    1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-fold CV with Out-of-Fold Prediction\n",
    "\n",
    "\n",
    "*Note: for demonstration purpose, I have dump down the parameters of the models to make them run faster, so please take time to find a good cominbation of parameter when you send them out to battle for real!*\n",
    "\n",
    "## 2.1 OOF utility functions\n",
    "Firstly, let's write this handy function to convert AUC score into Gini Normalised Coeficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_to_gini_norm(auc_score):\n",
    "    return 2*auc_score-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Sklearn K-fold & OOF function\n",
    "Next up next provide a K-fold function that generate out-of-fold predictions for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_sklearn(clf, x_train, y_train , x_test, kf,scale=False, verbose=True):\n",
    "    start_time=time.time()\n",
    "    \n",
    "    # initialise the size of out-of-fold train an test prediction\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # use the kfold object to generate the required folds\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        # generate training folds and validation fold\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[test_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        # perform scaling if required i.e. for linear algorithms\n",
    "        if scale:\n",
    "            scaler = StandardScaler().fit(x_train_kf.values)\n",
    "            x_train_kf_values = scaler.transform(x_train_kf.values)\n",
    "            x_val_kf_values = scaler.transform(x_val_kf.values)\n",
    "            x_test_values = scaler.transform(x_test.values)\n",
    "        else:\n",
    "            x_train_kf_values = x_train_kf.values\n",
    "            x_val_kf_values = x_val_kf.values\n",
    "            x_test_values = x_test.values\n",
    "        \n",
    "        # fit the input classifier and perform prediction.\n",
    "        clf.fit(x_train_kf_values, y_train_kf.values)\n",
    "        val_pred=clf.predict_proba(x_val_kf_values)[:,1]\n",
    "        train_pred[test_index] += val_pred\n",
    "\n",
    "        y_test_preds = clf.predict_proba(x_test_values)[:,1]\n",
    "        test_pred += y_test_preds\n",
    "\n",
    "        fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "\n",
    "        if verbose:\n",
    "            print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_score = [cv_auc, cv_gini_norm]\n",
    "    if verbose:\n",
    "        print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    return cv_score, train_pred,test_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Xgboost K-fold & OOF function\n",
    "In this part, we are going to use the native interface of XGB and LGB, so the following functions are tailor for this. For sure it would be easiler just to call the respective sklearn api, but the native interfaces provide some nice additional capability. For instance, the 'hist' option to use fast histogram in XGB is only available via the native interface as far as I know. \n",
    "\n",
    "Also, we need to provide the following function to convert probability into rank for these two OOF function. The needs to use normalised rank instead of predicted probabilities will become appearent later in this notebook :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability_to_rank(prediction, scaler=1):\n",
    "    pred_df=pd.DataFrame(columns=['probability'])\n",
    "    pred_df['probability']=prediction\n",
    "    pred_df['rank']=pred_df['probability'].rank()/len(prediction)*scaler\n",
    "    return pred_df['rank'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the k-fold function for XGB to generate OOF predictions, this function is very much similar to its sklearn counter part. The difference is that we need to use the XGB interface to facilitate the classifer, also we provide an option cover probability into rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, x_test, kf, cat_cols=[], verbose=True, \n",
    "                       verbose_eval=50, num_boost_round=4000, use_rank=True):\n",
    "    start_time=time.time()\n",
    "\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        x_test_kf=x_test.copy()\n",
    "\n",
    "        d_train_kf = xgb.DMatrix(x_train_kf, label=y_train_kf)\n",
    "        d_val_kf = xgb.DMatrix(x_val_kf, label=y_val_kf)\n",
    "        d_test = xgb.DMatrix(x_test_kf)\n",
    "\n",
    "        bst = xgb.train(params, d_train_kf, num_boost_round=num_boost_round,\n",
    "                        evals=[(d_train_kf, 'train'), (d_val_kf, 'val')], verbose_eval=verbose_eval,\n",
    "                        early_stopping_rounds=50)\n",
    "\n",
    "        val_pred = bst.predict(d_val_kf, ntree_limit=bst.best_ntree_limit)\n",
    "        if use_rank:\n",
    "            train_pred[val_index] += probability_to_rank(val_pred)\n",
    "            test_pred+=probability_to_rank(bst.predict(d_test))\n",
    "        else:\n",
    "            train_pred[val_index] += val_pred\n",
    "            test_pred+=bst.predict(d_test)\n",
    "\n",
    "        fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "\n",
    "        if verbose:\n",
    "            print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, \n",
    "                                                                                     fold_gini_norm))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_score = [cv_auc, cv_gini_norm]\n",
    "    if verbose:\n",
    "        print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "\n",
    "        return cv_score, train_pred,test_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 LigthGBM K-fold & OOF function\n",
    "The same function for LGB, this one is almost identifical to the one for XGB, apart from code that call the LightGBM interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_lgb(params, x_train, y_train, x_test, kf, cat_cols=[],\n",
    "                       verbose=True, verbose_eval=50, use_cat=True, use_rank=True):\n",
    "    start_time = time.time()\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    if len(cat_cols)==0: use_cat=False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        if use_cat:\n",
    "            lgb_train = lgb.Dataset(x_train_kf, y_train_kf, categorical_feature=cat_cols)\n",
    "            lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train, categorical_feature=cat_cols)\n",
    "        else:\n",
    "            lgb_train = lgb.Dataset(x_train_kf, y_train_kf)\n",
    "            lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train)\n",
    "\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=4000,\n",
    "                        valid_sets=lgb_val,\n",
    "                        early_stopping_rounds=30,\n",
    "                        verbose_eval=verbose_eval)\n",
    "\n",
    "        val_pred = gbm.predict(x_val_kf)\n",
    "\n",
    "        if use_rank:\n",
    "            train_pred[val_index] += probability_to_rank(val_pred)\n",
    "            test_pred += probability_to_rank(gbm.predict(x_test))\n",
    "            # test_pred += gbm.predict(x_test)\n",
    "        else:\n",
    "            train_pred[val_index] += val_pred\n",
    "            test_pred += gbm.predict(x_test)\n",
    "\n",
    "        # test_pred += gbm.predict(x_test)\n",
    "        fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "        if verbose:\n",
    "            print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_score = [cv_auc, cv_gini_norm]\n",
    "    if verbose:\n",
    "        print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    return cv_score, train_pred,test_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate level 1 OOF predictions\n",
    "Almost there to actually generate some level OOF output! last things to do is the prepare our train and test data for our dear machine learning algorithms, and create the StratifiedKFold object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_cols=['id','target']\n",
    "y_train=train['target']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "x_test=test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I would like remind you that for stacking, you SHALL use consistent fold distribution at ALL level for ALL your model. The technical reaons for this had been discussed at length in this [forum thread](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43467)  by our right honourable fellow competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right! next generate some level 1 model output..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Random Forest\n",
    "Let's use the old good random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.625795, Gini_Norm score is 0.251590\n",
      "fold cv 1 AUC score is 0.626817, Gini_Norm score is 0.253634\n",
      "fold cv 2 AUC score is 0.627344, Gini_Norm score is 0.254688\n",
      "fold cv 3 AUC score is 0.624523, Gini_Norm score is 0.249046\n",
      "fold cv 4 AUC score is 0.635070, Gini_Norm score is 0.270140\n",
      "cv AUC score is 0.627850, Gini_Norm score is 0.255700\n",
      "it takes 254.929 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=200, n_jobs=6, min_samples_split=5, max_depth=7,\n",
    "                          criterion='gini', random_state=0)\n",
    "\n",
    "outcomes =cross_validate_sklearn(rf, x_train, y_train ,x_test, kf, scale=False, verbose=True)\n",
    "\n",
    "rf_cv=outcomes[0]\n",
    "rf_train_pred=outcomes[1]\n",
    "rf_test_pred=outcomes[2]\n",
    "\n",
    "rf_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=rf_train_pred)\n",
    "rf_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=rf_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Extra Tree\n",
    "We love tree! We love more trees, and therefore let's have extra tree :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.620518, Gini_Norm score is 0.241035\n",
      "fold cv 1 AUC score is 0.621667, Gini_Norm score is 0.243333\n",
      "fold cv 2 AUC score is 0.621837, Gini_Norm score is 0.243674\n",
      "fold cv 3 AUC score is 0.618974, Gini_Norm score is 0.237949\n",
      "fold cv 4 AUC score is 0.628749, Gini_Norm score is 0.257498\n",
      "cv AUC score is 0.622247, Gini_Norm score is 0.244494\n",
      "it takes 104.600 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "et=RandomForestClassifier(n_estimators=100, n_jobs=6, min_samples_split=5, max_depth=5,\n",
    "                          criterion='gini', random_state=0)\n",
    "\n",
    "outcomes =cross_validate_sklearn(et, x_train, y_train ,x_test, kf, scale=False, verbose=True)\n",
    "\n",
    "et_cv=outcomes[0]\n",
    "et_train_pred=outcomes[1]\n",
    "et_test_pred=outcomes[2]\n",
    "\n",
    "et_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=et_train_pred)\n",
    "et_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=et_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Logistic Regression\n",
    "Let's now throw in our favourite linear friend - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.627904, Gini_Norm score is 0.255807\n",
      "fold cv 1 AUC score is 0.625503, Gini_Norm score is 0.251006\n",
      "fold cv 2 AUC score is 0.627603, Gini_Norm score is 0.255205\n",
      "fold cv 3 AUC score is 0.627591, Gini_Norm score is 0.255181\n",
      "fold cv 4 AUC score is 0.634728, Gini_Norm score is 0.269456\n",
      "cv AUC score is 0.628654, Gini_Norm score is 0.257308\n",
      "it takes 137.670 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "logit=LogisticRegression(random_state=0, C=0.5)\n",
    "\n",
    "outcomes = cross_validate_sklearn(logit, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n",
    "\n",
    "logit_cv=outcomes[0]\n",
    "logit_train_pred=outcomes[1]\n",
    "logit_test_pred=outcomes[2]\n",
    "\n",
    "logit_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=logit_train_pred)\n",
    "logit_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=logit_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 BernoulliNB\n",
    "A little bit of diversity from Naive Bayes never heard, this one of those algorithms that normally don't generate sigle output that rival XGB/LGB, but nevertheless help to improve the overal stacking performance due the diversity it bring to the party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.616210, Gini_Norm score is 0.232420\n",
      "fold cv 1 AUC score is 0.617357, Gini_Norm score is 0.234713\n",
      "fold cv 2 AUC score is 0.619080, Gini_Norm score is 0.238161\n",
      "fold cv 3 AUC score is 0.617449, Gini_Norm score is 0.234899\n",
      "fold cv 4 AUC score is 0.627373, Gini_Norm score is 0.254747\n",
      "cv AUC score is 0.619464, Gini_Norm score is 0.238929\n",
      "it takes 39.029 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "nb=BernoulliNB()\n",
    "\n",
    "outcomes =cross_validate_sklearn(nb, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n",
    "\n",
    "nb_cv=outcomes[0]\n",
    "nb_train_pred=outcomes[1]\n",
    "nb_test_pred=outcomes[2]\n",
    "\n",
    "nb_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=nb_train_pred)\n",
    "nb_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=nb_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 XGB\n",
    "Now this is our go-to GBM Bazooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.640712, Gini_Norm score is 0.281423\n",
      "fold cv 1 AUC score is 0.639335, Gini_Norm score is 0.278671\n",
      "fold cv 2 AUC score is 0.641237, Gini_Norm score is 0.282475\n",
      "fold cv 3 AUC score is 0.640754, Gini_Norm score is 0.281508\n",
      "fold cv 4 AUC score is 0.644637, Gini_Norm score is 0.289273\n",
      "cv AUC score is 0.641211, Gini_Norm score is 0.282422\n",
      "it takes 195.904 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"booster\"  :  \"gbtree\", \n",
    "    \"objective\"         :  \"binary:logistic\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"gamma\": 0.70,\n",
    "    \"subsample\": 0.76,\n",
    "    \"colsample_bytree\": 0.95,\n",
    "    \"nthread\": 6,\n",
    "    \"seed\": 0,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "outcomes=cross_validate_xgb(xgb_params, x_train, y_train, x_test, kf, use_rank=False, verbose_eval=False)\n",
    "\n",
    "xgb_cv=outcomes[0]\n",
    "xgb_train_pred=outcomes[1]\n",
    "xgb_test_pred=outcomes[2]\n",
    "\n",
    "xgb_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=xgb_train_pred)\n",
    "xgb_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=xgb_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 LightGBM\n",
    "There is a crack in everything, that's how the light gets in :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1005: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.639668, Gini_Norm score is 0.279337\n",
      "fold cv 1 AUC score is 0.639214, Gini_Norm score is 0.278427\n",
      "fold cv 2 AUC score is 0.640306, Gini_Norm score is 0.280612\n",
      "fold cv 3 AUC score is 0.640433, Gini_Norm score is 0.280866\n",
      "fold cv 4 AUC score is 0.643997, Gini_Norm score is 0.287994\n",
      "cv AUC score is 0.638933, Gini_Norm score is 0.277866\n",
      "it takes 94.960 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc'},\n",
    "    'num_leaves': 22,\n",
    "    'min_sum_hessian_in_leaf': 20,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.1,  # 0.618580\n",
    "    'num_threads': 6,\n",
    "    'feature_fraction': 0.6894,\n",
    "    'bagging_fraction': 0.4218,\n",
    "    'max_drop': 5,\n",
    "    'drop_rate': 0.0123,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 0.01,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "cat_cols=['ps_ind_02_cat','ps_car_04_cat', 'ps_car_09_cat','ps_ind_05_cat', 'ps_car_01_cat']\n",
    "outcomes=cross_validate_lgb(lgb_params,x_train, y_train ,x_test,kf, cat_cols, use_cat=True, \n",
    "                            verbose_eval=False, use_rank=False)\n",
    "\n",
    "lgb_cv=outcomes[0]\n",
    "lgb_train_pred=outcomes[1]\n",
    "lgb_test_pred=outcomes[2]\n",
    "\n",
    "lgb_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=lgb_train_pred)\n",
    "lgb_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=lgb_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our level 1 friends ready, lets proceed and send them into the stacking party!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Level 2 ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Generate L1 output dataframe\n",
    "Let's group ouf level 1 OOF predictions output together to genenerate the input for level 2 stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns=['rf','et','logit','nb','xgb','lgb']\n",
    "train_pred_df_list=[rf_train_pred_df, et_train_pred_df, logit_train_pred_df, nb_train_pred_df,\n",
    "                    xgb_train_pred_df, lgb_train_pred_df]\n",
    "\n",
    "test_pred_df_list=[rf_test_pred_df, et_test_pred_df, logit_test_pred_df, nb_test_pred_df,\n",
    "                    xgb_test_pred_df, lgb_test_pred_df]\n",
    "\n",
    "lv1_train_df=pd.DataFrame(columns=columns)\n",
    "lv1_test_df=pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(0,len(columns)):\n",
    "    lv1_train_df[columns[i]]=train_pred_df_list[i]['prediction_probability']\n",
    "    lv1_test_df[columns[i]]=test_pred_df_list[i]['prediction_probability']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Level 2 XGB\n",
    "Back to XGB for level 2! everything shall be the same, paint old easy mdoel building, right? well.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.641396, Gini_Norm score is 0.282792\n",
      "fold cv 1 AUC score is 0.640072, Gini_Norm score is 0.280144\n",
      "fold cv 2 AUC score is 0.642492, Gini_Norm score is 0.284983\n",
      "fold cv 3 AUC score is 0.640646, Gini_Norm score is 0.281291\n",
      "fold cv 4 AUC score is 0.645430, Gini_Norm score is 0.290860\n",
      "cv AUC score is 0.551233, Gini_Norm score is 0.102467\n",
      "it takes 39.725 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "xgb_lv2_outcomes=cross_validate_xgb(xgb_params, lv1_train_df, y_train, lv1_test_df, kf, \n",
    "                                          verbose=True, verbose_eval=False, use_rank=False)\n",
    "\n",
    "xgb_lv2_cv=xgb_lv2_outcomes[0]\n",
    "xgb_lv2_train_pred=xgb_lv2_outcomes[1]\n",
    "xgb_lv2_test_pred=xgb_lv2_outcomes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what just happened there?  Our CV score for each training fold is pretty descent, but our overall training CV score just fell through the crack!  Well, it turns out since we are using AUC/Gini as metric which is ranking dependent, and it turns out that if you apply xgb and lgb at level 2 stacking, the ranking get messed up when each fold's prediction scores are put together.  And this goes back to why we implemented that function to convert probability into ranks earlier.\n",
    "\n",
    "Now, let's use the *use_rank* option, and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.641396, Gini_Norm score is 0.282792\n",
      "fold cv 1 AUC score is 0.640072, Gini_Norm score is 0.280144\n",
      "fold cv 2 AUC score is 0.642492, Gini_Norm score is 0.284983\n",
      "fold cv 3 AUC score is 0.640646, Gini_Norm score is 0.281291\n",
      "fold cv 4 AUC score is 0.645430, Gini_Norm score is 0.290860\n",
      "cv AUC score is 0.642071, Gini_Norm score is 0.284141\n",
      "it takes 42.303 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "xgb_lv2_outcomes=cross_validate_xgb(xgb_params, lv1_train_df, y_train, lv1_test_df, kf, \n",
    "                                          verbose=True, verbose_eval=False, use_rank=True)\n",
    "\n",
    "xgb_lv2_cv=xgb_lv2_outcomes[0]\n",
    "xgb_lv2_train_pred=xgb_lv2_outcomes[1]\n",
    "xgb_lv2_test_pred=xgb_lv2_outcomes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, the OOF score for train prediction looks great! and you can see the score here is better already than any of the level 1 OOF train score. The best score in level 1 comes from XGB with 0.282 region, and we are now on 0.284"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.3 Level 2 LightGBM\n",
    "Same story for LightGBM at level 2, we need to use the *use_rank* option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.639042, Gini_Norm score is 0.278084\n",
      "fold cv 1 AUC score is 0.638521, Gini_Norm score is 0.277041\n",
      "fold cv 2 AUC score is 0.640026, Gini_Norm score is 0.280053\n",
      "fold cv 3 AUC score is 0.638856, Gini_Norm score is 0.277711\n",
      "fold cv 4 AUC score is 0.644200, Gini_Norm score is 0.288399\n",
      "cv AUC score is 0.640161, Gini_Norm score is 0.280322\n",
      "it takes 12.749 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "lgb_lv2_outcomes=cross_validate_lgb(lgb_params,lv1_train_df, y_train ,lv1_test_df,kf, [], use_cat=False, \n",
    "                                    verbose_eval=False, use_rank=True)\n",
    "\n",
    "lgb_lv2_cv=xgb_lv2_outcomes[0]\n",
    "lgb_lv2_train_pred=lgb_lv2_outcomes[1]\n",
    "lgb_lv2_test_pred=lgb_lv2_outcomes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Level 2 Random Forest\n",
    "Now let's try a few more algorithms on level 2, and let's revisit random forest again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.640414, Gini_Norm score is 0.280827\n",
      "fold cv 1 AUC score is 0.639648, Gini_Norm score is 0.279295\n",
      "fold cv 2 AUC score is 0.641957, Gini_Norm score is 0.283913\n",
      "fold cv 3 AUC score is 0.640310, Gini_Norm score is 0.280619\n",
      "fold cv 4 AUC score is 0.645669, Gini_Norm score is 0.291337\n",
      "cv AUC score is 0.640730, Gini_Norm score is 0.281461\n",
      "it takes 327.870 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "rf_lv2=RandomForestClassifier(n_estimators=200, n_jobs=6, min_samples_split=5, max_depth=7,\n",
    "                          criterion='gini', random_state=0)\n",
    "rf_lv2_outcomes = cross_validate_sklearn(rf_lv2, lv1_train_df, y_train ,lv1_test_df, kf, \n",
    "                                            scale=True, verbose=True)\n",
    "rf_lv2_cv=rf_lv2_outcomes[0]\n",
    "rf_lv2_train_pred=rf_lv2_outcomes[1]\n",
    "rf_lv2_test_pred=rf_lv2_outcomes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Level 2 Logistic Regression\n",
    "Logistic Regression, take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.640367, Gini_Norm score is 0.280733\n",
      "fold cv 1 AUC score is 0.639172, Gini_Norm score is 0.278344\n",
      "fold cv 2 AUC score is 0.641695, Gini_Norm score is 0.283391\n",
      "fold cv 3 AUC score is 0.641005, Gini_Norm score is 0.282011\n",
      "fold cv 4 AUC score is 0.644079, Gini_Norm score is 0.288159\n",
      "cv AUC score is 0.639993, Gini_Norm score is 0.279987\n",
      "it takes 8.241 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "logit_lv2=LogisticRegression(random_state=0, C=0.5)\n",
    "logit_lv2_outcomes = cross_validate_sklearn(logit_lv2, lv1_train_df, y_train ,lv1_test_df, kf, \n",
    "                                            scale=True, verbose=True)\n",
    "logit_lv2_cv=logit_lv2_outcomes[0]\n",
    "logit_lv2_train_pred=logit_lv2_outcomes[1]\n",
    "logit_lv2_test_pred=logit_lv2_outcomes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully by now you can see that on level 2, models like random forest and logistic regression are now producing very competivie results thanks to the meta-features from the level 1 OOF output.\n",
    "\n",
    "We are having fun! and why stop in level 2? let's bring on level 3 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Level 3 ensemble\n",
    "On level 3, we follow simlar workflow as level 2. First we put the OOF output from level 2 together, and then send them to our chosen algorithms.\n",
    "\n",
    "## 5.1 Generate L2 output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lv2_columns=['rf_lf2', 'logit_lv2', 'xgb_lv2','lgb_lv2']\n",
    "train_lv2_pred_list=[rf_lv2_train_pred, logit_lv2_train_pred, xgb_lv2_train_pred, lgb_lv2_train_pred]\n",
    "\n",
    "test_lv2_pred_list=[rf_lv2_test_pred, logit_lv2_test_pred, xgb_lv2_test_pred, lgb_lv2_test_pred]\n",
    "\n",
    "lv2_train=pd.DataFrame(columns=lv2_columns)\n",
    "lv2_test=pd.DataFrame(columns=lv2_columns)\n",
    "\n",
    "for i in range(0,len(lv2_columns)):\n",
    "    lv2_train[lv2_columns[i]]=train_lv2_pred_list[i]\n",
    "    lv2_test[lv2_columns[i]]=test_lv2_pred_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Level 3 XGB \n",
    "On this level, let's just stay with our trusted weapon XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.641384, Gini_Norm score is 0.282769\n",
      "fold cv 1 AUC score is 0.640397, Gini_Norm score is 0.280793\n",
      "fold cv 2 AUC score is 0.642263, Gini_Norm score is 0.284526\n",
      "fold cv 3 AUC score is 0.641031, Gini_Norm score is 0.282061\n",
      "fold cv 4 AUC score is 0.645652, Gini_Norm score is 0.291303\n",
      "cv AUC score is 0.642170, Gini_Norm score is 0.284341\n",
      "it takes 50.266 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "xgb_lv3_params = {\n",
    "    \"booster\"  :  \"gbtree\", \n",
    "    \"objective\"         :  \"binary:logistic\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 2,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"gamma\": 0.70,\n",
    "    \"subsample\": 0.76,\n",
    "    \"colsample_bytree\": 0.95,\n",
    "    \"nthread\": 6,\n",
    "    \"seed\": 0,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "xgb_lv3_outcomes=cross_validate_xgb(xgb_lv3_params, lv2_train, y_train, lv2_test, kf, \n",
    "                                          verbose=True, verbose_eval=False, use_rank=True)\n",
    "\n",
    "xgb_lv3_cv=xgb_lv3_outcomes[0]\n",
    "xgb_lv3_train_pred=xgb_lv3_outcomes[1]\n",
    "xgb_lv3_test_pred=xgb_lv3_outcomes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly better than the XGB ouput at level 2, but not by much, as we are now seeing diminsing return as the level improve. Let's try tp pair this with something linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Level 3 Logistic Regression\n",
    "and of course that something linear is going to be Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 AUC score is 0.641391, Gini_Norm score is 0.282782\n",
      "fold cv 1 AUC score is 0.640421, Gini_Norm score is 0.280841\n",
      "fold cv 2 AUC score is 0.642635, Gini_Norm score is 0.285269\n",
      "fold cv 3 AUC score is 0.640969, Gini_Norm score is 0.281938\n",
      "fold cv 4 AUC score is 0.645677, Gini_Norm score is 0.291354\n",
      "cv AUC score is 0.642087, Gini_Norm score is 0.284175\n",
      "it takes 7.335 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "logit_lv3=LogisticRegression(random_state=0, C=0.5)\n",
    "logit_lv3_outcomes = cross_validate_sklearn(logit_lv3, lv2_train, y_train ,lv2_test, kf, \n",
    "                                            scale=True, verbose=True)\n",
    "logit_lv3_cv=logit_lv3_outcomes[0]\n",
    "logit_lv3_train_pred=logit_lv3_outcomes[1]\n",
    "logit_lv3_test_pred=logit_lv3_outcomes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this level, we don't see that much different between XGB and Logistic Regression anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Average L3 outputs & Submission Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always still do a simple weight average, to bring the two together and see if there any extra juice to be squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.284340415728\n"
     ]
    }
   ],
   "source": [
    "weight_avg=logit_lv3_train_pred*0.5+ xgb_lv3_train_pred*0.5\n",
    "print(auc_to_gini_norm(roc_auc_score(y_train, weight_avg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, for training score, we manage to arravie at 0.28443.\n",
    "We can now try to apply the same weight distribution to generate our submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission=sample_submission.copy()\n",
    "submission['target']=logit_lv3_test_pred*0.5+ xgb_lv3_test_pred*0.5\n",
    "filename='stacking_demonstration.csv.gz'\n",
    "submission.to_csv(filename,compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 After Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I hope this three-level stacking guide is useful to demonstrate how you can capture more information from the training data, and hopefully this can generate to better test prediction. Well I use the word \"hopefully\" here as we all know the dataset in this competition is pretty noisy.   and in truth I am not sure if stacking beyong 2 level would bring much benefit, but then we will learn from the highflyers who survive the shake up!\n",
    "\n",
    "My personal likely strategy to approach stacking is probably:\n",
    "* go with a 2-level approach, and weight average on level 2\n",
    "* applying the same stacking routine to several different random seed. \n",
    "* weigh average the above\n",
    "\n",
    "I seriously think robust CV is the key for this competition, and always be suspicious of all things shared on kernel. Alternatively, perhaps with 1 day to go, someone will share a leak or a 0.291 script to send us into a frenzy? One can always hope...\n",
    "\n",
    "Enjoy every bit of these last days - May all the insured drivers never have to claim! :)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
